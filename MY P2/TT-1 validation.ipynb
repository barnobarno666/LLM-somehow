{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeddfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils_validation.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants_validation import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT, TRAIN_PERCENTAGE, VALIDATION_PERCENTAGE\n",
    "import random, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_dataframe_to_train_validation(data_path, mode='train'):\n",
    "    \"\"\"\n",
    "    Modified function to support train/validation split from training data\n",
    "    mode: 'train' to get training portion, 'validation' to get validation portion\n",
    "    \"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split training data into train and validation\n",
    "    train_split, val_split = train_test_split(\n",
    "        train_dataset, \n",
    "        test_size=VALIDATION_PERCENTAGE, \n",
    "        random_state=42, \n",
    "        stratify=train_dataset['rule_violation']\n",
    "    )\n",
    "    \n",
    "    if mode == 'train':\n",
    "        chosen_dataset = train_split\n",
    "        print(f\"Using {len(chosen_dataset)} samples for training ({TRAIN_PERCENTAGE*100:.1f}% of original training data)\")\n",
    "    elif mode == 'validation':\n",
    "        chosen_dataset = val_split\n",
    "        print(f\"Using {len(chosen_dataset)} samples for validation ({VALIDATION_PERCENTAGE*100:.1f}% of original training data)\")\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'train' or 'validation'\")\n",
    "\n",
    "    flatten = []\n",
    "\n",
    "    # ---------- Process chosen dataset ----------\n",
    "    chosen_df = chosen_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                              \"positive_example_1\",\"positive_example_2\",\n",
    "                              \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # Randomly select positive_example and negative_example\n",
    "    chosen_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(chosen_df)) < 0.5,\n",
    "        chosen_df[\"positive_example_1\"],\n",
    "        chosen_df[\"positive_example_2\"]\n",
    "    )\n",
    "    chosen_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(chosen_df)) < 0.5,\n",
    "        chosen_df[\"negative_example_1\"],\n",
    "        chosen_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # Drop original candidate columns\n",
    "    chosen_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                           \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "    flatten.append(chosen_df)\n",
    "\n",
    "    # ---------- Process test dataset (only for training mode) ----------\n",
    "    if mode == 'train':\n",
    "        for violation_type in [\"positive\", \"negative\"]:\n",
    "            for i in range(1, 3):\n",
    "                sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                            \"positive_example_1\",\"positive_example_2\",\n",
    "                                            \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "                if violation_type == \"positive\":\n",
    "                    # body uses current positive_example\n",
    "                    body_col = f\"positive_example_{i}\"\n",
    "                    other_positive_col = f\"positive_example_{3-i}\"  # other positive\n",
    "                    sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                    sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                    # negative_example randomly selected\n",
    "                    sub_dataset[\"negative_example\"] = np.where(\n",
    "                        np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                        sub_dataset[\"negative_example_1\"],\n",
    "                        sub_dataset[\"negative_example_2\"]\n",
    "                    )\n",
    "                    sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "                else:  # violation_type == \"negative\"\n",
    "                    body_col = f\"negative_example_{i}\"\n",
    "                    other_negative_col = f\"negative_example_{3-i}\"\n",
    "                    sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                    sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                    sub_dataset[\"positive_example\"] = np.where(\n",
    "                        np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                        sub_dataset[\"positive_example_1\"],\n",
    "                        sub_dataset[\"positive_example_2\"]\n",
    "                    )\n",
    "                    sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "                # Drop original candidate columns\n",
    "                sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                          \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "                flatten.append(sub_dataset)\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    dataset.to_pandas().to_csv(\"/kaggle/working/dataset_validation.csv\", index=False)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_validation_dataframe_with_labels(data_path):\n",
    "    \"\"\"\n",
    "    Get validation dataframe with true labels for evaluation\n",
    "    \"\"\"\n",
    "    dataframe = get_dataframe_to_train_validation(data_path, mode='validation')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf4d9a8",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluation_metrics.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, confusion_matrix, classification_report, \n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def comprehensive_evaluation_report(y_true, y_pred_proba, threshold=0.5, class_names=['No Violation', 'Violation'], model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive evaluation report with metrics and visualizations\n",
    "    Exactly matches the reference unsloth-bnb implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    try:\n",
    "        accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "        f1 = f1_score(y_true, y_pred_binary)\n",
    "        precision = precision_score(y_true, y_pred_binary)\n",
    "        recall = recall_score(y_true, y_pred_binary)\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ðŸ“Š SUMMARY METRICS ({model_name})\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ðŸŽ¯ Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"ðŸŽ¯ F1 Score:  {f1:.4f}\")\n",
    "        print(f\"ðŸŽ¯ Precision: {precision:.4f}\")\n",
    "        print(f\"ðŸŽ¯ Recall:    {recall:.4f}\")\n",
    "        print(f\"ðŸŽ¯ ROC AUC:   {auc:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred_binary)\n",
    "        print(\"\\nðŸ“ˆ Confusion Matrix:\")\n",
    "        if cm.shape == (2, 2):\n",
    "            print(f\"True Negative: {cm[0,0]:4d} | False Positive: {cm[0,1]:4d}\")\n",
    "            print(f\"False Negative: {cm[1,0]:4d} | True Positive:  {cm[1,1]:4d}\")\n",
    "        else:\n",
    "            print(cm)\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nðŸ“‹ Classification Report:\")\n",
    "        print(classification_report(y_true, y_pred_binary, target_names=class_names))\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ðŸ“Š SUMMARY METRICS ({model_name})\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"âŒ Error calculating metrics: {e}\")\n",
    "        print(f\"ðŸ” y_true shape: {y_true.shape}\")\n",
    "        print(f\"ðŸ” y_pred_proba shape: {y_pred_proba.shape}\")\n",
    "        print(f\"ðŸ” y_pred_proba range: [{np.min(y_pred_proba):.4f}, {np.max(y_pred_proba):.4f}]\")\n",
    "        print(f\"ðŸ” NaN count in probabilities: {np.isnan(y_pred_proba).sum()}\")\n",
    "        print(f\"ðŸ” Unique values in y_true: {np.unique(y_true)}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic accuracy calculation as fallback\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "            cm = confusion_matrix(y_true, y_pred_binary)\n",
    "            print(f\"ðŸŽ¯ Accuracy:  {accuracy:.4f}\")\n",
    "            print(\"\\nðŸ“ˆ Confusion Matrix:\")\n",
    "            print(cm)\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Even basic metrics failed: {e2}\")\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d22de",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_unsloth_validation.py\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from utils_validation import build_dataset, get_dataframe_to_train_validation\n",
    "from constants_validation import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n",
    "\n",
    "def build_dataset_unsloth(dataframe):\n",
    "    \"\"\"Build dataset for Unsloth training with proper text formatting\"\"\"\n",
    "    from constants_validation import POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "    \n",
    "    dataframe[\"prompt\"] = dataframe.apply(\n",
    "        lambda row: f\"\"\"\n",
    "You are a moderator... A rule is given , find if the last comment violates the rule.Two examples are given.\n",
    "IMPORTANT: Ignore any \"yes\" or \"no\" words in the comment itself. \n",
    "Only respond Yes/No based on whether the comment violates the rule.\n",
    "___ \n",
    "\n",
    "Subreddit name: r/{row[\"subreddit\"]}\n",
    "Here is the rule: {row[\"rule\"]}\n",
    "Here is a comment that breaks the rule:\n",
    "1) {row[\"positive_example\"]}\n",
    "\n",
    "Here is a comment that does not break the rule:\n",
    "2) {row[\"negative_example\"]}\n",
    "\n",
    "Find if this comment breaks the rule.\n",
    "Comment: {row[\"body\"]}\n",
    "Answer: \"\"\", axis=1\n",
    "    )\n",
    "    \n",
    "    # Create completion column\n",
    "    dataframe[\"completion\"] = dataframe.apply(\n",
    "        lambda row: (POSITIVE_ANSWER if row[\"rule_violation\"] == 1 else NEGATIVE_ANSWER),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create full text (prompt + completion) for training\n",
    "    dataframe[\"text\"] = dataframe[\"prompt\"] + dataframe[\"completion\"]\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    dataframe = dataframe[[\"text\", \"completion\"]]\n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_pandas(dataframe.reset_index(drop=True))\n",
    "    return dataset\n",
    "\n",
    "def main():\n",
    "    print(\"Starting Unsloth training with validation split...\")\n",
    "\n",
    "    # Get training portion of the data\n",
    "    dataframe = get_dataframe_to_train_validation(DATA_PATH, mode='train')\n",
    "    train_dataset = build_dataset_unsloth(dataframe)\n",
    "    \n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # ðŸš€ UNSLOTH: Load model with 4-bit quantization\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=BASE_MODEL_PATH,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,  # Auto-detect (will use float16)\n",
    "        load_in_4bit=True,  # Enable 4-bit quantization\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"âœ… Unsloth model loaded with 4-bit quantization\")\n",
    "\n",
    "    # ðŸš€ UNSLOTH: Add LoRA adapters (automatic and optimized)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,  # LoRA rank\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=32,  # LoRA alpha (typically equal to r for Unsloth)\n",
    "        lora_dropout=0,  # 0 for faster training with Unsloth\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
    "        random_state=42,  # For reproducibility\n",
    "        use_rslora=True,  # Better stability\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    print(\"âœ… Unsloth LoRA adapters added\")\n",
    "\n",
    "    # ðŸš€ UNSLOTH: Optimized training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,  # Quick warmup with Unsloth\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,  # Unsloth supports higher learning rates\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=LORA_PATH,\n",
    "        save_steps=50,\n",
    "        save_total_limit=3,\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    print(\"âœ… Unsloth training arguments configured\")\n",
    "\n",
    "    # ðŸš€ UNSLOTH: Fast SFT Trainer (optimized for Unsloth)\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"text\",  # The text column containing the full conversation\n",
    "        max_seq_length=2048,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,\n",
    "        args=training_args,\n",
    "    )\n",
    "    print(\"âœ… Unsloth SFT Trainer created\")\n",
    "\n",
    "    # ðŸš€ Training with Unsloth (2-20x faster than standard)\n",
    "    print(\"ðŸš€ Starting Unsloth training...\")\n",
    "    trainer.train()\n",
    "    print(\"âœ… Unsloth training completed!\")\n",
    "\n",
    "    # ðŸš€ UNSLOTH: Save merged model for faster inference\n",
    "    merged_folder = \"/kaggle/working/Merged_unsloth_model\"\n",
    "    model.save_pretrained_merged(merged_folder, tokenizer, save_method=\"merged_16bit\")\n",
    "\n",
    "    # ðŸš€ UNSLOTH: Save LoRA adapters\n",
    "    model.save_pretrained(LORA_PATH)\n",
    "    tokenizer.save_pretrained(LORA_PATH)\n",
    "    \n",
    "    print(f\"âœ… LoRA adapters saved to {LORA_PATH}\")\n",
    "    print(f\"âœ… Merged model saved to {merged_folder}\")\n",
    "    print(\"ðŸŽ¯ Unsloth training complete! Ready for validation.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c477448",
   "metadata": {},
   "source": [
    "## Validation Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c318e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_validation.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "from utils_validation import build_dataset, get_validation_dataframe_with_labels\n",
    "from constants_validation import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "from evaluation_metrics import comprehensive_evaluation_report\n",
    "\n",
    "\n",
    "def run_validation_inference():\n",
    "    \"\"\"Run inference on validation set and return predictions with true labels\"\"\"\n",
    "    \n",
    "    print(\"Loading validation dataset...\")\n",
    "    validation_df = get_validation_dataframe_with_labels(DATA_PATH)\n",
    "    \n",
    "    # Build dataset for inference (only prompts needed)\n",
    "    validation_dataset = build_dataset(validation_df)\n",
    "    texts = validation_dataset[\"prompt\"]\n",
    "    \n",
    "    # Get true labels\n",
    "    y_true = validation_df[\"rule_violation\"].values\n",
    "    \n",
    "    print(f\"Validation set size: {len(texts)}\")\n",
    "    print(f\"Class distribution: {np.bincount(y_true)}\")\n",
    "    \n",
    "    print(\"Initializing vLLM model...\")\n",
    "    llm = vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        quantization=\"gptq\",\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.98,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2836,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "    )\n",
    "    \n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n",
    "    \n",
    "    print(\"Running inference...\")\n",
    "    outputs = llm.generate(\n",
    "        texts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n",
    "    )\n",
    "    \n",
    "    # Extract predictions and probabilities\n",
    "    log_probs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "    \n",
    "    predictions_df = pd.DataFrame(log_probs)\n",
    "    \n",
    "    # Convert log probabilities to probabilities\n",
    "    yes_logprobs = predictions_df[POSITIVE_ANSWER].values\n",
    "    no_logprobs = predictions_df[NEGATIVE_ANSWER].values\n",
    "    \n",
    "    # Calculate softmax to get proper probabilities\n",
    "    yes_probs = np.exp(yes_logprobs)\n",
    "    no_probs = np.exp(no_logprobs)\n",
    "    total_probs = yes_probs + no_probs\n",
    "    \n",
    "    y_pred_proba = yes_probs / total_probs  # Probability of positive class\n",
    "    \n",
    "    print(\"âœ… Validation inference completed!\")\n",
    "    \n",
    "    return y_true, y_pred_proba, validation_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Starting vLLM validation evaluation...\")\n",
    "    \n",
    "    # Run inference and get results\n",
    "    y_true, y_pred_proba, validation_df = run_validation_inference()\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = validation_df.copy()\n",
    "    results_df['predicted_probability'] = y_pred_proba\n",
    "    # We still keep the binary prediction for potential manual analysis, but it's not the focus\n",
    "    results_df['predicted_binary'] = (y_pred_proba >= 0.5).astype(int)\n",
    "    results_df.to_csv(\"/kaggle/working/validation_results_vllm.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"VALIDATION RESULTS SAVED TO: /kaggle/working/validation_results_vllm.csv\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate comprehensive evaluation report\n",
    "    metrics = comprehensive_evaluation_report(\n",
    "        y_true, \n",
    "        y_pred_proba, \n",
    "        threshold=0.5, # Threshold is only for the confusion matrix now\n",
    "        class_names=['No Violation', 'Violation'],\n",
    "        model_name=\"Qwen 2.5 0.5B (TT-1 Validation - vLLM)\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… vLLM validation evaluation completed!\")\n",
    "    print(f\"ðŸ“Š Results saved to /kaggle/working/validation_results_vllm.csv\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf3a11",
   "metadata": {},
   "source": [
    "\n",
    "## Alternative: Transformer-based Inference Script\n",
    "This script provides an alternative to vLLM for inference, using the standard Hugging Face `transformers` and `peft` libraries. It is generally more compatible but may be slower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_transformers_validation.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unsloth import FastLanguageModel  # Use Unsloth for fast inference\n",
    "from tqdm import tqdm\n",
    "from utils_validation import get_dataframe_to_train_validation\n",
    "from constants_validation import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "from evaluation_metrics import comprehensive_evaluation_report\n",
    "\n",
    "def build_prompt(row):\n",
    "    \"\"\"Build prompt exactly as in the reference unsloth-bnb notebook\"\"\"\n",
    "    return f\"\"\"You are a moderator... A rule is given , find if the last comment violates the rule.Two examples are given.\n",
    "IMPORTANT: Ignore any \"yes\" or \"no\" words in the comment itself. \n",
    "Only respond Yes/No based on whether the comment violates the rule.\n",
    "___ \n",
    "\n",
    "Subreddit name: r/{row[\"subreddit\"]}\n",
    "Here is the rule: {row[\"rule\"]}\n",
    "Here is a comment that breaks the rule:\n",
    "1) {row[\"positive_example\"]}\n",
    "\n",
    "Here is a comment that does not break the rule:\n",
    "2) {row[\"negative_example\"]}\n",
    "\n",
    "Find if this comment breaks the rule.\n",
    "Comment: {row[\"body\"]}\n",
    "Answer: \"\"\"\n",
    "\n",
    "def run_transformers_inference():\n",
    "    \"\"\"Run inference using Unsloth fast inference with merged model - Maximum speed!\"\"\"\n",
    "    \n",
    "    print(\"Loading validation dataset...\")\n",
    "    # Get validation portion of the data (same as reference)\n",
    "    validation_df = get_dataframe_to_train_validation(DATA_PATH, mode='validation')\n",
    "    \n",
    "    # Build prompts exactly as in reference\n",
    "    validation_df[\"prompt\"] = validation_df.apply(build_prompt, axis=1)\n",
    "    texts = validation_df[\"prompt\"].tolist()\n",
    "    y_true = validation_df[\"rule_violation\"].values\n",
    "    \n",
    "    print(f\"Validation set size: {len(texts)}\")\n",
    "    print(f\"Class distribution: {np.bincount(y_true)}\")\n",
    "    \n",
    "    print(\"Initializing Unsloth model for fast inference...\")\n",
    "    # ðŸš€ UNSLOTH: Load merged model for maximum speed (same as reference)\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"/kaggle/working/Merged_unsloth_model\",  # Use merged model path\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,  # Keep 4-bit for speed\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    # ðŸš€ UNSLOTH: Enable fast inference mode\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Get token IDs for \"Yes\" and \"No\" - exactly as in reference\n",
    "    yes_token_id = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "    no_token_id = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Token IDs: Yes={yes_token_id}, No={no_token_id}\")\n",
    "    \n",
    "    # Validate token IDs\n",
    "    if yes_token_id is None or no_token_id is None:\n",
    "        print(\"âŒ Token ID detection failed!\")\n",
    "        # Try alternative approaches\n",
    "        print(\"Trying alternative token detection...\")\n",
    "        yes_token_id = tokenizer.encode(\"Yes\")[0] if tokenizer.encode(\"Yes\") else None\n",
    "        no_token_id = tokenizer.encode(\"No\")[0] if tokenizer.encode(\"No\") else None\n",
    "        print(f\"ðŸ”„ Alternative Token IDs: Yes={yes_token_id}, No={no_token_id}\")\n",
    "        \n",
    "        if yes_token_id is None or no_token_id is None:\n",
    "            print(\"âŒ Critical error: Cannot find Yes/No token IDs\")\n",
    "            return None, None, None\n",
    "    \n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    batch_size = 8  # Same as reference\n",
    "    \n",
    "    print(\"ðŸš€ Running fast inference with Unsloth...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # ðŸš€ UNSLOTH: Optimized tokenization and inference (exact same as reference)\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # ðŸš€ UNSLOTH: Fast forward pass\n",
    "            outputs = model(**inputs)\n",
    "            next_token_logits = outputs.logits[:, -1, :]  # Get last token logits\n",
    "            \n",
    "            # Get probabilities for \"Yes\" and \"No\" tokens (exact same as reference)\n",
    "            yes_logits = next_token_logits[:, yes_token_id]\n",
    "            no_logits = next_token_logits[:, no_token_id]\n",
    "            \n",
    "            # Convert to probabilities using softmax over Yes/No only (exact same as reference)\n",
    "            combined_logits = torch.stack([no_logits, yes_logits], dim=1)  # [batch, 2]\n",
    "            probs = torch.softmax(combined_logits, dim=1)  # [batch, 2]\n",
    "            \n",
    "            # Extract predictions and probabilities (exact same as reference)\n",
    "            batch_predictions = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "            batch_probabilities = probs[:, 1].cpu().numpy()  # Probability of \"Yes\" (violation)\n",
    "            \n",
    "            predictions.extend(batch_predictions.tolist())\n",
    "            probabilities.extend(batch_probabilities.tolist())\n",
    "    \n",
    "    y_pred_proba = np.array(probabilities)\n",
    "    \n",
    "    print(\"âœ… Transformers validation inference completed!\")\n",
    "    print(f\"ðŸ” Sample probabilities: {y_pred_proba[:5]}\")\n",
    "    print(f\"ðŸ” Probability range: [{y_pred_proba.min():.4f}, {y_pred_proba.max():.4f}]\")\n",
    "    print(f\"ðŸ” NaN count: {np.isnan(y_pred_proba).sum()}\")\n",
    "    \n",
    "    return y_true, y_pred_proba, validation_df\n",
    "\n",
    "def main():\n",
    "    print(\"Starting Transformers validation evaluation...\")\n",
    "    \n",
    "    # Run inference\n",
    "    result = run_transformers_inference()\n",
    "    if result[0] is None:\n",
    "        print(\"âŒ Inference failed due to token ID issues\")\n",
    "        return\n",
    "        \n",
    "    y_true, y_pred_proba, validation_df = result\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if np.isnan(y_pred_proba).any():\n",
    "        print(f\"âŒ Found {np.isnan(y_pred_proba).sum()} NaN values in probabilities\")\n",
    "        print(\"Replacing NaN values with 0.5 (neutral probability)\")\n",
    "        y_pred_proba = np.nan_to_num(y_pred_proba, nan=0.5)\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = validation_df.copy()\n",
    "    results_df['predicted_probability'] = y_pred_proba\n",
    "    results_df['predicted_binary'] = (y_pred_proba >= 0.5).astype(int)\n",
    "    results_df.to_csv(\"/kaggle/working/validation_results_transformers.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"VALIDATION RESULTS SAVED TO: /kaggle/working/validation_results_transformers.csv\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate comprehensive evaluation report\n",
    "    comprehensive_evaluation_report(\n",
    "        y_true, \n",
    "        y_pred_proba, \n",
    "        threshold=0.5,\n",
    "        class_names=['No Violation', 'Violation'],\n",
    "        model_name=\"Qwen 2.5 0.5B (Unsloth Transformers Validation)\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Transformers validation evaluation completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f89d86",
   "metadata": {},
   "source": [
    "## Accelerate Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e7d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accelerate_config_validation.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 64\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e2b5a",
   "metadata": {},
   "source": [
    "## Execution: Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run Unsloth training on the controlled portion of training data\n",
    "!python train_unsloth_validation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Run Validation Inference\n",
    "# Choose ONE of the following methods to run validation.\n",
    "\n",
    "# Method 1: vLLM (Fast, recommended if hardware is compatible)\n",
    "!python inference_validation.py\n",
    "\n",
    "# Method 2: Standard Transformers (Slower, more compatible)\n",
    "# !python inference_transformers_validation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea420dc9",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eef9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine validation results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- CHOOSE YOUR RESULTS SOURCE ---\n",
    "# Change this to 'transformers' to analyze the results from the transformers script\n",
    "# validation_source = 'vllm' \n",
    "validation_source = 'transformers'\n",
    "\n",
    "# --- DO NOT EDIT BELOW THIS LINE ---\n",
    "\n",
    "results_file = f'/kaggle/working/validation_results_{validation_source}.csv'\n",
    "threshold_file = f'/kaggle/working/threshold_analysis_{validation_source}.csv'\n",
    "\n",
    "if not os.path.exists(results_file):\n",
    "    raise FileNotFoundError(f\"Results file not found: {results_file}. Please run the corresponding validation script first.\")\n",
    "\n",
    "# Load validation results\n",
    "validation_results = pd.read_csv(results_file)\n",
    "if os.path.exists(threshold_file):\n",
    "    threshold_analysis = pd.read_csv(threshold_file)\n",
    "else:\n",
    "    threshold_analysis = None # May not exist for transformers script\n",
    "\n",
    "print(f\"--- Analyzing results from: {validation_source.upper()} ---\")\n",
    "print(\"\\nValidation Results Overview:\")\n",
    "print(f\"Total validation samples: {len(validation_results)}\")\n",
    "print(f\"Actual class distribution:\")\n",
    "print(validation_results['rule_violation'].value_counts().sort_index())\n",
    "print(f\"\\nPredicted class distribution (threshold=0.5):\")\n",
    "print(validation_results['predicted_binary'].value_counts().sort_index())\n",
    "\n",
    "# Display sample of results\n",
    "print(f\"\\nSample of validation results:\")\n",
    "print(validation_results[['body', 'rule_violation', 'predicted_probability', 'predicted_binary']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae718d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Analysis: Prediction Distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Prediction probability distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(validation_results['predicted_probability'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Predicted Probabilities', fontweight='bold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Prediction probabilities by true class\n",
    "plt.subplot(1, 3, 2)\n",
    "no_violation = validation_results[validation_results['rule_violation'] == 0]['predicted_probability']\n",
    "violation = validation_results[validation_results['rule_violation'] == 1]['predicted_probability']\n",
    "\n",
    "plt.hist(no_violation, bins=30, alpha=0.7, label='No Violation (True)', color='blue', edgecolor='black')\n",
    "plt.hist(violation, bins=30, alpha=0.7, label='Violation (True)', color='red', edgecolor='black')\n",
    "plt.title('Predicted Probabilities by True Class', fontweight='bold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Box plot of predictions by true class\n",
    "plt.subplot(1, 3, 3)\n",
    "data_for_boxplot = [no_violation, violation]\n",
    "labels = ['No Violation', 'Violation']\n",
    "plt.boxplot(data_for_boxplot, labels=labels)\n",
    "plt.title('Prediction Distribution by True Class', fontweight='bold')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display calibration metrics\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Reliability diagram (calibration curve)\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    validation_results['rule_violation'], \n",
    "    validation_results['predicted_probability'], \n",
    "    n_bins=10\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Model\")\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Plot (Reliability Diagram)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\\\nSummary Statistics:\")\n",
    "print(f\"Mean predicted probability: {validation_results['predicted_probability'].mean():.4f}\")\n",
    "print(f\"Std predicted probability: {validation_results['predicted_probability'].std():.4f}\")\n",
    "print(f\"Min predicted probability: {validation_results['predicted_probability'].min():.4f}\")\n",
    "print(f\"Max predicted probability: {validation_results['predicted_probability'].max():.4f}\")\n",
    "\n",
    "# Most confident correct and incorrect predictions\n",
    "correct_predictions = validation_results[validation_results['rule_violation'] == validation_results['predicted_binary']]\n",
    "incorrect_predictions = validation_results[validation_results['rule_violation'] != validation_results['predicted_binary']]\n",
    "\n",
    "print(f\"\\\\nModel Performance Summary:\")\n",
    "print(f\"Correct predictions: {len(correct_predictions)} ({len(correct_predictions)/len(validation_results)*100:.1f}%)\")\n",
    "print(f\"Incorrect predictions: {len(incorrect_predictions)} ({len(incorrect_predictions)/len(validation_results)*100:.1f}%)\")\n",
    "\n",
    "if len(incorrect_predictions) > 0:\n",
    "    print(f\"\\\\nMost confident incorrect predictions:\")\n",
    "    # For incorrect predictions, show those with highest confidence (furthest from 0.5)\n",
    "    incorrect_predictions['confidence'] = np.abs(incorrect_predictions['predicted_probability'] - 0.5)\n",
    "    most_confident_wrong = incorrect_predictions.nlargest(3, 'confidence')\n",
    "    for idx, row in most_confident_wrong.iterrows():\n",
    "        print(f\"True: {row['rule_violation']}, Pred: {row['predicted_binary']}, Prob: {row['predicted_probability']:.3f}\")\n",
    "        print(f\"Comment: {row['body'][:100]}...\")\n",
    "        print(f\"Rule: {row['rule'][:100]}...\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6527c38",
   "metadata": {},
   "source": [
    "## Configuration Summary\n",
    "\n",
    "This validation notebook provides a comprehensive evaluation of the TT-1 Qwen 2.5 0.5B model, accelerated with **Unsloth**.\n",
    "\n",
    "### Key Features:\n",
    "1. **Unsloth-Powered Training**: Leverages Unsloth for significantly faster fine-tuning and reduced memory usage.\n",
    "2. **Dual Inference Options**:\n",
    "   - **vLLM**: High-performance inference for quick validation.\n",
    "   - **Transformers**: Standard, compatible inference as a reliable alternative.\n",
    "3. **Controlled Training Split**: Uses 70% of training data for training, 30% for validation, ensuring no data leakage.\n",
    "4. **Comprehensive Metrics**: AUC, confusion matrix, classification report, ROC curve, and precision-recall curve.\n",
    "5. **Flexible Analysis**: The analysis section can dynamically load and visualize results from either the vLLM or Transformers output.\n",
    "\n",
    "### Configuration:\n",
    "- **Training Framework**: **Unsloth**\n",
    "- **Training Percentage**: 70% of original training data\n",
    "- **Validation Percentage**: 30% of original training data  \n",
    "- **Model**: Qwen 2.5 0.5B with 4-bit quantization\n",
    "- **LoRA Configuration**: r=16, alpha=32\n",
    "- **Training**: 1 epoch with adamw_8bit optimizer\n",
    "\n",
    "### Output Files:\n",
    "- `validation_results_vllm.csv` / `validation_results_transformers.csv`: Detailed per-sample results.\n",
    "- `threshold_analysis_vllm.csv`: Performance metrics across different thresholds (vLLM only).\n",
    "- Comprehensive visualizations and analysis charts.\n",
    "\n",
    "This setup allows for robust, accelerated validation and provides deep insights for model improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058ddd9",
   "metadata": {},
   "source": [
    "# TT-1 Validation Notebook (Unsloth Edition): Qwen 2.5 0.5B Model\n",
    "\n",
    "This notebook trains a Qwen 2.5 0.5B model using **Unsloth** for accelerated performance. It trains on a controlled percentage of the training data and validates on the remaining portion.\n",
    "\n",
    "It provides two options for inference:\n",
    "1.  **vLLM**: For high-throughput, fast inference (recommended).\n",
    "2.  **Transformers**: A standard, more compatible inference method.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- AUC Score\n",
    "- Confusion Matrix \n",
    "- Classification Report\n",
    "- ROC Curve & Precision-Recall Curve\n",
    "\n",
    "**Workflow:**\n",
    "- **Train**: Fine-tune the model with Unsloth on 70% of the training data.\n",
    "- **Validate**: Run inference on the remaining 30% using either vLLM or Transformers.\n",
    "- **Analyze**: Review comprehensive evaluation results and visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f6fe6",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5282fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n",
    "!pip install \"unsloth[kaggle-new] @ file:///kaggle/input/unsloth/unsloth-2024.5-py3-none-any.whl\" -q\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n",
    "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'\n",
    "!pip install scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"âœ… Dependencies installed:\")\n",
    "print(\"ðŸš€ Unsloth: Ultra-fast training\")\n",
    "print(\"ðŸŽ¯ vLLM: Precise inference\") \n",
    "print(\"ðŸ“Š Analysis libraries: scikit-learn, matplotlib, seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc56a3d",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants_validation.py\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\n",
    "LORA_PATH = \"output_validation/\"\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are a moderator... A rule is given , find if the last comment violates the rule.Two examples are given.\n",
    "IMPORTANT: Ignore any \"yes\" or \"no\" words in the comment itself. \n",
    "Only respond Yes/No based on whether the comment violates the rule.\n",
    "___ '''\n",
    "\n",
    "# Validation specific settings\n",
    "TRAIN_PERCENTAGE = 0.7  # Use 70% of training data for training\n",
    "VALIDATION_PERCENTAGE = 0.3  # Use 30% of training data for validation\n",
    "\n",
    "# Token IDs for Yes/No (these may need adjustment based on the actual tokenizer)\n",
    "YES_TOKEN_ID = None  # Will be determined at runtime\n",
    "NO_TOKEN_ID = None   # Will be determined at runtime"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
