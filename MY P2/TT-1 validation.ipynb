{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeddfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils_validation.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants_validation import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT, TRAIN_PERCENTAGE, VALIDATION_PERCENTAGE\n",
    "import random, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_dataframe_to_train_validation(data_path, mode='train'):\n",
    "    \"\"\"\n",
    "    Modified function to support train/validation split from training data\n",
    "    mode: 'train' to get training portion, 'validation' to get validation portion\n",
    "    \"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split training data into train and validation\n",
    "    train_split, val_split = train_test_split(\n",
    "        train_dataset, \n",
    "        test_size=VALIDATION_PERCENTAGE, \n",
    "        random_state=42, \n",
    "        stratify=train_dataset['rule_violation']\n",
    "    )\n",
    "    \n",
    "    if mode == 'train':\n",
    "        chosen_dataset = train_split\n",
    "        print(f\"Using {len(chosen_dataset)} samples for training ({TRAIN_PERCENTAGE*100:.1f}% of original training data)\")\n",
    "    elif mode == 'validation':\n",
    "        chosen_dataset = val_split\n",
    "        print(f\"Using {len(chosen_dataset)} samples for validation ({VALIDATION_PERCENTAGE*100:.1f}% of original training data)\")\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'train' or 'validation'\")\n",
    "\n",
    "    flatten = []\n",
    "\n",
    "    # ---------- Process chosen dataset ----------\n",
    "    chosen_df = chosen_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                              \"positive_example_1\",\"positive_example_2\",\n",
    "                              \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # Randomly select positive_example and negative_example\n",
    "    chosen_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(chosen_df)) < 0.5,\n",
    "        chosen_df[\"positive_example_1\"],\n",
    "        chosen_df[\"positive_example_2\"]\n",
    "    )\n",
    "    chosen_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(chosen_df)) < 0.5,\n",
    "        chosen_df[\"negative_example_1\"],\n",
    "        chosen_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # Drop original candidate columns\n",
    "    chosen_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                           \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "    flatten.append(chosen_df)\n",
    "\n",
    "    # ---------- Process test dataset (only for training mode) ----------\n",
    "    if mode == 'train':\n",
    "        for violation_type in [\"positive\", \"negative\"]:\n",
    "            for i in range(1, 3):\n",
    "                sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                            \"positive_example_1\",\"positive_example_2\",\n",
    "                                            \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "                if violation_type == \"positive\":\n",
    "                    # body uses current positive_example\n",
    "                    body_col = f\"positive_example_{i}\"\n",
    "                    other_positive_col = f\"positive_example_{3-i}\"  # other positive\n",
    "                    sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                    sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                    # negative_example randomly selected\n",
    "                    sub_dataset[\"negative_example\"] = np.where(\n",
    "                        np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                        sub_dataset[\"negative_example_1\"],\n",
    "                        sub_dataset[\"negative_example_2\"]\n",
    "                    )\n",
    "                    sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "                else:  # violation_type == \"negative\"\n",
    "                    body_col = f\"negative_example_{i}\"\n",
    "                    other_negative_col = f\"negative_example_{3-i}\"\n",
    "                    sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                    sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                    sub_dataset[\"positive_example\"] = np.where(\n",
    "                        np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                        sub_dataset[\"positive_example_1\"],\n",
    "                        sub_dataset[\"positive_example_2\"]\n",
    "                    )\n",
    "                    sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "                # Drop original candidate columns\n",
    "                sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                          \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "                flatten.append(sub_dataset)\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    dataset.to_pandas().to_csv(\"/kaggle/working/dataset_validation.csv\", index=False)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_validation_dataframe_with_labels(data_path):\n",
    "    \"\"\"\n",
    "    Get validation dataframe with true labels for evaluation\n",
    "    \"\"\"\n",
    "    dataframe = get_dataframe_to_train_validation(data_path, mode='validation')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf4d9a8",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluation_metrics.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, confusion_matrix, classification_report, \n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_detailed_metrics(y_true, y_pred_proba, y_pred_binary=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics\n",
    "    \n",
    "    Args:\n",
    "        y_true: True binary labels (0 or 1)\n",
    "        y_pred_proba: Predicted probabilities for positive class\n",
    "        y_pred_binary: Predicted binary labels (optional, will be calculated from proba if not provided)\n",
    "        threshold: Threshold for converting probabilities to binary predictions\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    if y_pred_binary is None:\n",
    "        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred_binary)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    metrics['recall'] = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    metrics['f1_score'] = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    \n",
    "    # AUC metrics\n",
    "    try:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n",
    "        metrics['pr_auc'] = average_precision_score(y_true, y_pred_proba)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Could not calculate AUC metrics: {e}\")\n",
    "        metrics['roc_auc'] = None\n",
    "        metrics['pr_auc'] = None\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    metrics['confusion_matrix'] = cm\n",
    "    \n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        metrics['true_negatives'] = tn\n",
    "        metrics['false_positives'] = fp\n",
    "        metrics['false_negatives'] = fn\n",
    "        metrics['true_positives'] = tp\n",
    "        \n",
    "        # Additional derived metrics\n",
    "        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        metrics['negative_predictive_value'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=['No Violation', 'Violation'], title='Confusion Matrix', normalize=False):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with nice formatting\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "        title += ' (Normalized)'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count' if not normalize else 'Proportion'})\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba, title='ROC Curve'):\n",
    "    \"\"\"\n",
    "    Plot ROC curve\n",
    "    \"\"\"\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        print(\"Cannot plot ROC curve: only one class present in y_true\")\n",
    "        return\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {auc_score:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "             label='Random classifier')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_pred_proba, title='Precision-Recall Curve'):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curve\n",
    "    \"\"\"\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        print(\"Cannot plot PR curve: only one class present in y_true\")\n",
    "        return\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='darkorange', lw=2,\n",
    "             label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "    \n",
    "    # Baseline (random classifier)\n",
    "    baseline = np.sum(y_true) / len(y_true)\n",
    "    plt.axhline(y=baseline, color='navy', linestyle='--', lw=2,\n",
    "                label=f'Random classifier (AP = {baseline:.3f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontweight='bold')\n",
    "    plt.ylabel('Precision', fontweight='bold')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_classification_report(y_true, y_pred_binary, class_names=['No Violation', 'Violation']):\n",
    "    \"\"\"\n",
    "    Print detailed classification report\n",
    "    \"\"\"\n",
    "    report = classification_report(y_true, y_pred_binary, \n",
    "                                 target_names=class_names, \n",
    "                                 digits=4)\n",
    "    print(\"Classification Report:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "def comprehensive_evaluation_report(y_true, y_pred_proba, threshold=0.5, \n",
    "                                  class_names=['No Violation', 'Violation'],\n",
    "                                  model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive evaluation report with all metrics and plots\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPREHENSIVE EVALUATION REPORT: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Calculate binary predictions\n",
    "    y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Get all metrics\n",
    "    metrics = calculate_detailed_metrics(y_true, y_pred_proba, y_pred_binary, threshold)\n",
    "    \n",
    "    # Print summary metrics\n",
    "    print(f\"\\nSUMMARY METRICS (Threshold = {threshold})\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Accuracy:        {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision:       {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:          {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score:        {metrics['f1_score']:.4f}\")\n",
    "    print(f\"ROC AUC:         {metrics['roc_auc']:.4f}\" if metrics['roc_auc'] else \"ROC AUC:         N/A\")\n",
    "    print(f\"PR AUC:          {metrics['pr_auc']:.4f}\" if metrics['pr_auc'] else \"PR AUC:          N/A\")\n",
    "    \n",
    "    if 'specificity' in metrics:\n",
    "        print(f\"Specificity:     {metrics['specificity']:.4f}\")\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    print(f\"\\n\")\n",
    "    print_classification_report(y_true, y_pred_binary, class_names)\n",
    "    \n",
    "    # Print confusion matrix details\n",
    "    if 'true_positives' in metrics:\n",
    "        print(f\"\\nCONFUSION MATRIX BREAKDOWN\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"True Positives:  {metrics['true_positives']}\")\n",
    "        print(f\"True Negatives:  {metrics['true_negatives']}\")\n",
    "        print(f\"False Positives: {metrics['false_positives']}\")\n",
    "        print(f\"False Negatives: {metrics['false_negatives']}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(metrics['confusion_matrix'], class_names, \n",
    "                         f'Confusion Matrix - {model_name}')\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "    plot_confusion_matrix(metrics['confusion_matrix'], class_names, \n",
    "                         f'Confusion Matrix - {model_name}', normalize=True)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    if metrics['roc_auc']:\n",
    "        plot_roc_curve(y_true, y_pred_proba, f'ROC Curve - {model_name}')\n",
    "    \n",
    "    # Plot PR curve\n",
    "    if metrics['pr_auc']:\n",
    "        plot_precision_recall_curve(y_true, y_pred_proba, f'Precision-Recall Curve - {model_name}')\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d22de",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_validation.py\n",
    "import pandas as pd\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils_validation import build_dataset, get_dataframe_to_train_validation\n",
    "from constants_validation import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Starting training with validation split...\")\n",
    "    \n",
    "    # Get training portion of the data\n",
    "    dataframe = get_dataframe_to_train_validation(DATA_PATH, mode='train')\n",
    "    train_dataset = build_dataset(dataframe)\n",
    "    \n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=1,\n",
    "        \n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=1e-4, #keep high, lora usually likes high. \n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        BASE_MODEL_PATH,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(LORA_PATH)\n",
    "    print(f\"âœ… Training completed! Model saved to {LORA_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c477448",
   "metadata": {},
   "source": [
    "## Validation Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c318e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_validation.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "from utils_validation import build_dataset, get_validation_dataframe_with_labels\n",
    "from constants_validation import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "from evaluation_metrics import comprehensive_evaluation_report\n",
    "\n",
    "\n",
    "def run_validation_inference():\n",
    "    \"\"\"Run inference on validation set and return predictions with true labels\"\"\"\n",
    "    \n",
    "    print(\"Loading validation dataset...\")\n",
    "    validation_df = get_validation_dataframe_with_labels(DATA_PATH)\n",
    "    \n",
    "    # Build dataset for inference (only prompts needed)\n",
    "    validation_dataset = build_dataset(validation_df)\n",
    "    texts = validation_dataset[\"prompt\"]\n",
    "    \n",
    "    # Get true labels\n",
    "    y_true = validation_df[\"rule_violation\"].values\n",
    "    \n",
    "    print(f\"Validation set size: {len(texts)}\")\n",
    "    print(f\"Class distribution: {np.bincount(y_true)}\")\n",
    "    \n",
    "    print(\"Initializing vLLM model...\")\n",
    "    llm = vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        quantization=\"gptq\",\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.98,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2836,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "    )\n",
    "    \n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n",
    "    \n",
    "    print(\"Running inference...\")\n",
    "    outputs = llm.generate(\n",
    "        texts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n",
    "    )\n",
    "    \n",
    "    # Extract predictions and probabilities\n",
    "    log_probs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "    \n",
    "    predictions_df = pd.DataFrame(log_probs)\n",
    "    \n",
    "    # Convert log probabilities to probabilities\n",
    "    yes_logprobs = predictions_df[POSITIVE_ANSWER].values\n",
    "    no_logprobs = predictions_df[NEGATIVE_ANSWER].values\n",
    "    \n",
    "    # Calculate softmax to get proper probabilities\n",
    "    yes_probs = np.exp(yes_logprobs)\n",
    "    no_probs = np.exp(no_logprobs)\n",
    "    total_probs = yes_probs + no_probs\n",
    "    \n",
    "    y_pred_proba = yes_probs / total_probs  # Probability of positive class\n",
    "    \n",
    "    print(\"âœ… Validation inference completed!\")\n",
    "    \n",
    "    return y_true, y_pred_proba, validation_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Starting validation evaluation...\")\n",
    "    \n",
    "    # Run inference and get results\n",
    "    y_true, y_pred_proba, validation_df = run_validation_inference()\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = validation_df.copy()\n",
    "    results_df['predicted_probability'] = y_pred_proba\n",
    "    results_df['predicted_binary'] = (y_pred_proba >= 0.5).astype(int)\n",
    "    results_df.to_csv(\"/kaggle/working/validation_results.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"VALIDATION RESULTS SAVED TO: /kaggle/working/validation_results.csv\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate comprehensive evaluation report\n",
    "    metrics = comprehensive_evaluation_report(\n",
    "        y_true, \n",
    "        y_pred_proba, \n",
    "        threshold=0.5,\n",
    "        class_names=['No Violation', 'Violation'],\n",
    "        model_name=\"Qwen 2.5 0.5B (TT-1 Validation)\"\n",
    "    )\n",
    "    \n",
    "    # Test different thresholds\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"THRESHOLD ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    threshold_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "        from evaluation_metrics import calculate_detailed_metrics\n",
    "        metrics_th = calculate_detailed_metrics(y_true, y_pred_proba, y_pred_binary, threshold)\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': threshold,\n",
    "            'accuracy': metrics_th['accuracy'],\n",
    "            'precision': metrics_th['precision'],\n",
    "            'recall': metrics_th['recall'],\n",
    "            'f1_score': metrics_th['f1_score']\n",
    "        })\n",
    "        \n",
    "        print(f\"Threshold {threshold}: Acc={metrics_th['accuracy']:.3f}, \"\n",
    "              f\"Prec={metrics_th['precision']:.3f}, Rec={metrics_th['recall']:.3f}, \"\n",
    "              f\"F1={metrics_th['f1_score']:.3f}\")\n",
    "    \n",
    "    # Save threshold analysis\n",
    "    threshold_df = pd.DataFrame(threshold_results)\n",
    "    threshold_df.to_csv(\"/kaggle/working/threshold_analysis.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Validation evaluation completed!\")\n",
    "    print(f\"ðŸ“Š Results saved to /kaggle/working/validation_results.csv\")\n",
    "    print(f\"ðŸ“ˆ Threshold analysis saved to /kaggle/working/threshold_analysis.csv\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f89d86",
   "metadata": {},
   "source": [
    "## Accelerate Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e7d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accelerate_config_validation.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 64\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e2b5a",
   "metadata": {},
   "source": [
    "## Execution: Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run training on the controlled portion of training data\n",
    "!accelerate launch --config_file accelerate_config_validation.yaml train_validation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Run validation inference and get detailed evaluation metrics\n",
    "!python inference_validation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea420dc9",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eef9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine validation results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load validation results\n",
    "validation_results = pd.read_csv('/kaggle/working/validation_results.csv')\n",
    "threshold_analysis = pd.read_csv('/kaggle/working/threshold_analysis.csv')\n",
    "\n",
    "print(\"Validation Results Overview:\")\n",
    "print(f\"Total validation samples: {len(validation_results)}\")\n",
    "print(f\"Actual class distribution:\")\n",
    "print(validation_results['rule_violation'].value_counts().sort_index())\n",
    "print(f\"\\nPredicted class distribution (threshold=0.5):\")\n",
    "print(validation_results['predicted_binary'].value_counts().sort_index())\n",
    "\n",
    "# Display sample of results\n",
    "print(f\"\\nSample of validation results:\")\n",
    "print(validation_results[['body', 'rule_violation', 'predicted_probability', 'predicted_binary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create subplots for different metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot accuracy vs threshold\n",
    "axes[0, 0].plot(threshold_analysis['threshold'], threshold_analysis['accuracy'], 'o-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_title('Accuracy vs Threshold', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Threshold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot precision vs threshold\n",
    "axes[0, 1].plot(threshold_analysis['threshold'], threshold_analysis['precision'], 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[0, 1].set_title('Precision vs Threshold', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Threshold')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot recall vs threshold\n",
    "axes[1, 0].plot(threshold_analysis['threshold'], threshold_analysis['recall'], 'o-', linewidth=2, markersize=8, color='green')\n",
    "axes[1, 0].set_title('Recall vs Threshold', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Threshold')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot F1-score vs threshold\n",
    "axes[1, 1].plot(threshold_analysis['threshold'], threshold_analysis['f1_score'], 'o-', linewidth=2, markersize=8, color='red')\n",
    "axes[1, 1].set_title('F1-Score vs Threshold', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Threshold')\n",
    "axes[1, 1].set_ylabel('F1-Score')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Threshold Analysis for TT-1 Validation', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Display threshold analysis table\n",
    "print(\"\\nThreshold Analysis:\")\n",
    "print(threshold_analysis.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae718d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Analysis: Prediction Distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Prediction probability distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(validation_results['predicted_probability'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Predicted Probabilities', fontweight='bold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Prediction probabilities by true class\n",
    "plt.subplot(1, 3, 2)\n",
    "no_violation = validation_results[validation_results['rule_violation'] == 0]['predicted_probability']\n",
    "violation = validation_results[validation_results['rule_violation'] == 1]['predicted_probability']\n",
    "\n",
    "plt.hist(no_violation, bins=30, alpha=0.7, label='No Violation (True)', color='blue', edgecolor='black')\n",
    "plt.hist(violation, bins=30, alpha=0.7, label='Violation (True)', color='red', edgecolor='black')\n",
    "plt.title('Predicted Probabilities by True Class', fontweight='bold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Box plot of predictions by true class\n",
    "plt.subplot(1, 3, 3)\n",
    "data_for_boxplot = [no_violation, violation]\n",
    "labels = ['No Violation', 'Violation']\n",
    "plt.boxplot(data_for_boxplot, labels=labels)\n",
    "plt.title('Prediction Distribution by True Class', fontweight='bold')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display calibration metrics\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Reliability diagram (calibration curve)\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    validation_results['rule_violation'], \n",
    "    validation_results['predicted_probability'], \n",
    "    n_bins=10\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Model\")\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Plot (Reliability Diagram)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\\\nSummary Statistics:\")\n",
    "print(f\"Mean predicted probability: {validation_results['predicted_probability'].mean():.4f}\")\n",
    "print(f\"Std predicted probability: {validation_results['predicted_probability'].std():.4f}\")\n",
    "print(f\"Min predicted probability: {validation_results['predicted_probability'].min():.4f}\")\n",
    "print(f\"Max predicted probability: {validation_results['predicted_probability'].max():.4f}\")\n",
    "\n",
    "# Most confident correct and incorrect predictions\n",
    "correct_predictions = validation_results[validation_results['rule_violation'] == validation_results['predicted_binary']]\n",
    "incorrect_predictions = validation_results[validation_results['rule_violation'] != validation_results['predicted_binary']]\n",
    "\n",
    "print(f\"\\\\nModel Performance Summary:\")\n",
    "print(f\"Correct predictions: {len(correct_predictions)} ({len(correct_predictions)/len(validation_results)*100:.1f}%)\")\n",
    "print(f\"Incorrect predictions: {len(incorrect_predictions)} ({len(incorrect_predictions)/len(validation_results)*100:.1f}%)\")\n",
    "\n",
    "if len(incorrect_predictions) > 0:\n",
    "    print(f\"\\\\nMost confident incorrect predictions:\")\n",
    "    # For incorrect predictions, show those with highest confidence (furthest from 0.5)\n",
    "    incorrect_predictions['confidence'] = np.abs(incorrect_predictions['predicted_probability'] - 0.5)\n",
    "    most_confident_wrong = incorrect_predictions.nlargest(3, 'confidence')\n",
    "    for idx, row in most_confident_wrong.iterrows():\n",
    "        print(f\"True: {row['rule_violation']}, Pred: {row['predicted_binary']}, Prob: {row['predicted_probability']:.3f}\")\n",
    "        print(f\"Comment: {row['body'][:100]}...\")\n",
    "        print(f\"Rule: {row['rule'][:100]}...\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6527c38",
   "metadata": {},
   "source": [
    "## Configuration Summary\n",
    "\n",
    "This validation notebook provides a comprehensive evaluation of the TT-1 Qwen 2.5 0.5B model:\n",
    "\n",
    "### Key Features:\n",
    "1. **Controlled Training Split**: Uses 70% of training data for training, 30% for validation\n",
    "2. **Comprehensive Metrics**: AUC, confusion matrix, classification report, ROC curve, precision-recall curve\n",
    "3. **Threshold Analysis**: Tests multiple thresholds to find optimal operating point\n",
    "4. **Calibration Analysis**: Evaluates how well predicted probabilities match actual outcomes\n",
    "5. **Error Analysis**: Identifies most confident incorrect predictions for model improvement\n",
    "\n",
    "### Configuration:\n",
    "- **Training Percentage**: 70% of original training data\n",
    "- **Validation Percentage**: 30% of original training data  \n",
    "- **Model**: Qwen 2.5 0.5B with GPTQ quantization\n",
    "- **LoRA Configuration**: r=16, alpha=32, dropout=0.1\n",
    "- **Training**: 1 epoch with paged_adamw_8bit optimizer\n",
    "\n",
    "### Output Files:\n",
    "- `validation_results.csv`: Detailed per-sample results with predictions and probabilities\n",
    "- `threshold_analysis.csv`: Performance metrics across different thresholds\n",
    "- Comprehensive visualizations and analysis charts\n",
    "\n",
    "This setup allows for proper validation without data leakage and provides insights for model improvement and deployment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058ddd9",
   "metadata": {},
   "source": [
    "# TT-1 Validation Notebook: Qwen 2.5 0.5B Model\n",
    "\n",
    "This notebook trains on a controlled percentage of training data and validates on the remaining portion.\n",
    "It provides detailed evaluation metrics including:\n",
    "- AUC Score\n",
    "- Confusion Matrix \n",
    "- Classification Report\n",
    "- ROC Curve\n",
    "- Precision-Recall Curve\n",
    "\n",
    "**Training Strategy:**\n",
    "- Train on X% of training data\n",
    "- Validate on (100-X)% of training data\n",
    "- Show comprehensive evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f6fe6",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5282fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n",
    "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc56a3d",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants_validation.py\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\n",
    "LORA_PATH = \"output_validation/\"\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''\n",
    "\n",
    "# Validation specific settings\n",
    "TRAIN_PERCENTAGE = 0.7  # Use 70% of training data for training\n",
    "VALIDATION_PERCENTAGE = 0.3  # Use 30% of training data for validation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
