{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc28b0e",
   "metadata": {},
   "source": [
    "\n",
    "### References\n",
    "\n",
    "*   [https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876](https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876)\n",
    "*   [https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo](https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo)\n",
    "*   [https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/](https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/)\n",
    "*   https://www.kaggle.com/code/mks2192/jigsaw-llama3-1-8b-instruct-training-one-epoch\n",
    "*   [https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference](https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference)\n",
    "*   https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc158d34",
   "metadata": {},
   "source": [
    "### I want to say thanks to @neibyr for your interesting idea: [Retrieve by Qwen3Embedding](http://https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278afcc3",
   "metadata": {},
   "source": [
    "# Original Notebook(1)\n",
    "https://www.kaggle.com/code/kishanvavdara/test-on-testdataset-qwenemdding-llama-lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb511812",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "This notebook is offline install ver of original notebook(1) with the 14B model omitted for submission.\n",
    "\n",
    "vllm==0.10.0 is installed internet-offline by jigsaw-packages2.\n",
    "\n",
    "**Note: This submission uses only the 0.5B Qwen model and 0.6B embedding model (14B model omitted).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n",
    "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b777bc0",
   "metadata": {},
   "source": [
    "# 1. Test time train Qwen 2.5 0.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants.py\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\n",
    "LORA_PATH = \"output/\"\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\n",
    "import random, numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "\n",
    "    # ---------- 处理训练集 ----------\n",
    "    train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                              \"positive_example_1\",\"positive_example_2\",\n",
    "                              \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # 随机选 positive_example 和 negative_example\n",
    "    train_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"positive_example_1\"],\n",
    "        train_df[\"positive_example_2\"]\n",
    "    )\n",
    "    train_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"negative_example_1\"],\n",
    "        train_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # 删除原来的候选列\n",
    "    train_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                           \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "    flatten.append(train_df)\n",
    "\n",
    "    # ---------- 处理测试集 ----------\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                        \"positive_example_1\",\"positive_example_2\",\n",
    "                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                # body 用当前 positive_example\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"  # 另一个 positive\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                # negative_example 随机选\n",
    "                sub_dataset[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"negative_example_1\"],\n",
    "                    sub_dataset[\"negative_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "            else:  # violation_type == \"negative\"\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                sub_dataset[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"positive_example_1\"],\n",
    "                    sub_dataset[\"positive_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "            # 删除原来的候选列\n",
    "            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    # 合并所有 DataFrame\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import pandas as pd\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_dataframe_to_train\n",
    "from constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    train_dataset = build_dataset(dataframe)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=1,\n",
    "        \n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=1e-4, #keep high, lora usually likes high. \n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        BASE_MODEL_PATH,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(LORA_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6540c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "from utils import build_dataset\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def run_inference_on_device(df_slice):\n",
    "    \"\"\"在当前进程可见的 GPU 上跑 vLLM 推理\"\"\"\n",
    "    llm = vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        quantization=\"gptq\",\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.98,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2836,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n",
    "\n",
    "    test_dataset = build_dataset(df_slice)\n",
    "    texts = test_dataset[\"prompt\"]\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        texts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n",
    "    )\n",
    "\n",
    "    log_probs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "    predictions = pd.DataFrame(log_probs)[[POSITIVE_ANSWER, NEGATIVE_ANSWER]]\n",
    "    predictions[\"row_id\"] = df_slice[\"row_id\"].values\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def worker(device_id, df_slice, return_dict):\n",
    "    # 限制该进程只看到一张 GPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n",
    "    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n",
    "\n",
    "    preds = run_inference_on_device(df_slice)\n",
    "    return_dict[device_id] = preds\n",
    "\n",
    "\n",
    "def main():\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "\n",
    "    # 随机选择例子\n",
    "    test_dataframe[\"positive_example\"] = test_dataframe.apply(\n",
    "        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]),\n",
    "        axis=1\n",
    "    )\n",
    "    test_dataframe[\"negative_example\"] = test_dataframe.apply(\n",
    "        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]),\n",
    "        axis=1\n",
    "    )\n",
    "    test_dataframe = test_dataframe.drop(\n",
    "        columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    # 切分数据\n",
    "    mid = len(test_dataframe) // 2\n",
    "    df0 = test_dataframe.iloc[:mid].reset_index(drop=True)\n",
    "    df1 = test_dataframe.iloc[mid:].reset_index(drop=True)\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "\n",
    "    # 两个进程并行\n",
    "    p0 = mp.Process(target=worker, args=(0, df0, return_dict))\n",
    "    p1 = mp.Process(target=worker, args=(1, df1, return_dict))\n",
    "    p0.start()\n",
    "    p1.start()\n",
    "    p0.join()\n",
    "    p1.join()\n",
    "\n",
    "    # 合并结果\n",
    "    predictions = pd.concat([return_dict[0], return_dict[1]], ignore_index=True)\n",
    "\n",
    "    # 构建 submission\n",
    "    submission = predictions[[\"row_id\", POSITIVE_ANSWER]].rename(columns={POSITIVE_ANSWER: \"rule_violation\"})\n",
    "    rq = submission['rule_violation'].rank(method='average') / (len(submission) + 1)\n",
    "    submission['rule_violation'] = rq\n",
    "\n",
    "    submission.to_csv(\"submission_qwen.csv\", index=False)\n",
    "    print(\"✅ Saved submission_qwen.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 64\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch --config_file accelerate_config.yaml train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9bc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission_qwen.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb951b",
   "metadata": {},
   "source": [
    "# 2. Qwen3 0.6b Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc45d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants.py\n",
    "# Choose your embedding model (uncomment one):\n",
    "# Option 1: Original Qwen3 0.6B (larger, potentially better)\n",
    "# EMBEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\n",
    "# MODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\n",
    "# USE_CUSTOM_EMBEDDING = True\n",
    "\n",
    "# Option 2: Lightweight alternatives (much smaller, faster)\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # 22.7M params, ~90MB\n",
    "# EMBEDDING_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"  # 109M params, ~420MB  \n",
    "# EMBEDDING_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"  # 33.4M params, ~130MB\n",
    "# EMBEDDING_MODEL_NAME = \"intfloat/e5-small-v2\"  # 33.4M params, ~130MB\n",
    "USE_CUSTOM_EMBEDDING = False\n",
    "\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "\n",
    "# Embedding search parameters\n",
    "EMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:\"\n",
    "CLEAN_TEXT = True\n",
    "TOP_K = 2000\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9cc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "import torch.distributed as dist\n",
    "\n",
    "from datasets import Dataset\n",
    "from cleantext import clean\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from constants import CLEAN_TEXT\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n",
    "\n",
    "\n",
    "def cleaner(text):\n",
    "    return clean(\n",
    "        text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=True,\n",
    "        lower=False,\n",
    "        no_line_breaks=False,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=False,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        lang=\"en\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.6, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "    flatten.append(train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\"]])\n",
    "    \n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[f\"{violation_type}_example_{i}\", \"rule\", \"subreddit\"]].copy()\n",
    "            sub_dataset = sub_dataset.rename(columns={f\"{violation_type}_example_{i}\": \"body\"})\n",
    "            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    dataframe = pd.concat(flatten, axis=0)    \n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def prepare_dataframe(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    \n",
    "    if CLEAN_TEXT:\n",
    "        tqdm.pandas(desc=\"cleaner\")\n",
    "        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n",
    "\n",
    "    if \"rule_violation\" in dataframe.columns:\n",
    "        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: 1,\n",
    "                0: -1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c292ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile semantic.py\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search, dot_score\n",
    "from tqdm.auto import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "from utils import get_dataframe_to_train, prepare_dataframe\n",
    "from constants import DATA_PATH, EMBDEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH\n",
    "\n",
    "\n",
    "\n",
    "def get_scores(test_dataframe):\n",
    "    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "    \n",
    "    # Load adapter configuration and model\n",
    "    adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "    lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "    merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "\n",
    "    # 4. Tạo lại SentenceTransformer từ encoder đã merge\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n",
    "\n",
    "    print('Done loading model!')\n",
    "\n",
    "    result = []\n",
    "    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=f\"Generate scores for each rule\"):\n",
    "        test_dataframe_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_dataframe_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_dataframe_part = corpus_dataframe_part.reset_index(names=\"row_id\")\n",
    "        \n",
    "        query_embeddings = embedding_model.encode(\n",
    "            sentences=test_dataframe_part[\"prompt\"].tolist(),\n",
    "            prompt=EMBEDDING_MODEL_QUERY,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        document_embeddings = embedding_model.encode(\n",
    "            sentences=corpus_dataframe_part[\"prompt\"].tolist(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        test_dataframe_part[\"semantic\"] = semantic_search(\n",
    "            query_embeddings,\n",
    "            document_embeddings,\n",
    "            top_k=TOP_K,\n",
    "            score_function=dot_score,\n",
    "        )\n",
    "        def get_score(semantic):\n",
    "            semantic = pd.DataFrame(semantic)\n",
    "            semantic = semantic.merge(\n",
    "                corpus_dataframe_part[[\"row_id\", \"rule_violation\"]],\n",
    "                how=\"left\",\n",
    "                left_on=\"corpus_id\",\n",
    "                right_on=\"row_id\",\n",
    "            )\n",
    "            semantic[\"score\"] = semantic[\"score\"]*semantic[\"rule_violation\"]\n",
    "            return semantic[\"score\"].sum()\n",
    "            \n",
    "        tqdm.pandas(desc=f\"Add label for {rule=}\")\n",
    "        test_dataframe_part[\"rule_violation\"] = test_dataframe_part[\"semantic\"].progress_apply(get_score)\n",
    "        result.append(test_dataframe_part[[\"row_id\", \"rule_violation\"]].copy())\n",
    "        \n",
    "    submission = pd.concat(result, axis=0)\n",
    "    return submission\n",
    "\n",
    "\n",
    "def generate_submission():\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    test_dataframe = prepare_dataframe(test_dataframe)\n",
    "    \n",
    "    submission = get_scores(test_dataframe)\n",
    "    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n",
    "    submission.to_csv(\"submission_qwen3.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_submission()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python semantic.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94b239",
   "metadata": {},
   "source": [
    "# 3. ENSEMBLE RESULT (Two Models Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load only two submissions (omitting 14B model)\n",
    "q = pd.read_csv('submission_qwen.csv')     # 0.5B model\n",
    "l = pd.read_csv('submission_qwen3.csv')    # 0.6B embedding model\n",
    "\n",
    "# Rank the predictions\n",
    "rq = q['rule_violation'].rank(method='average') / (len(q)+1)\n",
    "rl = l['rule_violation'].rank(method='average') / (len(l)+1)\n",
    "\n",
    "# Blend only two models (adjusted weights since we're omitting the 14B model)\n",
    "# Original was: 0.5*rq + 0.3*rl + 0.2*rm (0.5B + embedding + 14B)\n",
    "# Modified to: 0.7*rq + 0.3*rl (0.5B + embedding only)\n",
    "blend = 0.7*rq + 0.3*rl\n",
    "\n",
    "q['rule_violation'] = blend\n",
    "q.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print(\"✅ Final submission created using 0.5B Qwen model + 0.6B embedding model (14B model omitted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e0e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('/kaggle/working/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190ee88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
