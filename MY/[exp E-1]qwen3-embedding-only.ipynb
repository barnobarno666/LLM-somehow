{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f136c4e",
   "metadata": {},
   "source": [
    "# E means Embedding model only\n",
    "This notebook contains only the Qwen3 0.6B embedding model for semantic search classification.\n",
    "\n",
    "**Note: This submission uses only the 0.6B Qwen3 embedding model for standalone classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36044bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n",
    "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf18aa",
   "metadata": {},
   "source": [
    "# 1. Qwen3 0.6b Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants.py\n",
    "# Choose your embedding model (uncomment one):\n",
    "# Option 1: Original Qwen3 0.6B (larger, potentially better)\n",
    "EMBEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\n",
    "MODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\n",
    "USE_CUSTOM_EMBEDDING = True\n",
    "\n",
    "# Option 2: Lightweight alternatives (much smaller, faster) - uncomment to use\n",
    "# EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # 22.7M params, ~90MB\n",
    "# EMBEDDING_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"  # 109M params, ~420MB  \n",
    "# EMBEDDING_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"  # 33.4M params, ~130MB\n",
    "# EMBEDDING_MODEL_NAME = \"intfloat/e5-small-v2\"  # 33.4M params, ~130MB\n",
    "# USE_CUSTOM_EMBEDDING = False\n",
    "\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "\n",
    "# Embedding search parameters\n",
    "EMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:\"\n",
    "CLEAN_TEXT = True\n",
    "TOP_K = 2000\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "import torch.distributed as dist\n",
    "\n",
    "from datasets import Dataset\n",
    "from cleantext import clean\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from constants import CLEAN_TEXT\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n",
    "\n",
    "\n",
    "def cleaner(text):\n",
    "    return clean(\n",
    "        text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=True,\n",
    "        lower=False,\n",
    "        no_line_breaks=False,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=False,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        lang=\"en\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.6, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "    flatten.append(train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\"]])\n",
    "    \n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[f\"{violation_type}_example_{i}\", \"rule\", \"subreddit\"]].copy()\n",
    "            sub_dataset = sub_dataset.rename(columns={f\"{violation_type}_example_{i}\": \"body\"})\n",
    "            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    dataframe = pd.concat(flatten, axis=0)    \n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def prepare_dataframe(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    \n",
    "    if CLEAN_TEXT:\n",
    "        tqdm.pandas(desc=\"cleaner\")\n",
    "        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n",
    "\n",
    "    if \"rule_violation\" in dataframe.columns:\n",
    "        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: 1,\n",
    "                0: -1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9eef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile semantic.py\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search, dot_score\n",
    "from tqdm.auto import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "from utils import get_dataframe_to_train, prepare_dataframe\n",
    "from constants import DATA_PATH, EMBEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH, USE_CUSTOM_EMBEDDING\n",
    "\n",
    "\n",
    "\n",
    "def get_scores(test_dataframe):\n",
    "    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n",
    "    \n",
    "    if USE_CUSTOM_EMBEDDING:\n",
    "        # Load base model\n",
    "        model = AutoModelForCausalLM.from_pretrained(EMBEDDING_MODEL_PATH)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_PATH)\n",
    "        \n",
    "        # Load adapter configuration and model\n",
    "        adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "        lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n",
    "        merged_model = lora_model.merge_and_unload()\n",
    "        tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "        merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "\n",
    "        # Create SentenceTransformer from merged encoder\n",
    "        embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n",
    "    else:\n",
    "        # Use lightweight model\n",
    "        from constants import EMBEDDING_MODEL_NAME\n",
    "        embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=\"cuda\")\n",
    "\n",
    "    print('Done loading model!')\n",
    "\n",
    "    result = []\n",
    "    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=f\"Generate scores for each rule\"):\n",
    "        test_dataframe_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_dataframe_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_dataframe_part = corpus_dataframe_part.reset_index(names=\"row_id\")\n",
    "        \n",
    "        # Encode embeddings\n",
    "        if USE_CUSTOM_EMBEDDING:\n",
    "            query_embeddings = embedding_model.encode(\n",
    "                sentences=test_dataframe_part[\"prompt\"].tolist(),\n",
    "                prompt=EMBEDDING_MODEL_QUERY,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_tensor=True,\n",
    "                device=\"cuda\",\n",
    "                normalize_embeddings=True,\n",
    "            )\n",
    "        else:\n",
    "            query_embeddings = embedding_model.encode(\n",
    "                sentences=test_dataframe_part[\"prompt\"].tolist(),\n",
    "                batch_size=BATCH_SIZE,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_tensor=True,\n",
    "                device=\"cuda\",\n",
    "                normalize_embeddings=True,\n",
    "            )\n",
    "            \n",
    "        document_embeddings = embedding_model.encode(\n",
    "            sentences=corpus_dataframe_part[\"prompt\"].tolist(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        \n",
    "        test_dataframe_part[\"semantic\"] = semantic_search(\n",
    "            query_embeddings,\n",
    "            document_embeddings,\n",
    "            top_k=TOP_K,\n",
    "            score_function=dot_score,\n",
    "        )\n",
    "        \n",
    "        def get_score(semantic):\n",
    "            semantic = pd.DataFrame(semantic)\n",
    "            semantic = semantic.merge(\n",
    "                corpus_dataframe_part[[\"row_id\", \"rule_violation\"]],\n",
    "                how=\"left\",\n",
    "                left_on=\"corpus_id\",\n",
    "                right_on=\"row_id\",\n",
    "            )\n",
    "            semantic[\"score\"] = semantic[\"score\"]*semantic[\"rule_violation\"]\n",
    "            return semantic[\"score\"].sum()\n",
    "            \n",
    "        tqdm.pandas(desc=f\"Add label for {rule=}\")\n",
    "        test_dataframe_part[\"rule_violation\"] = test_dataframe_part[\"semantic\"].progress_apply(get_score)\n",
    "        result.append(test_dataframe_part[[\"row_id\", \"rule_violation\"]].copy())\n",
    "        \n",
    "    submission = pd.concat(result, axis=0)\n",
    "    return submission\n",
    "\n",
    "\n",
    "def generate_submission():\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    test_dataframe = prepare_dataframe(test_dataframe)\n",
    "    \n",
    "    submission = get_scores(test_dataframe)\n",
    "    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n",
    "    \n",
    "    # Rank normalize the predictions\n",
    "    rq = submission['rule_violation'].rank(method='average') / (len(submission) + 1)\n",
    "    submission['rule_violation'] = rq\n",
    "    \n",
    "    submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
    "    print(\"✅ Saved submission.csv using Qwen3 0.6B embedding model only\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python semantic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /kaggle/working/submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c765dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('/kaggle/working/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
