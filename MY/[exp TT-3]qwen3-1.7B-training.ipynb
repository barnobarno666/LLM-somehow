{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e4aff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97ae4795",
   "metadata": {},
   "source": [
    "# Qwen3 1.7B Training Notebook (DoRA Fine-tuning)\n",
    "\n",
    "This notebook fine-tunes Qwen3-1.7B-GPTQ-Int8 on training data only using DoRA and saves the model for later use.\n",
    "\n",
    "**Key Changes from Original:**\n",
    "- Uses Qwen3-1.7B-GPTQ-Int8 instead of Qwen2.5-0.5B\n",
    "- Uses DoRA instead of LoRA (use_dora=True)\n",
    "- Trains only on training data (no test-time training)\n",
    "- Saves the fine-tuned model for later loading\n",
    "\n",
    "**DoRA Settings Changed:**\n",
    "- `use_dora=True` - Enables DoRA instead of LoRA\n",
    "- `r=32` - Increased rank for better capacity (tunable: 16-64)\n",
    "- `lora_alpha=64` - Scaled proportionally with rank\n",
    "- `lora_dropout=0.05` - Reduced dropout for stability (tunable: 0.05-0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbdcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies - DoRA requires newer PEFT version\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n",
    "# Install latest PEFT for DoRA support (ensure v0.10.0+)\n",
    "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'\n",
    "\n",
    "# If DoRA is not supported in the above, you may need to install from source:\n",
    "# !pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56341a7a",
   "metadata": {},
   "source": [
    "# 1. Train Qwen3 1.7B on Training Data Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants.py\n",
    "# Changed: Using Qwen3 1.7B GPTQ Int8 model\n",
    "BASE_MODEL_PATH = \"Qwen/Qwen3-1.7B-GPTQ-Int8\"  # Note: This needs to be available on Kaggle\n",
    "LORA_PATH = \"qwen3_1.7b_dora_output/\"  # Changed: Different output path\n",
    "FINAL_MODEL_PATH = \"qwen3_1.7b_finetuned/\"  # New: Path for merged final model\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are a moderator of subreddit.  given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\n",
    "import random, numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_dataframe_to_train(data_path, training_only=True):\n",
    "    \"\"\"Modified: Only use training data when training_only=True\"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    \n",
    "    flatten = []\n",
    "\n",
    "    # ---------- Process training data ----------\n",
    "    train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                              \"positive_example_1\",\"positive_example_2\",\n",
    "                              \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # Randomly select positive_example and negative_example\n",
    "    train_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"positive_example_1\"],\n",
    "        train_df[\"positive_example_2\"]\n",
    "    )\n",
    "    train_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"negative_example_1\"],\n",
    "        train_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # Drop original candidate columns\n",
    "    train_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                           \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "    flatten.append(train_df)\n",
    "    \n",
    "    # Changed: Skip test data processing when training_only=True\n",
    "    if not training_only:\n",
    "        test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # ---------- Process test data ----------\n",
    "        for violation_type in [\"positive\", \"negative\"]:\n",
    "            for i in range(1, 3):\n",
    "                sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                            \"positive_example_1\",\"positive_example_2\",\n",
    "                                            \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "                if violation_type == \"positive\":\n",
    "                    body_col = f\"positive_example_{i}\"\n",
    "                    other_positive_col = f\"positive_example_{3-i}\"\n",
    "                    sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                    sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                    sub_dataset[\"negative_example\"] = np.where(\n",
    "                        np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                        sub_dataset[\"negative_example_1\"],\n",
    "                        sub_dataset[\"negative_example_2\"]\n",
    "                    )\n",
    "                    sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "                else:  # violation_type == \"negative\"\n",
    "                    body_col = f\"negative_example_{i}\"\n",
    "                    other_negative_col = f\"negative_example_{3-i}\"\n",
    "                    sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                    sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                    sub_dataset[\"positive_example\"] = np.where(\n",
    "                        np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                        sub_dataset[\"positive_example_1\"],\n",
    "                        sub_dataset[\"positive_example_2\"]\n",
    "                    )\n",
    "                    sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "                sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                          \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "                flatten.append(sub_dataset)\n",
    "\n",
    "    # Merge all DataFrames\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    dataset.to_pandas().to_csv(\"/kaggle/working/training_dataset.csv\", index=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa63d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel  # Added PeftModel for saving\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_dataframe_to_train\n",
    "from constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH, FINAL_MODEL_PATH\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Changed: Only use training data (training_only=True)\n",
    "    dataframe = get_dataframe_to_train(DATA_PATH, training_only=True)\n",
    "    train_dataset = build_dataset(dataframe)\n",
    "    \n",
    "    print(f\"Training dataset size: {len(train_dataset)} samples\")\n",
    "    \n",
    "    # Changed: DoRA configuration with optimized settings\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,  # Increased rank for 1.7B model (tunable: 16-64)\n",
    "        lora_alpha=64,  # Scaled proportionally (tunable: 32-128)\n",
    "        lora_dropout=0.05,  # Reduced for stability (tunable: 0.05-0.15)\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        use_dora=True,  # KEY CHANGE: Enable DoRA instead of LoRA\n",
    "    )\n",
    "    \n",
    "    # Changed: Adjusted training config for 1.7B model\n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=2,  # Increased epochs since no test-time training (tunable: 1-3)\n",
    "        \n",
    "        per_device_train_batch_size=2,  # Reduced for 1.7B model (tunable: 1-4)\n",
    "        gradient_accumulation_steps=8,  # Increased to maintain effective batch size (tunable: 4-16)\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=8e-5,  # Slightly reduced for larger model (tunable: 5e-5 - 1e-4)\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,  # Slightly increased warmup (tunable: 0.03-0.1)\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"epoch\",  # Changed: Save after each epoch\n",
    "        save_steps=500,  # Additional save steps (tunable)\n",
    "        output_dir=LORA_PATH,\n",
    "        logging_steps=50,  # Added logging (tunable: 10-100)\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        BASE_MODEL_PATH,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting DoRA training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the LoRA/DoRA adapters\n",
    "    trainer.save_model(LORA_PATH)\n",
    "    print(f\"DoRA adapters saved to: {LORA_PATH}\")\n",
    "    \n",
    "    # NEW: Merge and save the final model for easier loading\n",
    "    print(\"Merging DoRA adapters with base model...\")\n",
    "    \n",
    "    # Load base model and tokenizer\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n",
    "    \n",
    "    # Load and merge LoRA/DoRA adapters\n",
    "    peft_model = PeftModel.from_pretrained(base_model, LORA_PATH)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    merged_model.save_pretrained(FINAL_MODEL_PATH)\n",
    "    tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "    \n",
    "    print(f\"Final merged model saved to: {FINAL_MODEL_PATH}\")\n",
    "    print(\"Training completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 8\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 32  # Changed: 2*8*2 = 32\n",
    "  train_micro_batch_size_per_gpu: 2  # Changed: Reduced for 1.7B model\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ebe766",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch --config_file accelerate_config.yaml train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6562a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training output and model files\n",
    "import os\n",
    "print(\"DoRA adapter files:\")\n",
    "!ls -la qwen3_1.7b_dora_output/\n",
    "print(\"\\nFinal merged model files:\")\n",
    "!ls -la qwen3_1.7b_finetuned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a30859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a compressed archive for easier upload to Kaggle datasets\n",
    "!tar -czf qwen3_1.7b_finetuned_model.tar.gz qwen3_1.7b_finetuned/\n",
    "print(\"Model archived as: qwen3_1.7b_finetuned_model.tar.gz\")\n",
    "print(\"Upload this file to Kaggle as a dataset for use in the inference notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4446237",
   "metadata": {},
   "source": [
    "# ⚡ Speed Optimization Guide for 2x T4 GPUs (28GB Total VRAM)\n",
    "\n",
    "## Current Settings Status: ✅ **GOOD** for 2x T4 GPUs\n",
    "- **Memory**: 0.6B model + DoRA fits comfortably in 14GB per GPU\n",
    "- **Batch Size**: 4 per device × 4 accumulation = 32 effective batch size\n",
    "- **DeepSpeed**: ZeRO Stage 2 with FP16 - optimal for this setup\n",
    "\n",
    "## 🚀 Settings You Can Change to Speed Up Training:\n",
    "\n",
    "### **Quick Wins (Change These First):**\n",
    "1. **Increase Batch Size** (Cell 7 - `train.py`):\n",
    "   ```python\n",
    "   per_device_train_batch_size=6,  # Change from 4 to 6-8\n",
    "   gradient_accumulation_steps=2,   # Reduce from 4 to 2-3\n",
    "   ```\n",
    "   \n",
    "2. **Reduce Dataset Size** (Cell 5 - `utils.py`):\n",
    "   ```python\n",
    "   # In get_dataframe_to_train(), add sampling:\n",
    "   train_df = train_df.sample(frac=0.7, random_state=42)  # Use 70% of data\n",
    "   ```\n",
    "\n",
    "3. **Faster Optimizer** (Cell 7 - `train.py`):\n",
    "   ```python\n",
    "   optim=\"adamw_torch_fused\",  # Change from \"paged_adamw_8bit\"\n",
    "   ```\n",
    "\n",
    "### **Medium Impact:**\n",
    "4. **Reduce DoRA Rank** (Cell 7 - `train.py`):\n",
    "   ```python\n",
    "   r=20,              # Change from 32 to 20-24\n",
    "   lora_alpha=40,     # Change from 64 to 40-48\n",
    "   ```\n",
    "\n",
    "5. **Disable Some Features** (Cell 7 - `train.py`):\n",
    "   ```python\n",
    "   dataloader_pin_memory=False,     # Change from True\n",
    "   remove_unused_columns=True,      # Change from False\n",
    "   ```\n",
    "\n",
    "### **Advanced (Use If Needed):**\n",
    "6. **Enable Packing** (Cell 7 - `train.py`):\n",
    "   ```python\n",
    "   packing=True,               # Change from False\n",
    "   max_seq_length=1024,        # Add this limit\n",
    "   ```\n",
    "\n",
    "7. **Faster Scheduler** (Cell 7 - `train.py`):\n",
    "   ```python\n",
    "   lr_scheduler_type=\"linear\",  # Change from \"cosine\"\n",
    "   warmup_ratio=0.01,          # Reduce from 0.03\n",
    "   ```\n",
    "\n",
    "## 💡 **Expected Speed Improvements:**\n",
    "- **Quick Wins**: 40-60% faster training\n",
    "- **All Changes**: 60-80% faster training\n",
    "- **Trade-off**: Slight accuracy reduction (usually <2%)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
