{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fb082f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T14:54:25.158318Z",
     "iopub.status.busy": "2025-09-18T14:54:25.158043Z",
     "iopub.status.idle": "2025-09-18T14:57:18.415880Z",
     "shell.execute_reply": "2025-09-18T14:57:18.415066Z",
     "shell.execute_reply.started": "2025-09-18T14:54:25.158291Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip3-autoremove\n",
      "  Downloading pip3_autoremove-2.0.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from pip3-autoremove) (24.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pip3-autoremove) (25.0)\n",
      "Downloading pip3_autoremove-2.0.1-py2.py3-none-any.whl (12 kB)\n",
      "Installing collected packages: pip3-autoremove\n",
      "Successfully installed pip3-autoremove-2.0.1\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Collecting xformers\n",
      "  Downloading https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Downloading https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, xformers\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xformers-0.0.29.post3\n",
      "Collecting accelerate==1.7.0\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (0.33.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.7.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.7.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.7.0) (2025.5.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.7.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.7.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.7.0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.7.0) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.7.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.7.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.7.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.7.0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.7.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.7.0) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.7.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate==1.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.7.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate==1.7.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate==1.7.0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate==1.7.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate==1.7.0) (2024.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==1.7.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==1.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==1.7.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==1.7.0) (2025.6.15)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate==1.7.0) (2024.2.0)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "Successfully installed accelerate-1.7.0\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
      "Collecting unsloth==2025.5.7\n",
      "  Downloading unsloth-2025.5.7-py3-none-any.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth-zoo==2025.5.8\n",
      "  Downloading unsloth_zoo-2025.5.8-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (2.6.0+cu124)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (0.0.29.post3)\n",
      "Collecting bitsandbytes (from unsloth==2025.5.7)\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (3.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (25.0)\n",
      "Collecting tyro (from unsloth==2025.5.7)\n",
      "  Downloading tyro-0.9.31-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers!=4.47.0,==4.51.3 (from unsloth==2025.5.7)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (3.6.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (7.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (0.45.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (1.7.0)\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth==2025.5.7)\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (0.15.2)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (0.33.1)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (0.34.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.7) (0.21.0+cu124)\n",
      "Collecting cut_cross_entropy (from unsloth-zoo==2025.5.8)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth-zoo==2025.5.8) (11.2.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from unsloth-zoo==2025.5.8) (2024.11.6)\n",
      "Collecting msgspec (from unsloth-zoo==2025.5.8)\n",
      "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.7) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.7) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.7) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.7) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.7) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth==2025.5.7) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth==2025.5.7) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.7) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.7) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.7) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.7) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.7) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.7) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.7) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth==2025.5.7) (1.3.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.7) (14.0.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth==2025.5.7) (8.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth==2025.5.7) (0.16)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth==2025.5.7)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth==2025.5.7) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (3.12.13)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.7) (2025.6.15)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.7) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.7) (2.19.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth==2025.5.7) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth==2025.5.7) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth==2025.5.7) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth==2025.5.7) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth==2025.5.7) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth==2025.5.7) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth==2025.5.7) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth==2025.5.7) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth==2025.5.7) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.7) (1.20.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth==2025.5.7) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.7) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth==2025.5.7) (1.17.0)\n",
      "Downloading unsloth-2025.5.7-py3-none-any.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.5.8-py3-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m292.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m157.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m338.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m211.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m356.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.31-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m327.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m348.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: shtab, msgspec, fsspec, tyro, cut_cross_entropy, transformers, trl, unsloth-zoo, bitsandbytes, unsloth\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.47.0 cut_cross_entropy-25.1.1 fsspec-2025.3.0 msgspec-0.19.0 shtab-1.7.2 transformers-4.51.3 trl-0.15.2 tyro-0.9.31 unsloth-2025.5.7 unsloth-zoo-2025.5.8\n",
      "Collecting opensloth==0.1.7\n",
      "  Downloading opensloth-0.1.7-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: fastcore<2.0.0,>=1.7.29 in /usr/local/lib/python3.11/dist-packages (from opensloth==0.1.7) (1.7.29)\n",
      "Collecting fire<0.8.0,>=0.7.0 (from opensloth==0.1.7)\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyterlab<5.0.0,>=4.3.5 (from opensloth==0.1.7)\n",
      "  Downloading jupyterlab-4.4.7-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from opensloth==0.1.7) (6.0.2)\n",
      "Collecting speedy-utils<0.2.0,>=0.1.20 (from opensloth==0.1.7)\n",
      "  Downloading speedy_utils-0.1.30-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting tensorboard<3.0.0,>=2.19.0 (from opensloth==0.1.7)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorboardx<3.0.0.0,>=2.6.2.2 (from opensloth==0.1.7)\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastcore<2.0.0,>=1.7.29->opensloth==0.1.7) (25.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire<0.8.0,>=0.7.0->opensloth==0.1.7) (3.1.0)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7)\n",
      "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.28.1)\n",
      "Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (6.17.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (5.8.1)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7)\n",
      "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2.12.5)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (75.2.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (6.5.1)\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (5.7.1)\n",
      "Collecting bump2version (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7)\n",
      "  Downloading bump2version-1.0.1-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (5.5.2)\n",
      "Requirement already satisfied: debugpy in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.8.0)\n",
      "Requirement already satisfied: fastprogress in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.0.3)\n",
      "Collecting freezegun<2.0.0,>=1.5.1 (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7)\n",
      "  Downloading freezegun-1.5.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting ipdb (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7)\n",
      "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (8.1.5)\n",
      "Collecting json-repair<0.41.0,>=0.40.0 (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7)\n",
      "  Downloading json_repair-0.40.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting loguru (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (3.7.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.26.4)\n",
      "Collecting packaging (from fastcore<2.0.0,>=1.7.29->opensloth==0.1.7)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2.2.3)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2.11.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2.32.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.2.2)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (0.9.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3.0.0,>=2.19.0->opensloth==0.1.7) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3.0.0,>=2.19.0->opensloth==0.1.7) (1.73.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3.0.0,>=2.19.0->opensloth==0.1.7) (3.8.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tensorboard<3.0.0,>=2.19.0->opensloth==0.1.7) (11.2.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3.0.0,>=2.19.0->opensloth==0.1.7) (3.20.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3.0.0,>=2.19.0->opensloth==0.1.7) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3.0.0,>=2.19.0->opensloth==0.1.7) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from freezegun<2.0.0,>=1.5.1->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.16.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (7.34.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (8.6.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (24.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.0.3->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (3.0.2)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (6.4.5)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (5.10.4)\n",
      "Requirement already satisfied: overrides in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.22.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.18.1)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.8.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (4.3.8)\n",
      "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (4.24.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2.5.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipdb->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (4.4.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (3.0.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.4.8)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (0.4.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (3.6.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.25.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.3.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.19.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (3.0.51)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2.19.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (4.9.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.25.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (3.3.0)\n",
      "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.3.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.4)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (6.2.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.5.1)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.6.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (4.13.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.5.13)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2.21.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->freezegun<2.0.0,>=1.5.1->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.17.0)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (21.2.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->speedy-utils<0.2.0,>=0.1.20->opensloth==0.1.7) (2024.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.8.4)\n",
      "Requirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (3.0.0)\n",
      "Requirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (24.11.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.2.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2.7)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab<5.0.0,>=4.3.5->opensloth==0.1.7) (2.9.0.20250516)\n",
      "Downloading opensloth-0.1.7-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab-4.4.7-py3-none-any.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading speedy_utils-0.1.30-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading freezegun-1.5.5-py3-none-any.whl (19 kB)\n",
      "Downloading json_repair-0.40.0-py3-none-any.whl (20 kB)\n",
      "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bump2version-1.0.1-py2.py3-none-any.whl (22 kB)\n",
      "Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: packaging, loguru, json-repair, fire, bump2version, async-lru, freezegun, ipdb, jupyter-lsp, jupyterlab, tensorboardx, tensorboard, speedy-utils, opensloth\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: jupyter-lsp\n",
      "    Found existing installation: jupyter-lsp 1.5.1\n",
      "    Uninstalling jupyter-lsp-1.5.1:\n",
      "      Successfully uninstalled jupyter-lsp-1.5.1\n",
      "  Attempting uninstall: jupyterlab\n",
      "    Found existing installation: jupyterlab 3.6.8\n",
      "    Uninstalling jupyterlab-3.6.8:\n",
      "      Successfully uninstalled jupyterlab-3.6.8\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "jupyterlab-lsp 3.10.2 requires jupyterlab<4.0.0a0,>=3.1.0, but you have jupyterlab 4.4.7 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.20.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed async-lru-2.0.5 bump2version-1.0.1 fire-0.7.1 freezegun-1.5.5 ipdb-0.13.13 json-repair-0.40.0 jupyter-lsp-2.3.0 jupyterlab-4.4.7 loguru-0.7.3 opensloth-0.1.7 packaging-24.2 speedy-utils-0.1.30 tensorboard-2.20.0 tensorboardx-2.6.4\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement TextStreamer (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for TextStreamer\u001b[0m\u001b[31m\n",
      "\u001b[0m✅ TT-12 Dependencies installed:\n",
      "🚀 OpenSloth: Multi-GPU training\n",
      "🎯 vLLM: Precise inference\n",
      "📊 Analysis libraries: scikit-learn, matplotlib, seaborn\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 14:56:48.011249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758207408.369397      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758207408.468583      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies - OpenSloth + vLLM + Analysis setup\n",
    "!pip install pip3-autoremove\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install accelerate==1.7.0\n",
    "!pip install triton==3.2.0 \n",
    "\n",
    "!pip install unsloth==2025.5.7 unsloth-zoo==2025.5.8 --no-cache\n",
    "\n",
    "!pip install opensloth==0.1.7 \n",
    "\n",
    "!pip install TextStreamer\n",
    "\n",
    "print(\"✅ TT-12 Dependencies installed:\")\n",
    "print(\"🚀 OpenSloth: Multi-GPU training\")\n",
    "print(\"🎯 vLLM: Precise inference\") \n",
    "print(\"📊 Analysis libraries: scikit-learn, matplotlib, seaborn\")\n",
    "import unsloth\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11239ed",
   "metadata": {},
   "source": [
    "# 1. Configuration and Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d63f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:57:18.419441Z",
     "iopub.status.busy": "2025-09-18T14:57:18.419240Z",
     "iopub.status.idle": "2025-09-18T14:57:18.424713Z",
     "shell.execute_reply": "2025-09-18T14:57:18.423954Z",
     "shell.execute_reply.started": "2025-09-18T14:57:18.419419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants.py\n",
    "from unsloth import FastLanguageModel\n",
    "# Using base Qwen3 1.7B model from Kaggle input (no internet needed)\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen3-1.7b-unsloth-bnb-4bit/gguf/default/1/qwen3_4bit\"  # Update this path as needed\n",
    "LORA_PATH = \"qwen3_1.7b_opensloth_lora_validation/\"  # OpenSloth LoRA output path for validation\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "_ , tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_PATH,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "CHAt_TEMPLATE = tokenizer.chat_template\n",
    "\n",
    "# TT-12 Validation Parameters\n",
    "TRAINING_DATA_PERCENTAGE = .3  # Controllable % of training data (0.1 = 10%, 1.0 = 100%)\n",
    "USE_STRATIFIED_SAMPLING = True  # Maintain rule distribution when sampling\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''\n",
    "\n",
    "print(\"✅ Using Qwen3 1.7B model from local Kaggle input\")\n",
    "print(f\"🎯 TT-12: OpenSloth training + vLLM inference with {TRAINING_DATA_PERCENTAGE*100:.0f}% of data\")\n",
    "print(f\"📊 Stratified sampling: {USE_STRATIFIED_SAMPLING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76e5f0a-d6a2-4d76-b9f3-3b51c643d488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:57:18.425725Z",
     "iopub.status.busy": "2025-09-18T14:57:18.425472Z",
     "iopub.status.idle": "2025-09-18T14:57:53.843113Z",
     "shell.execute_reply": "2025-09-18T14:57:53.842380Z",
     "shell.execute_reply.started": "2025-09-18T14:57:18.425701Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2025-09-18 14:57:25.094818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758207445.116329     180 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758207445.123073     180 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "✅ Using Qwen3 1.7B model from local Kaggle input\n",
      "🎯 TT-12: OpenSloth training + vLLM inference with 30% of data\n",
      "📊 Stratified sampling: True\n"
     ]
    }
   ],
   "source": [
    "!python constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e665e3-1b2e-41e7-9075-b430579f7323",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T14:57:53.844274Z",
     "iopub.status.busy": "2025-09-18T14:57:53.844061Z",
     "iopub.status.idle": "2025-09-18T14:57:58.736060Z",
     "shell.execute_reply": "2025-09-18T14:57:58.735143Z",
     "shell.execute_reply.started": "2025-09-18T14:57:53.844253Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "✅ Using Qwen3 1.7B model from local Kaggle input\n",
      "🎯 TT-12: OpenSloth training + vLLM inference with 30% of data\n",
      "📊 Stratified sampling: True\n",
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for forward_message in messages %}\n",
      "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
      "    {%- set message = messages[index] %}\n",
      "    {%- set tool_start = '<tool_response>' %}\n",
      "    {%- set tool_start_length = tool_start|length %}\n",
      "    {%- set start_of_message = message.content[:tool_start_length] %}\n",
      "    {%- set tool_end = '</tool_response>' %}\n",
      "    {%- set tool_end_length = tool_end|length %}\n",
      "    {%- set start_pos = (message.content|length) - tool_end_length %}\n",
      "    {%- if start_pos < 0 %}\n",
      "        {%- set start_pos = 0 %}\n",
      "    {%- endif %}\n",
      "    {%- set end_of_message = message.content[start_pos:] %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n",
      "                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n",
      "                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n"
     ]
    }
   ],
   "source": [
    "from constants import CHAt_TEMPLATE\n",
    "print(CHAt_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75e2fad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:21:02.901209Z",
     "iopub.status.busy": "2025-09-18T15:21:02.900854Z",
     "iopub.status.idle": "2025-09-18T15:21:02.911930Z",
     "shell.execute_reply": "2025-09-18T15:21:02.911199Z",
     "shell.execute_reply.started": "2025-09-18T15:21:02.901181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from constants import CHAt_TEMPLATE ,POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT, TRAINING_DATA_PERCENTAGE, USE_STRATIFIED_SAMPLING, DATA_PATH\n",
    "import random, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import os\n",
    "import shutil\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_example_based_training_data(data_path):\n",
    "    \"\"\"\n",
    "    TT-12: Create training data from examples (like test-time training)\n",
    "    This trains the model on examples, not actual comments\n",
    "    \"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    \n",
    "    # Sample data if needed while maintaining rule distribution\n",
    "    if TRAINING_DATA_PERCENTAGE < 1.0:\n",
    "        if USE_STRATIFIED_SAMPLING:\n",
    "            # Stratified sampling to maintain rule distribution\n",
    "            train_dataset = train_dataset.groupby('rule', group_keys=False).apply(\n",
    "                lambda x: x.sample(frac=TRAINING_DATA_PERCENTAGE, random_state=42)\n",
    "            ).reset_index(drop=True)\n",
    "            print(f\"📊 Stratified sampling: {len(train_dataset)} samples ({TRAINING_DATA_PERCENTAGE*100:.0f}%)\")\n",
    "        else:\n",
    "            # Simple random sampling\n",
    "            train_dataset = train_dataset.sample(frac=TRAINING_DATA_PERCENTAGE, random_state=42).reset_index(drop=True)\n",
    "            print(f\"📊 Random sampling: {len(train_dataset)} samples ({TRAINING_DATA_PERCENTAGE*100:.0f}%)\")\n",
    "    \n",
    "    print(f\"📊 Training data size: {len(train_dataset)} samples\")\n",
    "    print(f\"📊 Rule distribution: {train_dataset['rule'].value_counts().to_dict()}\")\n",
    "    \n",
    "    flatten = []\n",
    "    \n",
    "    # Create training data from examples (similar to test-time training)\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = train_dataset[[\"rule\",\"subreddit\",\n",
    "                                        \"positive_example_1\",\"positive_example_2\",\n",
    "                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                # Use positive example as the \"body\" to classify\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"  # other positive\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                # negative_example randomly selected\n",
    "                sub_dataset[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"negative_example_1\"],\n",
    "                    sub_dataset[\"negative_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 1  # Positive examples violate rules\n",
    "\n",
    "            else:  # violation_type == \"negative\"\n",
    "                # Use negative example as the \"body\" to classify\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                sub_dataset[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"positive_example_1\"],\n",
    "                    sub_dataset[\"positive_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 0  # Negative examples don't violate rules\n",
    "\n",
    "            # Drop original candidate columns\n",
    "            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    # Merge all DataFrames\n",
    "    example_training_df = pd.concat(flatten, axis=0)\n",
    "    example_training_df = example_training_df.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    print(f\"📊 Example-based training dataset: {len(example_training_df)} samples\")\n",
    "    print(f\"📊 Positive examples: {sum(example_training_df['rule_violation'] == 1)}\")\n",
    "    print(f\"📊 Negative examples: {sum(example_training_df['rule_violation'] == 0)}\")\n",
    "    \n",
    "    return example_training_df\n",
    "\n",
    "\n",
    "def get_real_comment_validation_data(data_path):\n",
    "    \"\"\"\n",
    "    TT-12: Get real comments with labels for validation\n",
    "    This is what we actually want to predict\n",
    "    \"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    \n",
    "    # Use actual comments and their labels for validation\n",
    "    validation_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                                  \"positive_example_1\",\"positive_example_2\",\n",
    "                                  \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # Randomly select positive_example and negative_example for prompts\n",
    "    validation_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(validation_df)) < 0.5,\n",
    "        validation_df[\"positive_example_1\"],\n",
    "        validation_df[\"positive_example_2\"]\n",
    "    )\n",
    "    validation_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(validation_df)) < 0.5,\n",
    "        validation_df[\"negative_example_1\"],\n",
    "        validation_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # Drop original candidate columns\n",
    "    validation_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                               \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "    \n",
    "    print(f\"📊 Real comment validation dataset: {len(validation_df)} samples\")\n",
    "    print(f\"📊 Rule violations: {sum(validation_df['rule_violation'] == 1)} positive, {sum(validation_df['rule_violation'] == 0)} negative\")\n",
    "    \n",
    "    return validation_df\n",
    "\n",
    "\n",
    "def build_dataset_for_opensloth(dataframe, tokenizer):\n",
    "    \"\"\"Build dataset for OpenSloth training with proper text formatting\"\"\"\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "    \n",
    "    # OpenSloth uses a text field\n",
    "    dataframe[\"text\"] = dataframe.apply(lambda row: \n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": row[\"prompt\"]},\n",
    "                {\"role\": \"assistant\", \"content\": POSITIVE_ANSWER if row[\"rule_violation\"] == 1 else NEGATIVE_ANSWER},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    dataset = Dataset.from_pandas(dataframe[[\"text\"]])\n",
    "    return dataset\n",
    "\n",
    "def build_dataset_unsloth(dataframe):\n",
    "    \"\"\"Build dataset for Unsloth training with proper text formatting\"\"\"\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "    \n",
    "    # Unsloth expects \"text\" field with full prompt + completion\n",
    "    dataframe[\"text\"] = dataframe.apply(lambda row: \n",
    "        row[\"prompt\"] + \" \" + (POSITIVE_ANSWER if row[\"rule_violation\"] == 1 else NEGATIVE_ANSWER), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    dataframe = dataframe[[\"text\"]]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_validation_dataset(dataframe):\n",
    "    \"\"\"Build dataset for validation (keep labels for evaluation)\"\"\"\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "    dataframe = dataframe[[\"prompt\", \"rule_violation\"]]  # Keep true labels for evaluation\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    return dataset\n",
    "\n",
    "def cache_dataset(cache_path):\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"🗑️ Deleting existing cache folder at {cache_path}\")\n",
    "        shutil.rmtree(cache_path)  # Recursively delete the folder and its contents\n",
    "\n",
    "    print(f\"💾 Caching dataset to {cache_path}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"/kaggle/input/qwen3-1.7b-unsloth-bnb-4bit/gguf/default/1/qwen3_4bit\",\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    # ✅ ADD: Configure LoRA adapters for the quantized model\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=3407,\n",
    "    )\n",
    "    \n",
    "    # Set chat template\n",
    "    tokenizer.chat_template = CHAt_TEMPLATE\n",
    "    \n",
    "    train_df = get_example_based_training_data(DATA_PATH)\n",
    "    dataset = build_dataset_for_opensloth(train_df, tokenizer)\n",
    "    \n",
    "    # Now SFTTrainer will work with LoRA-enabled model\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,  # ← Now has LoRA adapters\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        args=SFTConfig(\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=2048,\n",
    "            dataset_num_proc=2,\n",
    "            packing=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    trainer.train_dataset.save_to_disk(cache_path)\n",
    "    print(f\"✅ Dataset cached to {cache_path}\")\n",
    "\n",
    "\n",
    "def get_cached_dataset(cache_path):\n",
    "    if not os.path.exists(cache_path):\n",
    "        raise RuntimeError(\"Dataset cache not found. Please run the caching step first.\")\n",
    "    return load_from_disk(cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aab218-387b-448c-b279-95c81f733380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:26:54.334784Z",
     "iopub.status.busy": "2025-09-18T14:26:54.334089Z",
     "iopub.status.idle": "2025-09-18T14:26:59.790147Z",
     "shell.execute_reply": "2025-09-18T14:26:59.789554Z",
     "shell.execute_reply.started": "2025-09-18T14:26:54.334757Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Caching dataset to ass\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "📊 Stratified sampling: 609 samples (30%)\n",
      "📊 Training data size: 609 samples\n",
      "📊 Rule distribution: {'No legal advice: Do not offer or request legal advice.': 305, 'No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.': 304}\n",
      "📊 Example-based training dataset: 2435 samples\n",
      "📊 Positive examples: 1218\n",
      "📊 Negative examples: 1217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95aa6e321ee64320b1517b7d9b3dcc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.exists(cache_path):\n",
    "        print(f\"💾 Loading cached dataset from {cache_path}\")\n",
    "\n",
    "print(f\"💾 Caching dataset to {cache_path}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"/kaggle/input/qwen3-1.7b-unsloth-bnb-4bit/gguf/default/1/qwen3_4bit\",\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    # Set chat template\n",
    "tokenizer.chat_template = CHAt_TEMPLATE\n",
    "    \n",
    "train_df = get_example_based_training_data(DATA_PATH)\n",
    "dataset = build_dataset_for_opensloth(train_df, tokenizer)\n",
    "    \n",
    "    # ✅ Save dataset directly without SFTTrainer\n",
    "dataset.save_to_disk(cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb8e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60f8a561-d945-4781-be7c-a8ddbf0f045b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:06:47.141841Z",
     "iopub.status.busy": "2025-09-18T14:06:47.141562Z",
     "iopub.status.idle": "2025-09-18T14:06:47.146285Z",
     "shell.execute_reply": "2025-09-18T14:06:47.145667Z",
     "shell.execute_reply.started": "2025-09-18T14:06:47.141809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "103a61ae-a43b-4f98-bf64-c454e09c1699",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T13:49:18.042011Z",
     "iopub.status.busy": "2025-09-18T13:49:18.041549Z",
     "iopub.status.idle": "2025-09-18T13:49:46.697872Z",
     "shell.execute_reply": "2025-09-18T13:49:46.696770Z",
     "shell.execute_reply.started": "2025-09-18T13:49:18.041986Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/3279665559.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth\n",
      "/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py:177: UserWarning: Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.\n",
      "  warnings.warn(\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py:211: UserWarning: Unsloth: CUDA is not linked properly.\n",
      "Try running `python -m bitsandbytes` then `python -m xformers.info`\n",
      "We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn't work.\n",
      "You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\n",
      "Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\n",
      "Unsloth will still run for now, but maybe it might crash - let's hope it works!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 13:49:25.710919: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758203366.050247      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758203366.150830      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'bitsandbytes' has no attribute 'functional'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/3279665559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0munsloth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mllama\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mFastLlamaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m    \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastVisionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmistral\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mFastMistralModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0m_prepare_4d_causal_attention_mask_for_sdpa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mHAS_FLASH_ATTENTION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/kernels/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from .cross_entropy_loss import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mfast_cross_entropy_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpost_patch_loss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/kernels/cross_entropy_loss.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mcalculate_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mMAX_FUSED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/kernels/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1330/files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mHAS_CUDA_STREAM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0.43.3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mget_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ptr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'bitsandbytes' has no attribute 'functional'"
     ]
    }
   ],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae027499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:06:02.724513Z",
     "iopub.status.busy": "2025-09-18T14:06:02.724238Z",
     "iopub.status.idle": "2025-09-18T14:06:07.658948Z",
     "shell.execute_reply": "2025-09-18T14:06:07.658304Z",
     "shell.execute_reply.started": "2025-09-18T14:06:02.724493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model , tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"/kaggle/input/qwen3-1.7b-unsloth-bnb-4bit/gguf/default/1/qwen3_4bit\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94b7da6-869f-4cf4-b53e-4018b18a84ad",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T13:56:04.877274Z",
     "iopub.status.busy": "2025-09-18T13:56:04.876971Z",
     "iopub.status.idle": "2025-09-18T13:56:04.881500Z",
     "shell.execute_reply": "2025-09-18T13:56:04.880749Z",
     "shell.execute_reply.started": "2025-09-18T13:56:04.877252Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for forward_message in messages %}\n",
      "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
      "    {%- set message = messages[index] %}\n",
      "    {%- set tool_start = '<tool_response>' %}\n",
      "    {%- set tool_start_length = tool_start|length %}\n",
      "    {%- set start_of_message = message.content[:tool_start_length] %}\n",
      "    {%- set tool_end = '</tool_response>' %}\n",
      "    {%- set tool_end_length = tool_end|length %}\n",
      "    {%- set start_pos = (message.content|length) - tool_end_length %}\n",
      "    {%- if start_pos < 0 %}\n",
      "        {%- set start_pos = 0 %}\n",
      "    {%- endif %}\n",
      "    {%- set end_of_message = message.content[start_pos:] %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n",
      "                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n",
      "                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28d34314-f34b-4459-a7c4-6c7770613acf",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T13:57:19.732328Z",
     "iopub.status.busy": "2025-09-18T13:57:19.731894Z",
     "iopub.status.idle": "2025-09-18T13:57:20.227830Z",
     "shell.execute_reply": "2025-09-18T13:57:20.227032Z",
     "shell.execute_reply.started": "2025-09-18T13:57:19.732293Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for forward_message in messages %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- set message = messages[index] %}\\n    {%- set tool_start = \\'<tool_response>\\' %}\\n    {%- set tool_start_length = tool_start|length %}\\n    {%- set start_of_message = message.content[:tool_start_length] %}\\n    {%- set tool_end = \\'</tool_response>\\' %}\\n    {%- set tool_end_length = tool_end|length %}\\n    {%- set start_pos = (message.content|length) - tool_end_length %}\\n    {%- if start_pos < 0 %}\\n        {%- set start_pos = 0 %}\\n    {%- endif %}\\n    {%- set end_of_message = message.content[start_pos:] %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set content = message.content %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in message.content %}\\n                {%- set content = (message.content.split(\\'</think>\\')|last).lstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = (message.content.split(\\'</think>\\')|first).rstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = (reasoning_content.split(\\'<think>\\')|last).lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok=AutoTokenizer.from_pretrained(r\"/kaggle/input/qwen3-1.7b-unsloth-bnb-4bit/gguf/default/1/qwen3_4bit\")\n",
    "tok.chat_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a6fdb-2982-492f-bc61-42c16a51d943",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6eddd-6794-4500-bf2c-7326e6d1aeaa",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d2ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4f6986-b802-49e1-a692-187ec41bf2d1",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T13:05:12.140633Z",
     "iopub.status.busy": "2025-09-18T13:05:12.140086Z",
     "iopub.status.idle": "2025-09-18T13:05:12.167184Z",
     "shell.execute_reply": "2025-09-18T13:05:12.166251Z",
     "shell.execute_reply.started": "2025-09-18T13:05:12.140606Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/2308695852.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tokenizer.apply_chat_template(\n\u001b[0m\u001b[1;32m      2\u001b[0m             [\n\u001b[1;32m      3\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPOSITIVE_ANSWER\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"rule_violation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNEGATIVE_ANSWER\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             ],\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mapply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[0mtokenizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_assistant_tokens_mask\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\{\\%-?\\s*generation\\s*-?\\%\\}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mget_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1820\u001b[0m                 \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1823\u001b[0m                     \u001b[0;34m\"Cannot use chat template functions because tokenizer.chat_template is not set and no template \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;34m\"argument was passed! For information about writing templates and setting the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d4d6d8-0008-4996-9b36-8cf0815e17fd",
   "metadata": {},
   "source": [
    "#TESTING SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf446ec-7516-425b-9167-918be6108a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:58.577900Z",
     "iopub.status.busy": "2025-09-18T13:01:58.577623Z",
     "iopub.status.idle": "2025-09-18T13:01:58.582136Z",
     "shell.execute_reply": "2025-09-18T13:01:58.581345Z",
     "shell.execute_reply.started": "2025-09-18T13:01:58.577877Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38d2447b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:05:36.381862Z",
     "iopub.status.busy": "2025-09-18T14:05:36.381547Z",
     "iopub.status.idle": "2025-09-18T14:05:36.887643Z",
     "shell.execute_reply": "2025-09-18T14:05:36.887060Z",
     "shell.execute_reply.started": "2025-09-18T14:05:36.381811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Stratified sampling: 609 samples (30%)\n",
      "📊 Training data size: 609 samples\n",
      "📊 Rule distribution: {'No legal advice: Do not offer or request legal advice.': 305, 'No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.': 304}\n",
      "📊 Example-based training dataset: 2435 samples\n",
      "📊 Positive examples: 1218\n",
      "📊 Negative examples: 1217\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "train_df = get_example_based_training_data(DATA_PATH)\n",
    "dataset = build_dataset_for_opensloth(train_df, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a22dd19a-f1cb-489d-aad5-9c9724f9598d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:13:13.897462Z",
     "iopub.status.busy": "2025-09-18T14:13:13.897179Z",
     "iopub.status.idle": "2025-09-18T14:13:13.901306Z",
     "shell.execute_reply": "2025-09-18T14:13:13.900596Z",
     "shell.execute_reply.started": "2025-09-18T14:13:13.897440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28796082-1b53-4631-8e52-e6bb34d1d626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:17:02.974893Z",
     "iopub.status.busy": "2025-09-18T14:17:02.974409Z",
     "iopub.status.idle": "2025-09-18T14:17:02.979587Z",
     "shell.execute_reply": "2025-09-18T14:17:02.978798Z",
     "shell.execute_reply.started": "2025-09-18T14:17:02.974854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model = PeftModel.from_pretrained(model, LORA_PATH)\n",
    "    model = model.merge_and_unload()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfbe3537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:06:33.715174Z",
     "iopub.status.busy": "2025-09-18T14:06:33.714721Z",
     "iopub.status.idle": "2025-09-18T14:06:33.719558Z",
     "shell.execute_reply": "2025-09-18T14:06:33.718638Z",
     "shell.execute_reply.started": "2025-09-18T14:06:33.715143Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<|im_start|>user\\n\\nYou are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.\\n\\nSubreddit: r/news\\nRule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\\nExamples:\\n1) [SANDERS SUPPORTERS DON'T WORRY. LENDING TREE IS WILLING TO MAKE LOANS OF UP TO 32000 DOLLARS SO YOU CAN DONATE TO YOUR LORD AND SAVIOR](https://www.lendingtree.com/personal-loans-index)\\nAnswer: Yes\\n\\n2) **HD** [ German Stream ](http://www.sportonline-tv.com/stream3.php) | Mobile : Yes\\nAnswer: No\\n\\n---\\nComment: 39 http://MySexFind.com - find girl for sex now 39!\\nAnswer:<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nYes<|im_end|>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "913d205b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:13:18.637162Z",
     "iopub.status.busy": "2025-09-18T14:13:18.636665Z",
     "iopub.status.idle": "2025-09-18T14:13:18.797357Z",
     "shell.execute_reply": "2025-09-18T14:13:18.796475Z",
     "shell.execute_reply.started": "2025-09-18T14:13:18.637140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'No': 0.0000\n",
      "Probability of 'Yes': 1.0000\n",
      "Prediction: Yes\n"
     ]
    }
   ],
   "source": [
    "# Enable inference mode\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare input\n",
    "inputs = tokenizer(dataset[666]['text'], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Get logits for the next token (without generating)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    next_token_logits = outputs.logits[0, -1, :]  # Last token's logits\n",
    "\n",
    "# Get Yes/No token IDs\n",
    "yes_token_id = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "no_token_id = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "\n",
    "# Extract logits for Yes/No tokens\n",
    "yes_logit = next_token_logits[yes_token_id]\n",
    "no_logit = next_token_logits[no_token_id]\n",
    "\n",
    "# Convert to probabilities\n",
    "import torch.nn.functional as F\n",
    "combined_logits = torch.stack([no_logit, yes_logit])  # [No, Yes]\n",
    "probabilities = F.softmax(combined_logits, dim=0)\n",
    "\n",
    "prob_no = probabilities[0].item()\n",
    "prob_yes = probabilities[1].item()\n",
    "\n",
    "print(f\"Probability of 'No': {prob_no:.4f}\")\n",
    "print(f\"Probability of 'Yes': {prob_yes:.4f}\")\n",
    "print(f\"Prediction: {'Yes' if prob_yes > prob_no else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "796888b0-2f24-4d8f-9185-1bf60e1ad19c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:14:57.016728Z",
     "iopub.status.busy": "2025-09-18T14:14:57.016437Z",
     "iopub.status.idle": "2025-09-18T14:14:57.165245Z",
     "shell.execute_reply": "2025-09-18T14:14:57.164372Z",
     "shell.execute_reply.started": "2025-09-18T14:14:57.016706Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'No': 0.0000\n",
      "Probability of 'Yes': 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Some versions support returning logits\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(dataset[666]['text'], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate with output_scores=True\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1,  # Only generate 1 token\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    do_sample=False,  # Greedy decoding\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "if hasattr(outputs, 'scores') and outputs.scores:\n",
    "    # Get the logits for the first (and only) generated token\n",
    "    next_token_logits = outputs.scores[0][0]  # [batch_size=1, vocab_size]\n",
    "    \n",
    "    yes_token_id = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "    no_token_id = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "    \n",
    "    yes_logit = next_token_logits[yes_token_id]\n",
    "    no_logit = next_token_logits[no_token_id]\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    combined_logits = torch.stack([no_logit, yes_logit])\n",
    "    probabilities = F.softmax(combined_logits, dim=0)\n",
    "    \n",
    "    prob_no = probabilities[0].item()\n",
    "    prob_yes = probabilities[1].item()\n",
    "    \n",
    "    print(f\"Probability of 'No': {prob_no:.4f}\")\n",
    "    print(f\"Probability of 'Yes': {prob_yes:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7083c80",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-17T13:45:05.486365Z",
     "iopub.status.busy": "2025-09-17T13:45:05.486062Z",
     "iopub.status.idle": "2025-09-17T13:45:05.492695Z",
     "shell.execute_reply": "2025-09-17T13:45:05.491884Z",
     "shell.execute_reply.started": "2025-09-17T13:45:05.486342Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🎯 Full Decoded Output (including thinking):\n",
      "user\n",
      "\n",
      "You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.\n",
      "\n",
      "Subreddit: r/news\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Examples:\n",
      "1) [SANDERS SUPPORTERS DON'T WORRY. LENDING TREE IS WILLING TO MAKE LOANS OF UP TO 32000 DOLLARS SO YOU CAN DONATE TO YOUR LORD AND SAVIOR](https://www.lendingtree.com/personal-loans-index)\n",
      "Answer: Yes\n",
      "\n",
      "2) **HD** [ German Stream ](http://www.sportonline-tv.com/stream3.php) | Mobile : Yes\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "Comment: 39 http://MySexFind.com - find girl for sex now 39!\n",
      "Answer:\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Yes\n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Decode the full output to readable text\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 Full Decoded Output (including thinking):\")\n",
    "print(decoded_output)\n",
    "print(\"=\"*512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402a971-332b-4ef7-a273-b985a94638cf",
   "metadata": {},
   "source": [
    "Trying Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0f69c75-8049-4d14-adb8-ba8bb08b8f9f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T14:07:13.777455Z",
     "iopub.status.busy": "2025-09-18T14:07:13.776643Z",
     "iopub.status.idle": "2025-09-18T14:07:37.673410Z",
     "shell.execute_reply": "2025-09-18T14:07:37.672755Z",
     "shell.execute_reply.started": "2025-09-18T14:07:13.777430Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule.\n",
      "\n",
      "First, think step-by-step about the comment and the rule.\n",
      "Then, conclude with your Single final answer: 'Answer: Yes' or 'Answer: No'.Full stop after answer.Say nothing after giving out the answer.\n",
      "\n",
      "Subreddit: r/news\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Examples:\n",
      "1) [SANDERS SUPPORTERS DON'T WORRY. LENDING TREE IS WILLING TO MAKE LOANS OF UP TO 32000 DOLLARS SO YOU CAN DONATE TO YOUR LORD AND SAVIOR](https://www.lendingtree.com/personal-loans-index)\n",
      "Answer: Yes\n",
      "\n",
      "2) **HD** [ German Stream ](http://www.sportonline-tv.com/stream3.php) | Mobile : Yes\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "Comment: 39 http://MySexFind.com - find girl for sex now 39!\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "Answer: Yes\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "\n",
      "\n",
      "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "🎯 Full Decoded Output (with thinking):\n",
      "You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule.\n",
      "\n",
      "First, think step-by-step about the comment and the rule.\n",
      "Then, conclude with your Single final answer: 'Answer: Yes' or 'Answer: No'.Full stop after answer.Say nothing after giving out the answer.\n",
      "\n",
      "Subreddit: r/news\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Examples:\n",
      "1) [SANDERS SUPPORTERS DON'T WORRY. LENDING TREE IS WILLING TO MAKE LOANS OF UP TO 32000 DOLLARS SO YOU CAN DONATE TO YOUR LORD AND SAVIOR](https://www.lendingtree.com/personal-loans-index)\n",
      "Answer: Yes\n",
      "\n",
      "2) **HD** [ German Stream ](http://www.sportonline-tv.com/stream3.php) | Mobile : Yes\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "Comment: 39 http://MySexFind.com - find girl for sex now 39!\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "Answer: Yes\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "\n",
      "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "🎯 New Generated Part:\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "Answer: Yes\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Thinking prompt (replace with your dataset[0]['text'] if needed)\n",
    "thinking_prompt = \"\"\"You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule.\n",
    "\n",
    "First, think step-by-step about the comment and the rule.\n",
    "Then, conclude with your Single final answer: 'Answer: Yes' or 'Answer: No'.Full stop after answer.Say nothing after giving out the answer.\n",
    "\n",
    "Subreddit: r/news\n",
    "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
    "Examples:\n",
    "1) [SANDERS SUPPORTERS DON'T WORRY. LENDING TREE IS WILLING TO MAKE LOANS OF UP TO 32000 DOLLARS SO YOU CAN DONATE TO YOUR LORD AND SAVIOR](https://www.lendingtree.com/personal-loans-index)\n",
    "Answer: Yes\n",
    "\n",
    "2) **HD** [ German Stream ](http://www.sportonline-tv.com/stream3.php) | Mobile : Yes\n",
    "Answer: No\n",
    "\n",
    "---\n",
    "Comment: 39 http://MySexFind.com - find girl for sex now 39!\n",
    "\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": thinking_prompt},\n",
    "    {\"role\": \"assistant\", \"content\": \"<think>\"}\n",
    "]\n",
    "thinking_input = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(thinking_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate with Qwen Thinking Mode settings\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=500,  # Allow room for thinking\n",
    "    temperature=0.6,     # Qwen Thinking Mode: Balanced creativity\n",
    "    top_p=0.95,          # Qwen Thinking Mode: Diverse selection\n",
    "    top_k=20,            # Qwen Thinking Mode: Limit to top 20 tokens\n",
    "    min_p=0.0,           # Qwen Thinking Mode: No minimum probability threshold\n",
    "    do_sample=True,      # Enable sampling for creativity\n",
    "    #stop=[\"\\n\"],         # Stop after newline to prevent repetition\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode the full output to readable text\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\n\" + \"=\"*500)\n",
    "print(\"🎯 Full Decoded Output (with thinking):\")\n",
    "print(decoded_output)\n",
    "print(\"=\"*500)\n",
    "\n",
    "# Optional: Extract just the new part (after the prompt)\n",
    "new_part = decoded_output[len(thinking_prompt):].strip()\n",
    "print(\"\\n🎯 New Generated Part:\")\n",
    "print(new_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "142e8d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:11:54.341491Z",
     "iopub.status.busy": "2025-09-18T15:11:54.341163Z",
     "iopub.status.idle": "2025-09-18T15:11:54.348866Z",
     "shell.execute_reply": "2025-09-18T15:11:54.348236Z",
     "shell.execute_reply.started": "2025-09-18T15:11:54.341466Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_opensloth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_opensloth.py\n",
    "from opensloth.opensloth_config import (\n",
    "    FastModelArgs,\n",
    "    LoraArgs,\n",
    "    OpenSlothConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from opensloth.scripts.opensloth_sft_trainer import run_mp_training, setup_envs\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH\n",
    "import torch\n",
    "\n",
    "# ✅ FIXED: Use developer's proven configuration\n",
    "GLOBAL_BZ = 16  # Reduced from 32 for 1.7B model\n",
    "DEVICES = [0, 1]  # Explicit device list like developer\n",
    "BZ = 1  # ✅ CRITICAL: Use 1 with sequence packing (from developer)\n",
    "\n",
    "opensloth_config = OpenSlothConfig(\n",
    "    data_cache_path=\"data/cache_qwen3_dataset_for_opensloth/\",\n",
    "    devices=DEVICES,\n",
    "    fast_model_args=FastModelArgs(\n",
    "        model_name=BASE_MODEL_PATH,\n",
    "        max_seq_length=2048,  # Keep your sequence length for memory\n",
    "        load_in_4bit=True,\n",
    "        # ✅ REMOVE: local_files_only and trust_remote_code (not in developer config)\n",
    "    ),\n",
    "    lora_args=LoraArgs(\n",
    "        r=8,  # ✅ REDUCED: Use developer's smaller rank\n",
    "        lora_alpha=16,  # ✅ MATCH: Developer's alpha\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_rslora=False,\n",
    "    ),\n",
    "    sequence_packing=True,\n",
    "    dataset_text_field=\"text\"\n",
    "    # ✅ NOTE: dataset_text_field is handled automatically by OpenSloth when using SFTTrainer-cached data\n",
    ")\n",
    "\n",
    "training_config = TrainingArguments(\n",
    "    output_dir=LORA_PATH,\n",
    "    per_device_train_batch_size=BZ,  # ✅ BZ=1 with packing\n",
    "    gradient_accumulation_steps=GLOBAL_BZ // (len(DEVICES) * BZ),  # 16/2 = 8\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=60,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=5,\n",
    "    save_total_limit=1,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Global batch size: {len(DEVICES) * BZ * training_config.gradient_accumulation_steps}\")\n",
    "    print(f\"Gradient accumulation steps: {training_config.gradient_accumulation_steps}\")\n",
    "\n",
    "    setup_envs(opensloth_config, training_config)\n",
    "    run_mp_training(opensloth_config.devices, opensloth_config, training_config)\n",
    "    \n",
    "    print(f\"✅ OpenSloth training completed! LoRA adapters saved to: {LORA_PATH}\")\n",
    "    print(\"🎯 Ready for vLLM inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a7fe130-3837-43ce-82cc-fb0c9d777207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:21:29.215214Z",
     "iopub.status.busy": "2025-09-18T15:21:29.214943Z",
     "iopub.status.idle": "2025-09-18T15:21:29.223198Z",
     "shell.execute_reply": "2025-09-18T15:21:29.222439Z",
     "shell.execute_reply.started": "2025-09-18T15:21:29.215195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_unsloth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_unsloth.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from utils import build_dataset_unsloth, get_example_based_training_data\n",
    "from constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n",
    "\n",
    "\n",
    "def main():\n",
    "    # TT-11: Get example-based training data (train on examples, not real comments)\n",
    "    train_df = get_example_based_training_data(DATA_PATH)\n",
    "    train_dataset = build_dataset_unsloth(train_df)\n",
    "    \n",
    "    print(f\"Training dataset size: {len(train_dataset)} samples\")\n",
    "    print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # 🚀 UNSLOTH: Load model with 4-bit quantization (2x T4 optimized)\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=BASE_MODEL_PATH,\n",
    "        max_seq_length=2048,  # Adjust based on your max sequence length\n",
    "        dtype=None,  # Auto-detect (will use float16)\n",
    "        load_in_4bit=True,  # Enable 4-bit quantization\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        device_map=\"balanced\"\n",
    "    )\n",
    "    print(\"✅ Unsloth model loaded with 4-bit quantization across 2x T4\")\n",
    "    \n",
    "    # 🚀 UNSLOTH: Add LoRA adapters (automatic and optimized)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,  # LoRA rank (can try 8, 16, 32, 64, 128)\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=32,  # LoRA alpha (typically equal to r for Unsloth)\n",
    "        lora_dropout=0,  # 0 for faster training with Unsloth\n",
    "        bias=\"none\",\n",
    "        #use_gradient_checkpointing=False,  # Enable for memory efficiency\n",
    "        random_state=3407,  # For reproducibility\n",
    "        use_rslora=True,  # Can try True for better stability\n",
    "        loftq_config=None,  # LoftQ for even better quality\n",
    "        use_gradient_checkpointing = \"unsloth\"\n",
    "    )\n",
    "    print(\"✅ Unsloth LoRA adapters added\")\n",
    "    \n",
    "    # 🚀 UNSLOTH: Optimized training arguments for 2x T4 GPUs (28GB total)\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=16,  # Larger batches with 2x T4 (28GB total)\n",
    "        gradient_accumulation_steps=8,  # Effective batch size = 4*2*2 = 16\n",
    "        warmup_steps=5,  # Quick warmup with Unsloth\n",
    "        #max_steps=250,  # Unsloth converges much faster (adjust based on data size)\n",
    "        num_train_epochs=1 , \n",
    "        learning_rate=2e-4,  # Higher LR works better with Unsloth\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,  # Frequent logging for monitoring\n",
    "        optim=\"adamw_8bit\",  # 8-bit optimizer for memory efficiency\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",  # Simple linear decay\n",
    "        seed=666,\n",
    "        output_dir=LORA_PATH,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=20,  # Save frequently for monitoring\n",
    "        save_total_limit=2,  # Keep only recent checkpoints\n",
    "        dataloader_pin_memory=False,  # Unsloth handles this\n",
    "        # Multi-GPU optimizations for 2x T4\n",
    "        dataloader_num_workers=4,  # Parallel data loading\n",
    "        remove_unused_columns=False,  # Keep all data\n",
    "        ddp_find_unused_parameters=False,  # DDP optimization\n",
    "        ddp_broadcast_buffers=False,  # Reduce communication overhead\n",
    "    )\n",
    "    print(\"✅ Unsloth training arguments configured for 2x T4\")\n",
    "    \n",
    "    # 🚀 UNSLOTH: Use SFTTrainer with Unsloth model\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"text\",  # Unsloth expects \"text\" field\n",
    "        max_seq_length=2048,\n",
    "        dataset_num_proc=4,  # More parallel processing for 2x T4\n",
    "        packing=False,  # Can try True for even faster training\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    print(\"🚀 Starting Unsloth training on 2x T4 (2x-5x faster than standard fine-tuning)...\")\n",
    "    \n",
    "    # 🚀 UNSLOTH: Train with optimized loop\n",
    "    trainer_stats = trainer.train()\n",
    "    \n",
    "    print(\"✅ Unsloth training completed!\")\n",
    "    print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"Samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "    print(f\"GPU utilization optimized for 2x T4 setup\")\n",
    "    \n",
    "    # 🚀 UNSLOTH: Save LoRA adapters in vLLM-compatible format\n",
    "    print(\"💾 Saving LoRA adapters for vLLM compatibility...\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(LORA_PATH)\n",
    "    \n",
    "    # Save model in PEFT format (vLLM compatible)\n",
    "    model.save_pretrained(LORA_PATH)\n",
    "    #model.save_pretrained(...)  \n",
    "    #tokenizer.save_pretrained(...)\n",
    "    folder=\"16 bit\"\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"forced_merged_4bit\",)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"✅ LoRA adapters saved to: {LORA_PATH} , model saved \")\n",
    "    print(\"🎯 Ready for vLLM inference!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69f944",
   "metadata": {},
   "source": [
    "# 🎯 2x T4 GPU Optimization Guide\n",
    "\n",
    "## ⚡ **Multi-GPU Configuration for TT-11**\n",
    "\n",
    "### **Your Setup: 2x T4 (28GB Total VRAM)**\n",
    "- **GPU 0**: ~14GB VRAM\n",
    "- **GPU 1**: ~14GB VRAM\n",
    "- **Total**: 28GB available for training\n",
    "\n",
    "### **Optimizations Applied:**\n",
    "\n",
    "#### **1. Model Distribution**\n",
    "```python\n",
    "device_map=\"auto\"  # Automatic distribution across GPUs\n",
    "max_memory={0: \"13GB\", 1: \"13GB\"}  # Reserve 1GB per GPU for operations\n",
    "```\n",
    "\n",
    "#### **2. Batch Size Scaling**\n",
    "```python\n",
    "per_device_train_batch_size=4,  # 4 samples per GPU (8 total)\n",
    "gradient_accumulation_steps=2,  # Effective batch = 4*2*2 = 16\n",
    "```\n",
    "\n",
    "#### **3. Memory Optimizations**\n",
    "```python\n",
    "load_in_4bit=True,              # 4-bit quantization saves ~75% memory\n",
    "use_gradient_checkpointing=True, # Trade compute for memory\n",
    "dataloader_pin_memory=False,     # Let Unsloth handle memory\n",
    "```\n",
    "\n",
    "#### **4. Multi-GPU Training**\n",
    "```python\n",
    "dataloader_num_workers=4,        # Parallel data loading\n",
    "ddp_find_unused_parameters=False, # DDP optimization\n",
    "ddp_broadcast_buffers=False,     # Reduce communication\n",
    "```\n",
    "\n",
    "### **Expected Performance:**\n",
    "- **Training Speed**: 3x-6x faster than single GPU\n",
    "- **Memory Usage**: ~12-13GB per GPU\n",
    "- **Effective Batch**: 16 samples (vs 4 on single GPU)\n",
    "- **Total Time**: 5-8 minutes for full training\n",
    "\n",
    "### **Troubleshooting 2x T4:**\n",
    "\n",
    "#### **If you get OOM (Out of Memory):**\n",
    "```python\n",
    "# Reduce batch size\n",
    "per_device_train_batch_size=2,   # 2 per GPU instead of 4\n",
    "gradient_accumulation_steps=4,   # Keep effective batch size\n",
    "\n",
    "# Or reduce sequence length\n",
    "max_seq_length=1024,             # Shorter sequences\n",
    "```\n",
    "\n",
    "#### **If training is slower than expected:**\n",
    "```python\n",
    "# Check GPU utilization\n",
    "nvidia-smi  # Should show ~90%+ on both GPUs\n",
    "\n",
    "# Increase batch size if memory allows\n",
    "per_device_train_batch_size=6,   # Try larger batches\n",
    "```\n",
    "\n",
    "#### **Memory Distribution Check:**\n",
    "```python\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_properties(i).total_memory // 1024**3}GB\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910e6893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:58:39.202054Z",
     "iopub.status.busy": "2025-09-18T14:58:39.201319Z",
     "iopub.status.idle": "2025-09-18T14:58:39.210240Z",
     "shell.execute_reply": "2025-09-18T14:58:39.209505Z",
     "shell.execute_reply.started": "2025-09-18T14:58:39.202027Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing validation_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile validation_vllm.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                           roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "from utils import build_validation_dataset, get_real_comment_validation_data\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "\n",
    "\n",
    "def run_validation_vllm():\n",
    "    \"\"\"Run validation using OpenSloth-trained model with vLLM for precise AUC\"\"\"\n",
    "    \n",
    "    # Get real comment validation data\n",
    "    val_df = get_real_comment_validation_data(DATA_PATH)\n",
    "    val_dataset = build_validation_dataset(val_df)\n",
    "    \n",
    "    print(f\"🔍 Running validation on {len(val_dataset)} real comments\")\n",
    "    \n",
    "    # 🎯 VLLM: Initialize with OpenSloth LoRA support for precise probabilities\n",
    "    llm = vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        gpu_memory_utilization=0.90,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=512,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "\n",
    "    texts = val_dataset[\"prompt\"]\n",
    "    true_labels = val_dataset[\"rule_violation\"]\n",
    "\n",
    "    # 🎯 VLLM: Generate with OpenSloth LoRA for most accurate probabilities\n",
    "    outputs = llm.generate(\n",
    "        texts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logprobs=20,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"opensloth_lora\", 1, LORA_PATH)  # Load OpenSloth LoRA\n",
    "    )\n",
    "\n",
    "    # Extract predictions and probabilities with vLLM precision\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    yes_token_id = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "    no_token_id = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "    \n",
    "    for out in outputs:\n",
    "        log_probs = out.outputs[0].logprobs[0]\n",
    "        \n",
    "        log_prob_yes = log_probs.get(yes_token_id)\n",
    "        log_prob_no = log_probs.get(no_token_id)\n",
    "        \n",
    "        if log_prob_yes is not None and log_prob_no is not None:\n",
    "            if log_prob_yes.logprob > log_prob_no.logprob:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "            \n",
    "            exp_pos = np.exp(log_prob_yes.logprob)\n",
    "            exp_neg = np.exp(log_prob_no.logprob)\n",
    "            prob_positive = exp_pos / (exp_pos + exp_neg)\n",
    "            probabilities.append(prob_positive)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "            probabilities.append(0.5)\n",
    "\n",
    "    return true_labels, predictions, probabilities, val_df\n",
    "\n",
    "\n",
    "def calculate_and_display_metrics(true_labels, predictions, probabilities):\n",
    "    \"\"\"Calculate comprehensive metrics and display results\"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 TT-12 VALIDATION RESULTS (OpenSloth + vLLM)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"🎯 Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"🎯 F1 Score:  {f1:.4f}\")\n",
    "    print(f\"🎯 Precision: {precision:.4f}\")\n",
    "    print(f\"🎯 Recall:    {recall:.4f}\")\n",
    "    print(f\"🎯 AUC Score: {auc:.4f} (High-precision vLLM)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    print(\"\\n📈 Confusion Matrix:\")\n",
    "    print(f\"True Negative: {cm[0,0]:4d} | False Positive: {cm[0,1]:4d}\")\n",
    "    print(f\"False Negative: {cm[1,0]:4d} | True Positive:  {cm[1,1]:4d}\")\n",
    "    \n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=['No Violation', 'Violation']))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy, 'f1': f1, 'precision': precision,\n",
    "        'recall': recall, 'auc': auc, 'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "\n",
    "def create_visualizations(true_labels, predictions, probabilities, metrics):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('TT-12: OpenSloth Training + vLLM Validation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    cm = metrics['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
    "                xticklabels=['No Violation', 'Violation'],\n",
    "                yticklabels=['No Violation', 'Violation'])\n",
    "    axes[0,0].set_title('Confusion Matrix')\n",
    "    axes[0,0].set_xlabel('Predicted')\n",
    "    axes[0,0].set_ylabel('Actual')\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(true_labels, probabilities)\n",
    "    axes[0,1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {metrics[\"auc\"]:.3f})')\n",
    "    axes[0,1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    axes[0,1].set_xlabel('False Positive Rate')\n",
    "    axes[0,1].set_ylabel('True Positive Rate')\n",
    "    axes[0,1].set_title('ROC Curve (vLLM High-Precision)')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    pos_probs = [p for p, t in zip(probabilities, true_labels) if t == 1]\n",
    "    neg_probs = [p for p, t in zip(probabilities, true_labels) if t == 0]\n",
    "    \n",
    "    axes[1,0].hist(neg_probs, bins=30, alpha=0.7, label='No Violation', color='blue', density=True)\n",
    "    axes[1,0].hist(pos_probs, bins=30, alpha=0.7, label='Violation', color='red', density=True)\n",
    "    axes[1,0].set_xlabel('Predicted Probability (vLLM Precision)')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Probability Distribution by True Label')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    metric_names = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'AUC']\n",
    "    metric_values = [metrics[k] for k in ['accuracy', 'f1', 'precision', 'recall', 'auc']]\n",
    "    \n",
    "    bars = axes[1,1].bar(metric_names, metric_values, color=['skyblue', 'lightgreen', 'orange', 'pink', 'gold'])\n",
    "    axes[1,1].set_ylabel('Score')\n",
    "    axes[1,1].set_title('Performance Metrics (OpenSloth + vLLM)')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                      f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/tt12_validation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_by_rule(true_labels, predictions, probabilities, val_df):\n",
    "    \"\"\"Analyze performance by rule type\"\"\"\n",
    "    \n",
    "    analysis_df = val_df.copy()\n",
    "    analysis_df['predictions'] = predictions\n",
    "    analysis_df['probabilities'] = probabilities\n",
    "    \n",
    "    print(\"\\n📊 PERFORMANCE BY RULE (vLLM High-Precision AUC):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    rule_metrics = []\n",
    "    for rule in analysis_df['rule'].unique():\n",
    "        rule_data = analysis_df[analysis_df['rule'] == rule]\n",
    "        \n",
    "        rule_true = rule_data['rule_violation'].values\n",
    "        rule_pred = rule_data['predictions'].values\n",
    "        rule_prob = rule_data['probabilities'].values\n",
    "        \n",
    "        rule_auc = roc_auc_score(rule_true, rule_prob) if len(np.unique(rule_true)) > 1 else np.nan\n",
    "        rule_acc = accuracy_score(rule_true, rule_pred)\n",
    "        rule_f1 = f1_score(rule_true, rule_pred) if len(np.unique(rule_true)) > 1 else np.nan\n",
    "        \n",
    "        print(f\"Rule: {rule}\\n  Samples: {len(rule_data)}\\n  Accuracy: {rule_acc:.3f}\\n  F1 Score: {rule_f1:.3f}\\n  AUC Score: {rule_auc:.3f}\\n\")\n",
    "        \n",
    "        rule_metrics.append({'rule': rule, 'samples': len(rule_data), 'accuracy': rule_acc, 'f1': rule_f1, 'auc': rule_auc})\n",
    "    \n",
    "    analysis_df.to_csv('/kaggle/working/tt12_detailed_results.csv', index=False)\n",
    "    pd.DataFrame(rule_metrics).to_csv('/kaggle/working/tt12_rule_metrics.csv', index=False)\n",
    "    \n",
    "    return rule_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"🔬 TT-12: OpenSloth Training + vLLM Validation\")\n",
    "    print(\"🚀 Multi-GPU training + High-precision inference!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    true_labels, predictions, probabilities, val_df = run_validation_vllm()\n",
    "    metrics = calculate_and_display_metrics(true_labels, predictions, probabilities)\n",
    "    create_visualizations(true_labels, predictions, probabilities, metrics)\n",
    "    analyze_by_rule(true_labels, predictions, probabilities, val_df)\n",
    "    \n",
    "    print(\"✅ TT-12 Validation completed!\")\n",
    "    print(\"📈 Visualizations saved: /kaggle/working/tt12_validation_results.png\")\n",
    "    print(\"📊 Detailed results: /kaggle/working/tt12_detailed_results.csv\")\n",
    "    print(\"📋 Rule metrics: /kaggle/working/tt12_rule_metrics.csv\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b04fb3",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T14:59:13.204530Z",
     "iopub.status.busy": "2025-09-18T14:59:13.203961Z",
     "iopub.status.idle": "2025-09-18T14:59:26.888214Z",
     "shell.execute_reply": "2025-09-18T14:59:26.887558Z",
     "shell.execute_reply.started": "2025-09-18T14:59:13.204502Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Deleting existing cache folder at data/cache_qwen3_dataset_for_opensloth/\n",
      "💾 Caching dataset to data/cache_qwen3_dataset_for_opensloth/\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "📊 Stratified sampling: 609 samples (30%)\n",
      "📊 Training data size: 609 samples\n",
      "📊 Rule distribution: {'No legal advice: Do not offer or request legal advice.': 305, 'No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.': 304}\n",
      "📊 Example-based training dataset: 2435 samples\n",
      "📊 Positive examples: 1218\n",
      "📊 Negative examples: 1217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313120e6b7364b49b225d82fccc48fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/2435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fb13225cac436eab7708aca3ebd12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset cached to data/cache_qwen3_dataset_for_opensloth/\n"
     ]
    }
   ],
   "source": [
    "from utils import cache_dataset\n",
    "cache_dataset(cache_path=\"data/cache_qwen3_dataset_for_opensloth/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd0717c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:58:45.445008Z",
     "iopub.status.busy": "2025-09-18T14:58:45.444339Z",
     "iopub.status.idle": "2025-09-18T14:58:45.453360Z",
     "shell.execute_reply": "2025-09-18T14:58:45.452639Z",
     "shell.execute_reply.started": "2025-09-18T14:58:45.444980Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing validation_transformers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile validation_transformers.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                           roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "from utils import build_validation_dataset, get_real_comment_validation_data\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "\n",
    "\n",
    "def run_validation_transformers():\n",
    "    \"\"\"Run validation using standard transformers with OpenSloth LoRA - Universal compatibility\"\"\"\n",
    "    \n",
    "    val_df = get_real_comment_validation_data(DATA_PATH)\n",
    "    val_dataset = build_validation_dataset(val_df)\n",
    "    \n",
    "    print(f\"🔍 Running validation on {len(val_dataset)} real comments (Transformers)\")\n",
    "    \n",
    "    print(\"📥 Loading base model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, local_files_only=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    \n",
    "    print(\"🔗 Loading OpenSloth LoRA adapters...\")\n",
    "    model = PeftModel.from_pretrained(model, LORA_PATH)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    \n",
    "    yes_token_id = tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "    no_token_id = tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "    \n",
    "    print(f\"🎯 Token IDs: Yes={yes_token_id}, No={no_token_id}\")\n",
    "    \n",
    "    texts = val_dataset[\"prompt\"]\n",
    "    true_labels = val_dataset[\"rule_violation\"]\n",
    "    \n",
    "    predictions, probabilities = [], []\n",
    "    batch_size = 8\n",
    "    \n",
    "    print(\"🚀 Running inference...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            yes_logits = next_token_logits[:, yes_token_id]\n",
    "            no_logits = next_token_logits[:, no_token_id]\n",
    "            \n",
    "            combined_logits = torch.stack([no_logits, yes_logits], dim=1)\n",
    "            probs = torch.softmax(combined_logits, dim=1)\n",
    "            \n",
    "            predictions.extend(torch.argmax(probs, dim=1).cpu().tolist())\n",
    "            probabilities.extend(probs[:, 1].cpu().tolist())\n",
    "    \n",
    "    print(\"✅ Inference completed!\")\n",
    "    return true_labels, predictions, probabilities, val_df\n",
    "\n",
    "\n",
    "def calculate_and_display_metrics(true_labels, predictions, probabilities):\n",
    "    \"\"\"Calculate comprehensive metrics and display results\"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 TT-12 VALIDATION RESULTS (OpenSloth + Transformers)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"🎯 Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"🎯 F1 Score:  {f1:.4f}\")\n",
    "    print(f\"🎯 Precision: {precision:.4f}\")\n",
    "    print(f\"🎯 Recall:    {recall:.4f}\")\n",
    "    print(f\"🎯 AUC Score: {auc:.4f} (Standard Transformers)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    print(\"\\n📈 Confusion Matrix:\")\n",
    "    print(f\"True Negative: {cm[0,0]:4d} | False Positive: {cm[0,1]:4d}\")\n",
    "    print(f\"False Negative: {cm[1,0]:4d} | True Positive:  {cm[1,1]:4d}\")\n",
    "    \n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=['No Violation', 'Violation']))\n",
    "    \n",
    "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall, 'auc': auc, 'confusion_matrix': cm}\n",
    "\n",
    "\n",
    "def create_visualizations(true_labels, predictions, probabilities, metrics):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('TT-12: OpenSloth Training + Transformers Validation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    cm = metrics['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
    "                xticklabels=['No Violation', 'Violation'],\n",
    "                yticklabels=['No Violation', 'Violation'])\n",
    "    axes[0,0].set_title('Confusion Matrix')\n",
    "    axes[0,0].set_xlabel('Predicted')\n",
    "    axes[0,0].set_ylabel('Actual')\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(true_labels, probabilities)\n",
    "    axes[0,1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {metrics[\"auc\"]:.3f})')\n",
    "    axes[0,1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    axes[0,1].set_xlabel('False Positive Rate')\n",
    "    axes[0,1].set_ylabel('True Positive Rate')\n",
    "    axes[0,1].set_title('ROC Curve (Transformers)')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    pos_probs = [p for p, t in zip(probabilities, true_labels) if t == 1]\n",
    "    neg_probs = [p for p, t in zip(probabilities, true_labels) if t == 0]\n",
    "    \n",
    "    axes[1,0].hist(neg_probs, bins=30, alpha=0.7, label='No Violation', color='blue', density=True)\n",
    "    axes[1,0].hist(pos_probs, bins=30, alpha=0.7, label='Violation', color='red', density=True)\n",
    "    axes[1,0].set_xlabel('Predicted Probability (Transformers)')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Probability Distribution by True Label')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    metric_names = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'AUC']\n",
    "    metric_values = [metrics[k] for k in ['accuracy', 'f1', 'precision', 'recall', 'auc']]\n",
    "    \n",
    "    bars = axes[1,1].bar(metric_names, metric_values, color=['skyblue', 'lightgreen', 'orange', 'pink', 'gold'])\n",
    "    axes[1,1].set_ylabel('Score')\n",
    "    axes[1,1].set_title('Performance Metrics (OpenSloth + Transformers)')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                      f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/tt12_transformers_validation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_by_rule(true_labels, predictions, probabilities, val_df):\n",
    "    \"\"\"Analyze performance by rule type\"\"\"\n",
    "    \n",
    "    analysis_df = val_df.copy()\n",
    "    analysis_df['predictions'] = predictions\n",
    "    analysis_df['probabilities'] = probabilities\n",
    "    \n",
    "    print(\"\\n📊 PERFORMANCE BY RULE (Transformers):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    rule_metrics = []\n",
    "    for rule in analysis_df['rule'].unique():\n",
    "        rule_data = analysis_df[analysis_df['rule'] == rule]\n",
    "        \n",
    "        rule_true = rule_data['rule_violation'].values\n",
    "        rule_pred = rule_data['predictions'].values\n",
    "        rule_prob = rule_data['probabilities'].values\n",
    "        \n",
    "        rule_auc = roc_auc_score(rule_true, rule_prob) if len(np.unique(rule_true)) > 1 else np.nan\n",
    "        rule_acc = accuracy_score(rule_true, rule_pred)\n",
    "        rule_f1 = f1_score(rule_true, rule_pred) if len(np.unique(rule_true)) > 1 else np.nan\n",
    "        \n",
    "        print(f\"Rule: {rule}\\n  Samples: {len(rule_data)}\\n  Accuracy: {rule_acc:.3f}\\n  F1 Score: {rule_f1:.3f}\\n  AUC Score: {rule_auc:.3f}\\n\")\n",
    "        \n",
    "        rule_metrics.append({'rule': rule, 'samples': len(rule_data), 'accuracy': rule_acc, 'f1': rule_f1, 'auc': rule_auc})\n",
    "    \n",
    "    analysis_df.to_csv('/kaggle/working/tt12_transformers_detailed_results.csv', index=False)\n",
    "    pd.DataFrame(rule_metrics).to_csv('/kaggle/working/tt12_transformers_rule_metrics.csv', index=False)\n",
    "    \n",
    "    return rule_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"🔬 TT-12: OpenSloth Training + Transformers Validation\")\n",
    "    print(\"🚀 Multi-GPU training + Universal compatibility!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    true_labels, predictions, probabilities, val_df = run_validation_transformers()\n",
    "    metrics = calculate_and_display_metrics(true_labels, predictions, probabilities)\n",
    "    create_visualizations(true_labels, predictions, probabilities, metrics)\n",
    "    analyze_by_rule(true_labels, predictions, probabilities, val_df)\n",
    "    \n",
    "    print(\"✅ TT-12 Transformers Validation completed!\")\n",
    "    print(\"📈 Visualizations saved: /kaggle/working/tt12_transformers_validation_results.png\")\n",
    "    print(\"📊 Detailed results: /kaggle/working/tt12_transformers_detailed_results.csv\")\n",
    "    print(\"📋 Rule metrics: /kaggle/working/tt12_transformers_rule_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6f23a14",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T15:12:01.604722Z",
     "iopub.status.busy": "2025-09-18T15:12:01.604229Z",
     "iopub.status.idle": "2025-09-18T15:12:54.322079Z",
     "shell.execute_reply": "2025-09-18T15:12:54.321281Z",
     "shell.execute_reply.started": "2025-09-18T15:12:01.604701Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2025-09-18 15:12:09.129503: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758208329.151014     551 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758208329.157677     551 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "✅ Using Qwen3 1.7B model from local Kaggle input\n",
      "🎯 TT-12: OpenSloth training + vLLM inference with 30% of data\n",
      "📊 Stratified sampling: True\n",
      "Global batch size: 16\n",
      "Gradient accumulation steps: 8\n",
      "Global batch size: 16\n",
      "[MP] Running on 2 GPUs\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2025-09-18 15:12:30.718078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758208350.739707     591 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758208350.746575     591 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-18 15:12:30.786377: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758208350.808511     590 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758208350.815251     590 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "✅ Using Qwen3 1.7B model from local Kaggle input\n",
      "🎯 TT-12: OpenSloth training + vLLM inference with 30% of data\n",
      "📊 Stratified sampling: True\n",
      "✅ Using Qwen3 1.7B model from local Kaggle input\n",
      "🎯 TT-12: OpenSloth training + vLLM inference with 30% of data\n",
      "📊 Stratified sampling: True\n",
      "\u001b[32m15:12:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:41\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir qwen3_1.7b_opensloth_lora_validation/\u001b[0m\n",
      "\u001b[32m15:12:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:44\u001b[0m | \u001b[1m🚀 Starting total training timer\u001b[0m\n",
      "Using compiler location: .cache/unsloth_compiled_cache_1\n",
      "Using compiler location: .cache/unsloth_compiled_cache_0\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/opensloth/scripts/opensloth_sft_trainer.py\", line 49, in train_on_single_gpu\n",
      "    trainer, model, tokenizer = setup_model_and_training(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/opensloth/opensloth_trainer_setup.py\", line 59, in setup_model_and_training\n",
      "    model, tokenizer = init_model_and_tokenizer(opensloth_config)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/opensloth/init_modules.py\", line 32, in init_model_and_tokenizer\n",
      "    model, tokenizer = FastModel.from_pretrained(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\", line 697, in from_pretrained\n",
      "    model_types, supports_sdpa = unsloth_compile_transformers(\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\", line 1199, in unsloth_compile_transformers\n",
      "    _unsloth_compile_transformers(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth_zoo/compiler.py\", line 2133, in unsloth_compile_transformers\n",
      "    exec(f\"{model_location}.{module} = combined_module.{module}\", globals(), locals())\n",
      "  File \"<string>\", line 1, in <module>\n",
      "AttributeError: module 'unsloth_compiled_module_qwen3' has no attribute 'Qwen3RotaryEmbedding'\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/opensloth/scripts/opensloth_sft_trainer.py\", line 49, in train_on_single_gpu\n",
      "    trainer, model, tokenizer = setup_model_and_training(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/opensloth/opensloth_trainer_setup.py\", line 59, in setup_model_and_training\n",
      "    model, tokenizer = init_model_and_tokenizer(opensloth_config)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/opensloth/init_modules.py\", line 32, in init_model_and_tokenizer\n",
      "    model, tokenizer = FastModel.from_pretrained(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\", line 697, in from_pretrained\n",
      "    model_types, supports_sdpa = unsloth_compile_transformers(\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\", line 1199, in unsloth_compile_transformers\n",
      "    _unsloth_compile_transformers(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth_zoo/compiler.py\", line 2133, in unsloth_compile_transformers\n",
      "    exec(f\"{model_location}.{module} = combined_module.{module}\", globals(), locals())\n",
      "  File \"<string>\", line 1, in <module>\n",
      "AttributeError: module 'unsloth_compiled_module_qwen3' has no attribute 'Qwen3RotaryEmbedding'\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/train_opensloth.py\", line 63, in <module>\n",
      "    run_mp_training(opensloth_config.devices, opensloth_config, training_config)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/opensloth/scripts/opensloth_sft_trainer.py\", line 256, in run_mp_training\n",
      "    raise Exception(\"Error in training\")\n",
      "Exception: Error in training\n"
     ]
    }
   ],
   "source": [
    "!python train_opensloth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fdab6c5-c83c-492c-8e94-d0b0aeb7f30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:21:33.834243Z",
     "iopub.status.busy": "2025-09-18T15:21:33.833507Z",
     "iopub.status.idle": "2025-09-18T15:22:10.979350Z",
     "shell.execute_reply": "2025-09-18T15:22:10.978139Z",
     "shell.execute_reply.started": "2025-09-18T15:21:33.834217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2025-09-18 15:21:40.508616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758208900.531111     804 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758208900.538182     804 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "✅ Using Qwen3 1.7B model from local Kaggle input\n",
      "🎯 TT-12: OpenSloth training + vLLM inference with 30% of data\n",
      "📊 Stratified sampling: True\n",
      "📊 Stratified sampling: 609 samples (30%)\n",
      "📊 Training data size: 609 samples\n",
      "📊 Rule distribution: {'No legal advice: Do not offer or request legal advice.': 305, 'No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.': 304}\n",
      "📊 Example-based training dataset: 2435 samples\n",
      "📊 Positive examples: 1218\n",
      "📊 Negative examples: 1217\n",
      "Training dataset size: 2435 samples\n",
      "Available GPUs: 2\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "✅ Unsloth model loaded with 4-bit quantization across 2x T4\n",
      "Unsloth 2025.5.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "✅ Unsloth LoRA adapters added\n",
      "✅ Unsloth training arguments configured for 2x T4\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=4): 100%|█| 2435/2435 [00:02<00:00, 1153.\n",
      "🚀 Starting Unsloth training on 2x T4 (2x-5x faster than standard fine-tuning)...\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n",
      "   \\\\   /|    Num examples = 2,435 | Num Epochs = 1 | Total steps = 19\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 8 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 17,432,576/7,000,000,000 (0.25% trained)\n",
      "  0%|                                                    | 0/19 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/kaggle/working/train_unsloth.py\", line 117, in <module>\n",
      "    main()\n",
      "  File \"/kaggle/working/train_unsloth.py\", line 90, in main\n",
      "    trainer_stats = trainer.train()\n",
      "                    ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 268, in _fast_inner_training_loop\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/unsloth_zoo/loss_utils.py\", line 273, in _unsloth_get_batch_samples\n",
      "    batch_samples += [next(epoch_iterator)]\n",
      "                      ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\", line 566, in __iter__\n",
      "    current_batch = next(dataloader_iter)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
      "    return self._process_data(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
      "    raise exception\n",
      "ValueError: Caught ValueError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 777, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 739, in as_tensor\n",
      "    return torch.tensor(value)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: too many dimensions 'str'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 46, in __call__\n",
      "    return self.torch_call(features)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 1013, in torch_call\n",
      "    batch = pad_without_fast_tokenizer_warning(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\", line 67, in pad_without_fast_tokenizer_warning\n",
      "    padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3407, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 241, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 793, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "  0%|          | 0/19 [00:01<?, ?it/s]                                          \n"
     ]
    }
   ],
   "source": [
    "!python train_unsloth.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ef5eb",
   "metadata": {},
   "source": [
    "# Appendix: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef0067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Run Validation**\n",
    "#@markdown Choose your validation method:\n",
    "VALIDATION_METHOD = \"vLLM\" #@param [\"vLLM\", \"Transformers\"]\n",
    "\n",
    "if VALIDATION_METHOD == \"vLLM\":\n",
    "    print(\"🚀 Running vLLM validation for maximum speed and precision...\")\n",
    "    !python validation_vllm.py\n",
    "else:\n",
    "    print(\"⚙️ Running Transformers validation for universal compatibility...\")\n",
    "    !python validation_transformers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display the validation results image\n",
    "if VALIDATION_METHOD == \"vLLM\":\n",
    "    display(Image(filename='/kaggle/working/tt12_validation_results.png'))\n",
    "else:\n",
    "    display(Image(filename='/kaggle/working/tt12_transformers_validation_results.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display the detailed results CSV\n",
    "if VALIDATION_METHOD == \"vLLM\":\n",
    "    df_detailed = pd.read_csv('/kaggle/working/tt12_detailed_results.csv')\n",
    "else:\n",
    "    df_detailed = pd.read_csv('/kaggle/working/tt12_transformers_detailed_results.csv')\n",
    "\n",
    "df_detailed.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 452934,
     "modelInstanceId": 436166,
     "sourceId": 583951,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 449553,
     "modelInstanceId": 432662,
     "sourceId": 579809,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
