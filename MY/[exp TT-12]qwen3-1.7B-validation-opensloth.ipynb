{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b63ac8c",
   "metadata": {},
   "source": [
    "# Alternative Validation Options\n",
    "ASSS\n",
    "NO MORE TT GAMES , This will be the only and the last thing ill nee before heading onto the submissions \n",
    "\n",
    "\n",
    "\n",
    "## üîß **Choose Your Validation Method:**\n",
    "\n",
    "This notebook now provides **two validation approaches**:\n",
    "\n",
    "### **Option 1: vLLM Validation (Original)**\n",
    "- **Pros**: Fastest inference, most precise probability calculations\n",
    "- **Cons**: Hardware compatibility issues with certain GPU/model combinations\n",
    "- **Use when**: You have compatible hardware and need maximum speed\n",
    "\n",
    "### **Option 2: Standard Transformers Validation (New)**\n",
    "- **Pros**: Universal compatibility, works with any Unsloth model, reliable\n",
    "- **Cons**: Slower than vLLM, but still faster than training\n",
    "- **Use when**: vLLM has compatibility issues or you want guaranteed reliability\n",
    "\n",
    "**Both methods produce identical metrics and visualizations** - the choice is purely based on your hardware compatibility and speed requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ea70f",
   "metadata": {},
   "source": [
    "# TT-12: Validation-Focused Training with OpenSloth + vLLM\n",
    "\n",
    "This notebook implements the same validation-focused approach as TT-10, but using **OpenSloth** for training.\n",
    "\n",
    "**Key Changes from TT-11:**\n",
    "- **üöÄ OpenSloth Training**: Utilizes OpenSloth for multi-GPU training orchestration.\n",
    "- **üéØ vLLM Inference**: Most accurate AUC calculations with precise log probabilities.\n",
    "- **üíæ Memory Efficient**: Optimized for 2x T4 GPU setup.\n",
    "\n",
    "**Methodology:**\n",
    "- **Training**: Model learns from positive/negative examples using OpenSloth.\n",
    "- **Validation**: Model predicts on real `body` comments with vLLM for precise probabilities.\n",
    "- **Analysis**: Comprehensive metrics to understand generalization from examples to real data.\n",
    "\n",
    "**Features:**\n",
    "- **Stratified Sampling**: Controllable % of training data while maintaining rule distribution.\n",
    "- **Example-Based Training**: Similar to test-time training approach.\n",
    "- **Real Comment Validation**: Test on actual comments with vLLM precision.\n",
    "- **Comprehensive Metrics**: AUC, F1, Recall, Precision, Confusion Matrix.\n",
    "- **Visualizations**: Performance plots and analysis.\n",
    "- **4-bit + LoRA**: Memory-efficient training, vLLM-compatible inference.\n",
    "\n",
    "**Benefits:**\n",
    "- **Structured Training**: OpenSloth provides a clear configuration for multi-GPU setups.\n",
    "- **Most Accurate AUC**: vLLM gives precise probability calculations.\n",
    "- **Combined Power**: OpenSloth for training, vLLM for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies - OpenSloth + vLLM + Analysis setup\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --upgrade-strategy eager \"opensloth @ git+https://github.com/unslothai/opensloth.git\"\n",
    "!pip install 'vllm==0.10.0' 'clean-text' 'scikit-learn' 'matplotlib' 'seaborn'\n",
    "\n",
    "print(\"‚úÖ TT-12 Dependencies installed:\")\n",
    "print(\"üöÄ OpenSloth: Multi-GPU training\")\n",
    "print(\"üéØ vLLM: Precise inference\") \n",
    "print(\"üìä Analysis libraries: scikit-learn, matplotlib, seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11239ed",
   "metadata": {},
   "source": [
    "# 1. Configuration and Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d63f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants.py\n",
    "# Using base Qwen3 1.7B model from Kaggle input (no internet needed)\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen-3/transformers/1.7b/1\"  # Update this path as needed\n",
    "LORA_PATH = \"qwen3_1.7b_opensloth_lora_validation/\"  # OpenSloth LoRA output path for validation\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "# TT-12 Validation Parameters\n",
    "TRAINING_DATA_PERCENTAGE = 1.0  # Controllable % of training data (0.1 = 10%, 1.0 = 100%)\n",
    "USE_STRATIFIED_SAMPLING = True  # Maintain rule distribution when sampling\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''\n",
    "\n",
    "print(\"‚úÖ Using Qwen3 1.7B model from local Kaggle input\")\n",
    "print(f\"üéØ TT-12: OpenSloth training + vLLM inference with {TRAINING_DATA_PERCENTAGE*100:.0f}% of data\")\n",
    "print(f\"üìä Stratified sampling: {USE_STRATIFIED_SAMPLING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT, TRAINING_DATA_PERCENTAGE, USE_STRATIFIED_SAMPLING, DATA_PATH\n",
    "import random, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import os\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_example_based_training_data(data_path):\n",
    "    \"\"\"\n",
    "    TT-12: Create training data from examples (like test-time training)\n",
    "    This trains the model on examples, not actual comments\n",
    "    \"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    \n",
    "    # Sample data if needed while maintaining rule distribution\n",
    "    if TRAINING_DATA_PERCENTAGE < 1.0:\n",
    "        if USE_STRATIFIED_SAMPLING:\n",
    "            # Stratified sampling to maintain rule distribution\n",
    "            train_dataset = train_dataset.groupby('rule', group_keys=False).apply(\n",
    "                lambda x: x.sample(frac=TRAINING_DATA_PERCENTAGE, random_state=42)\n",
    "            ).reset_index(drop=True)\n",
    "            print(f\"üìä Stratified sampling: {len(train_dataset)} samples ({TRAINING_DATA_PERCENTAGE*100:.0f}%)\")\n",
    "        else:\n",
    "            # Simple random sampling\n",
    "            train_dataset = train_dataset.sample(frac=TRAINING_DATA_PERCENTAGE, random_state=42).reset_index(drop=True)\n",
    "            print(f\"üìä Random sampling: {len(train_dataset)} samples ({TRAINING_DATA_PERCENTAGE*100:.0f}%)\")\n",
    "    \n",
    "    print(f\"üìä Training data size: {len(train_dataset)} samples\")\n",
    "    print(f\"üìä Rule distribution: {train_dataset['rule'].value_counts().to_dict()}\")\n",
    "    \n",
    "    flatten = []\n",
    "    \n",
    "    # Create training data from examples (similar to test-time training)\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = train_dataset[[\"rule\",\"subreddit\",\n",
    "                                        \"positive_example_1\",\"positive_example_2\",\n",
    "                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                # Use positive example as the \"body\" to classify\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"  # other positive\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                # negative_example randomly selected\n",
    "                sub_dataset[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"negative_example_1\"],\n",
    "                    sub_dataset[\"negative_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 1  # Positive examples violate rules\n",
    "\n",
    "            else:  # violation_type == \"negative\"\n",
    "                # Use negative example as the \"body\" to classify\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                sub_dataset[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"positive_example_1\"],\n",
    "                    sub_dataset[\"positive_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 0  # Negative examples don't violate rules\n",
    "\n",
    "            # Drop original candidate columns\n",
    "            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    # Merge all DataFrames\n",
    "    example_training_df = pd.concat(flatten, axis=0)\n",
    "    example_training_df = example_training_df.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    print(f\"üìä Example-based training dataset: {len(example_training_df)} samples\")\n",
    "    print(f\"üìä Positive examples: {sum(example_training_df['rule_violation'] == 1)}\")\n",
    "    print(f\"üìä Negative examples: {sum(example_training_df['rule_violation'] == 0)}\")\n",
    "    \n",
    "    return example_training_df\n",
    "\n",
    "\n",
    "def get_real_comment_validation_data(data_path):\n",
    "    \"\"\"\n",
    "    TT-12: Get real comments with labels for validation\n",
    "    This is what we actually want to predict\n",
    "    \"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    \n",
    "    # Use actual comments and their labels for validation\n",
    "    validation_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                                  \"positive_example_1\",\"positive_example_2\",\n",
    "                                  \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # Randomly select positive_example and negative_example for prompts\n",
    "    validation_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(validation_df)) < 0.5,\n",
    "        validation_df[\"positive_example_1\"],\n",
    "        validation_df[\"positive_example_2\"]\n",
    "    )\n",
    "    validation_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(validation_df)) < 0.5,\n",
    "        validation_df[\"negative_example_1\"],\n",
    "        validation_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # Drop original candidate columns\n",
    "    validation_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                               \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "    \n",
    "    print(f\"üìä Real comment validation dataset: {len(validation_df)} samples\")\n",
    "    print(f\"üìä Rule violations: {sum(validation_df['rule_violation'] == 1)} positive, {sum(validation_df['rule_violation'] == 0)} negative\")\n",
    "    \n",
    "    return validation_df\n",
    "\n",
    "\n",
    "def build_dataset_for_opensloth(dataframe, tokenizer):\n",
    "    \"\"\"Build dataset for OpenSloth training with proper text formatting\"\"\"\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "    \n",
    "    # OpenSloth uses a text field\n",
    "    dataframe[\"text\"] = dataframe.apply(lambda row: \n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": row[\"prompt\"]},\n",
    "                {\"role\": \"assistant\", \"content\": POSITIVE_ANSWER if row[\"rule_violation\"] == 1 else NEGATIVE_ANSWER},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    dataset = Dataset.from_pandas(dataframe[[\"text\"]])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_validation_dataset(dataframe):\n",
    "    \"\"\"Build dataset for validation (keep labels for evaluation)\"\"\"\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "    dataframe = dataframe[[\"prompt\", \"rule_violation\"]]  # Keep true labels for evaluation\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    return dataset\n",
    "\n",
    "def cache_dataset(cache_path):\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"üíæ Loading cached dataset from {cache_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üíæ Caching dataset to {cache_path}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Qwen3-0.6B-Base-bnb-4bit\",\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + '<|im_end|>' }}{% endif %}{% endfor %}\"\n",
    "    \n",
    "    \n",
    "    train_df = get_example_based_training_data(DATA_PATH)\n",
    "    dataset = build_dataset_for_opensloth(train_df, tokenizer)\n",
    "    \n",
    "    # This is a bit of a hack. SFTTrainer is used for its dataset processing\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        args=SFTConfig(\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=2048,\n",
    "            dataset_num_proc=2,\n",
    "            packing=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    trainer.train_dataset.save_to_disk(cache_path)\n",
    "    print(f\"‚úÖ Dataset cached to {cache_path}\")\n",
    "\n",
    "def get_cached_dataset(cache_path):\n",
    "    if not os.path.exists(cache_path):\n",
    "        raise RuntimeError(\"Dataset cache not found. Please run the caching step first.\")\n",
    "    return load_from_disk(cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_opensloth.py\n",
    "from opensloth.opensloth_config import (\n",
    "    FastModelArgs,\n",
    "    LoraArgs,\n",
    "    OpenSlothConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from opensloth.scripts.opensloth_sft_trainer import run_mp_training, setup_envs\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH\n",
    "import torch\n",
    "\n",
    "# OpenSloth Configuration for 2 GPUs\n",
    "GLOBAL_BZ = 16\n",
    "DEVICES = [i for i in range(torch.cuda.device_count())]\n",
    "BZ = 2  # Batch size per device\n",
    "\n",
    "opensloth_config = OpenSlothConfig(\n",
    "    data_cache_path=\"data/cache_qwen3_dataset_for_opensloth/\",\n",
    "    devices=DEVICES,\n",
    "    fast_model_args=FastModelArgs(\n",
    "        model_name=BASE_MODEL_PATH,\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "        local_files_only=True,\n",
    "        trust_remote_code=True,\n",
    "    ),\n",
    "    lora_args=LoraArgs(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_rslora=False,\n",
    "    ),\n",
    "    sequence_packing=True,\n",
    ")\n",
    "\n",
    "training_config = TrainingArguments(\n",
    "    output_dir=LORA_PATH,\n",
    "    per_device_train_batch_size=BZ,\n",
    "    gradient_accumulation_steps=GLOBAL_BZ // (len(DEVICES) * BZ) if DEVICES else 1,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=1, # Using max_steps instead\n",
    "    max_steps=60,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=5,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "\n",
    "    print(f\"Global batch size: {len(DEVICES) * BZ * training_config.gradient_accumulation_steps}\")\n",
    "    print(f\"Gradient accumulation steps: {training_config.gradient_accumulation_steps}\")\n",
    "\n",
    "    setup_envs(opensloth_config, training_config)\n",
    "    run_mp_training(opensloth_config.devices, opensloth_config, training_config)\n",
    "    \n",
    "    print(f\"‚úÖ OpenSloth training completed! LoRA adapters saved to: {LORA_PATH}\")\n",
    "    print(\"üéØ Ready for vLLM inference!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69f944",
   "metadata": {},
   "source": [
    "# üéØ 2x T4 GPU Optimization Guide\n",
    "\n",
    "## ‚ö° **Multi-GPU Configuration for TT-11**\n",
    "\n",
    "### **Your Setup: 2x T4 (28GB Total VRAM)**\n",
    "- **GPU 0**: ~14GB VRAM\n",
    "- **GPU 1**: ~14GB VRAM\n",
    "- **Total**: 28GB available for training\n",
    "\n",
    "### **Optimizations Applied:**\n",
    "\n",
    "#### **1. Model Distribution**\n",
    "```python\n",
    "device_map=\"auto\"  # Automatic distribution across GPUs\n",
    "max_memory={0: \"13GB\", 1: \"13GB\"}  # Reserve 1GB per GPU for operations\n",
    "```\n",
    "\n",
    "#### **2. Batch Size Scaling**\n",
    "```python\n",
    "per_device_train_batch_size=4,  # 4 samples per GPU (8 total)\n",
    "gradient_accumulation_steps=2,  # Effective batch = 4*2*2 = 16\n",
    "```\n",
    "\n",
    "#### **3. Memory Optimizations**\n",
    "```python\n",
    "load_in_4bit=True,              # 4-bit quantization saves ~75% memory\n",
    "use_gradient_checkpointing=True, # Trade compute for memory\n",
    "dataloader_pin_memory=False,     # Let Unsloth handle memory\n",
    "```\n",
    "\n",
    "#### **4. Multi-GPU Training**\n",
    "```python\n",
    "dataloader_num_workers=4,        # Parallel data loading\n",
    "ddp_find_unused_parameters=False, # DDP optimization\n",
    "ddp_broadcast_buffers=False,     # Reduce communication\n",
    "```\n",
    "\n",
    "### **Expected Performance:**\n",
    "- **Training Speed**: 3x-6x faster than single GPU\n",
    "- **Memory Usage**: ~12-13GB per GPU\n",
    "- **Effective Batch**: 16 samples (vs 4 on single GPU)\n",
    "- **Total Time**: 5-8 minutes for full training\n",
    "\n",
    "### **Troubleshooting 2x T4:**\n",
    "\n",
    "#### **If you get OOM (Out of Memory):**\n",
    "```python\n",
    "# Reduce batch size\n",
    "per_device_train_batch_size=2,   # 2 per GPU instead of 4\n",
    "gradient_accumulation_steps=4,   # Keep effective batch size\n",
    "\n",
    "# Or reduce sequence length\n",
    "max_seq_length=1024,             # Shorter sequences\n",
    "```\n",
    "\n",
    "#### **If training is slower than expected:**\n",
    "```python\n",
    "# Check GPU utilization\n",
    "nvidia-smi  # Should show ~90%+ on both GPUs\n",
    "\n",
    "# Increase batch size if memory allows\n",
    "per_device_train_batch_size=6,   # Try larger batches\n",
    "```\n",
    "\n",
    "#### **Memory Distribution Check:**\n",
    "```python\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_properties(i).total_memory // 1024**3}GB\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile validation_vllm.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                           roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "from utils import build_validation_dataset, get_real_comment_validation_data\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "\n",
    "\n",
    "def run_validation_vllm():\n",
    "    \"\"\"Run validation using OpenSloth-trained model with vLLM for precise AUC\"\"\"\n",
    "    \n",
    "    # Get real comment validation data\n",
    "    val_df = get_real_comment_validation_data(DATA_PATH)\n",
    "    val_dataset = build_validation_dataset(val_df)\n",
    "    \n",
    "    print(f\"üîç Running validation on {len(val_dataset)} real comments\")\n",
    "    \n",
    "    # üéØ VLLM: Initialize with OpenSloth LoRA support for precise probabilities\n",
    "    llm = vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        gpu_memory_utilization=0.90,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=512,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "\n",
    "    texts = val_dataset[\"prompt\"]\n",
    "    true_labels = val_dataset[\"rule_violation\"]\n",
    "\n",
    "    # üéØ VLLM: Generate with OpenSloth LoRA for most accurate probabilities\n",
    "    outputs = llm.generate(\n",
    "        texts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logprobs=20,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"opensloth_lora\", 1, LORA_PATH)  # Load OpenSloth LoRA\n",
    "    )\n",
    "\n",
    "    # Extract predictions and probabilities with vLLM precision\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    yes_token_id = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "    no_token_id = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "    \n",
    "    for out in outputs:\n",
    "        log_probs = out.outputs[0].logprobs[0]\n",
    "        \n",
    "        log_prob_yes = log_probs.get(yes_token_id)\n",
    "        log_prob_no = log_probs.get(no_token_id)\n",
    "        \n",
    "        if log_prob_yes is not None and log_prob_no is not None:\n",
    "            if log_prob_yes.logprob > log_prob_no.logprob:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "            \n",
    "            exp_pos = np.exp(log_prob_yes.logprob)\n",
    "            exp_neg = np.exp(log_prob_no.logprob)\n",
    "            prob_positive = exp_pos / (exp_pos + exp_neg)\n",
    "            probabilities.append(prob_positive)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "            probabilities.append(0.5)\n",
    "\n",
    "    return true_labels, predictions, probabilities, val_df\n",
    "\n",
    "\n",
    "def calculate_and_display_metrics(true_labels, predictions, probabilities):\n",
    "    \"\"\"Calculate comprehensive metrics and display results\"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä TT-12 VALIDATION RESULTS (OpenSloth + vLLM)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üéØ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"üéØ F1 Score:  {f1:.4f}\")\n",
    "    print(f\"üéØ Precision: {precision:.4f}\")\n",
    "    print(f\"üéØ Recall:    {recall:.4f}\")\n",
    "    print(f\"üéØ AUC Score: {auc:.4f} (High-precision vLLM)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    print(\"\\nüìà Confusion Matrix:\")\n",
    "    print(f\"True Negative: {cm[0,0]:4d} | False Positive: {cm[0,1]:4d}\")\n",
    "    print(f\"False Negative: {cm[1,0]:4d} | True Positive:  {cm[1,1]:4d}\")\n",
    "    \n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=['No Violation', 'Violation']))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy, 'f1': f1, 'precision': precision,\n",
    "        'recall': recall, 'auc': auc, 'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "\n",
    "def create_visualizations(true_labels, predictions, probabilities, metrics):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('TT-12: OpenSloth Training + vLLM Validation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    cm = metrics['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
    "                xticklabels=['No Violation', 'Violation'],\n",
    "                yticklabels=['No Violation', 'Violation'])\n",
    "    axes[0,0].set_title('Confusion Matrix')\n",
    "    axes[0,0].set_xlabel('Predicted')\n",
    "    axes[0,0].set_ylabel('Actual')\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(true_labels, probabilities)\n",
    "    axes[0,1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {metrics[\"auc\"]:.3f})')\n",
    "    axes[0,1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    axes[0,1].set_xlabel('False Positive Rate')\n",
    "    axes[0,1].set_ylabel('True Positive Rate')\n",
    "    axes[0,1].set_title('ROC Curve (vLLM High-Precision)')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    pos_probs = [p for p, t in zip(probabilities, true_labels) if t == 1]\n",
    "    neg_probs = [p for p, t in zip(probabilities, true_labels) if t == 0]\n",
    "    \n",
    "    axes[1,0].hist(neg_probs, bins=30, alpha=0.7, label='No Violation', color='blue', density=True)\n",
    "    axes[1,0].hist(pos_probs, bins=30, alpha=0.7, label='Violation', color='red', density=True)\n",
    "    axes[1,0].set_xlabel('Predicted Probability (vLLM Precision)')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Probability Distribution by True Label')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    metric_names = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'AUC']\n",
    "    metric_values = [metrics[k] for k in ['accuracy', 'f1', 'precision', 'recall', 'auc']]\n",
    "    \n",
    "    bars = axes[1,1].bar(metric_names, metric_values, color=['skyblue', 'lightgreen', 'orange', 'pink', 'gold'])\n",
    "    axes[1,1].set_ylabel('Score')\n",
    "    axes[1,1].set_title('Performance Metrics (OpenSloth + vLLM)')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                      f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/tt12_validation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_by_rule(true_labels, predictions, probabilities, val_df):\n",
    "    \"\"\"Analyze performance by rule type\"\"\"\n",
    "    \n",
    "    analysis_df = val_df.copy()\n",
    "    analysis_df['predictions'] = predictions\n",
    "    analysis_df['probabilities'] = probabilities\n",
    "    \n",
    "    print(\"\\nüìä PERFORMANCE BY RULE (vLLM High-Precision AUC):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    rule_metrics = []\n",
    "    for rule in analysis_df['rule'].unique():\n",
    "        rule_data = analysis_df[analysis_df['rule'] == rule]\n",
    "        \n",
    "        rule_true = rule_data['rule_violation'].values\n",
    "        rule_pred = rule_data['predictions'].values\n",
    "        rule_prob = rule_data['probabilities'].values\n",
    "        \n",
    "        rule_auc = roc_auc_score(rule_true, rule_prob) if len(np.unique(rule_true)) > 1 else np.nan\n",
    "        rule_acc = accuracy_score(rule_true, rule_pred)\n",
    "        rule_f1 = f1_score(rule_true, rule_pred) if len(np.unique(rule_true)) > 1 else np.nan\n",
    "        \n",
    "        print(f\"Rule: {rule}\\n  Samples: {len(rule_data)}\\n  Accuracy: {rule_acc:.3f}\\n  F1 Score: {rule_f1:.3f}\\n  AUC Score: {rule_auc:.3f}\\n\")\n",
    "        \n",
    "        rule_metrics.append({'rule': rule, 'samples': len(rule_data), 'accuracy': rule_acc, 'f1': rule_f1, 'auc': rule_auc})\n",
    "    \n",
    "    analysis_df.to_csv('/kaggle/working/tt12_detailed_results.csv', index=False)\n",
    "    pd.DataFrame(rule_metrics).to_csv('/kaggle/working/tt12_rule_metrics.csv', index=False)\n",
    "    \n",
    "    return rule_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"üî¨ TT-12: OpenSloth Training + vLLM Validation\")\n",
    "    print(\"üöÄ Multi-GPU training + High-precision inference!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    true_labels, predictions, probabilities, val_df = run_validation_vllm()\n",
    "    metrics = calculate_and_display_metrics(true_labels, predictions, probabilities)\n",
    "    create_visualizations(true_labels, predictions, probabilities, metrics)\n",
    "    analyze_by_rule(true_labels, predictions, probabilities, val_df)\n",
    "    \n",
    "    print(\"‚úÖ TT-12 Validation completed!\")\n",
    "    print(\"üìà Visualizations saved: /kaggle/working/tt12_validation_results.png\")\n",
    "    print(\"üìä Detailed results: /kaggle/working/tt12_detailed_results.csv\")\n",
    "    print(\"üìã Rule metrics: /kaggle/working/tt12_rule_metrics.csv\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd0717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile validation_transformers.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                           roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "from utils import build_validation_dataset, get_real_comment_validation_data\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "\n",
    "\n",
    "def run_validation_transformers():\n",
    "    \"\"\"Run validation using standard transformers with OpenSloth LoRA - Universal compatibility\"\"\"\n",
    "    \n",
    "    val_df = get_real_comment_validation_data(DATA_PATH)\n",
    "    val_dataset = build_validation_dataset(val_df)\n",
    "    \n",
    "    print(f\"üîç Running validation on {len(val_dataset)} real comments (Transformers)\")\n",
    "    \n",
    "    print(\"üì• Loading base model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, local_files_only=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    \n",
    "    print(\"üîó Loading OpenSloth LoRA adapters...\")\n",
    "    model = PeftModel.from_pretrained(model, LORA_PATH)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    \n",
    "    yes_token_id = tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "    no_token_id = tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "    \n",
    "    print(f\"üéØ Token IDs: Yes={yes_token_id}, No={no_token_id}\")\n",
    "    \n",
    "    texts = val_dataset[\"prompt\"]\n",
    "    true_labels = val_dataset[\"rule_violation\"]\n",
    "    \n",
    "    predictions, probabilities = [], []\n",
    "    batch_size = 8\n",
    "    \n",
    "    print(\"üöÄ Running inference...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            yes_logits = next_token_logits[:, yes_token_id]\n",
    "            no_logits = next_token_logits[:, no_token_id]\n",
    "            \n",
    "            combined_logits = torch.stack([no_logits, yes_logits], dim=1)\n",
    "            probs = torch.softmax(combined_logits, dim=1)\n",
    "            \n",
    "            predictions.extend(torch.argmax(probs, dim=1).cpu().tolist())\n",
    "            probabilities.extend(probs[:, 1].cpu().tolist())\n",
    "    \n",
    "    print(\"‚úÖ Inference completed!\")\n",
    "    return true_labels, predictions, probabilities, val_df\n",
    "\n",
    "\n",
    "def calculate_and_display_metrics(true_labels, predictions, probabilities):\n",
    "    \"\"\"Calculate comprehensive metrics and display results\"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä TT-12 VALIDATION RESULTS (OpenSloth + Transformers)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üéØ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"üéØ F1 Score:  {f1:.4f}\")\n",
    "    print(f\"üéØ Precision: {precision:.4f}\")\n",
    "    print(f\"üéØ Recall:    {recall:.4f}\")\n",
    "    print(f\"üéØ AUC Score: {auc:.4f} (Standard Transformers)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    print(\"\\nüìà Confusion Matrix:\")\n",
    "    print(f\"True Negative: {cm[0,0]:4d} | False Positive: {cm[0,1]:4d}\")\n",
    "    print(f\"False Negative: {cm[1,0]:4d} | True Positive:  {cm[1,1]:4d}\")\n",
    "    \n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=['No Violation', 'Violation']))\n",
    "    \n",
    "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall, 'auc': auc, 'confusion_matrix': cm}\n",
    "\n",
    "\n",
    "def create_visualizations(true_labels, predictions, probabilities, metrics):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('TT-12: OpenSloth Training + Transformers Validation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    cm = metrics['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
    "                xticklabels=['No Violation', 'Violation'],\n",
    "                yticklabels=['No Violation', 'Violation'])\n",
    "    axes[0,0].set_title('Confusion Matrix')\n",
    "    axes[0,0].set_xlabel('Predicted')\n",
    "    axes[0,0].set_ylabel('Actual')\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(true_labels, probabilities)\n",
    "    axes[0,1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {metrics[\"auc\"]:.3f})')\n",
    "    axes[0,1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    axes[0,1].set_xlabel('False Positive Rate')\n",
    "    axes[0,1].set_ylabel('True Positive Rate')\n",
    "    axes[0,1].set_title('ROC Curve (Transformers)')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    pos_probs = [p for p, t in zip(probabilities, true_labels) if t == 1]\n",
    "    neg_probs = [p for p, t in zip(probabilities, true_labels) if t == 0]\n",
    "    \n",
    "    axes[1,0].hist(neg_probs, bins=30, alpha=0.7, label='No Violation', color='blue', density=True)\n",
    "    axes[1,0].hist(pos_probs, bins=30, alpha=0.7, label='Violation', color='red', density=True)\n",
    "    axes[1,0].set_xlabel('Predicted Probability (Transformers)')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Probability Distribution by True Label')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    metric_names = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'AUC']\n",
    "    metric_values = [metrics[k] for k in ['accuracy', 'f1', 'precision', 'recall', 'auc']]\n",
    "    \n",
    "    bars = axes[1,1].bar(metric_names, metric_values, color=['skyblue', 'lightgreen', 'orange', 'pink', 'gold'])\n",
    "    axes[1,1].set_ylabel('Score')\n",
    "    axes[1,1].set_title('Performance Metrics (OpenSloth + Transformers)')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                      f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/tt12_transformers_validation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_by_rule(true_labels, predictions, probabilities, val_df):\n",
    "    \"\"\"Analyze performance by rule type\"\"\"\n",
    "    \n",
    "    analysis_df = val_df.copy()\n",
    "    analysis_df['predictions'] = predictions\n",
    "    analysis_df['probabilities'] = probabilities\n",
    "    \n",
    "    print(\"\\nüìä PERFORMANCE BY RULE (Transformers):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    rule_metrics = []\n",
    "    for rule in analysis_df['rule'].unique():\n",
    "        rule_data = analysis_df[analysis_df['rule'] == rule]\n",
    "        \n",
    "        rule_true = rule_data['rule_violation'].values\n",
    "        rule_pred = rule_data['predictions'].values\n",
    "        rule_prob = rule_data['probabilities'].values\n",
    "        \n",
    "        rule_auc = roc_auc_score(rule_true, rule_prob) if len(np.unique(rule_true)) > 1 else np.nan\n",
    "        rule_acc = accuracy_score(rule_true, rule_pred)\n",
    "        rule_f1 = f1_score(rule_true, rule_pred) if len(np.unique(rule_true)) > 1 else np.nan\n",
    "        \n",
    "        print(f\"Rule: {rule}\\n  Samples: {len(rule_data)}\\n  Accuracy: {rule_acc:.3f}\\n  F1 Score: {rule_f1:.3f}\\n  AUC Score: {rule_auc:.3f}\\n\")\n",
    "        \n",
    "        rule_metrics.append({'rule': rule, 'samples': len(rule_data), 'accuracy': rule_acc, 'f1': rule_f1, 'auc': rule_auc})\n",
    "    \n",
    "    analysis_df.to_csv('/kaggle/working/tt12_transformers_detailed_results.csv', index=False)\n",
    "    pd.DataFrame(rule_metrics).to_csv('/kaggle/working/tt12_transformers_rule_metrics.csv', index=False)\n",
    "    \n",
    "    return rule_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"üî¨ TT-12: OpenSloth Training + Transformers Validation\")\n",
    "    print(\"üöÄ Multi-GPU training + Universal compatibility!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    true_labels, predictions, probabilities, val_df = run_validation_transformers()\n",
    "    metrics = calculate_and_display_metrics(true_labels, predictions, probabilities)\n",
    "    create_visualizations(true_labels, predictions, probabilities, metrics)\n",
    "    analyze_by_rule(true_labels, predictions, probabilities, val_df)\n",
    "    \n",
    "    print(\"‚úÖ TT-12 Transformers Validation completed!\")\n",
    "    print(\"üìà Visualizations saved: /kaggle/working/tt12_transformers_validation_results.png\")\n",
    "    print(\"üìä Detailed results: /kaggle/working/tt12_transformers_detailed_results.csv\")\n",
    "    print(\"üìã Rule metrics: /kaggle/working/tt12_transformers_rule_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b04fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cache_dataset\n",
    "cache_dataset(cache_path=\"data/cache_qwen3_dataset_for_opensloth/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f23a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_opensloth.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ef5eb",
   "metadata": {},
   "source": [
    "# Appendix: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef0067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Run Validation**\n",
    "#@markdown Choose your validation method:\n",
    "VALIDATION_METHOD = \"vLLM\" #@param [\"vLLM\", \"Transformers\"]\n",
    "\n",
    "if VALIDATION_METHOD == \"vLLM\":\n",
    "    print(\"üöÄ Running vLLM validation for maximum speed and precision...\")\n",
    "    !python validation_vllm.py\n",
    "else:\n",
    "    print(\"‚öôÔ∏è Running Transformers validation for universal compatibility...\")\n",
    "    !python validation_transformers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display the validation results image\n",
    "if VALIDATION_METHOD == \"vLLM\":\n",
    "    display(Image(filename='/kaggle/working/tt12_validation_results.png'))\n",
    "else:\n",
    "    display(Image(filename='/kaggle/working/tt12_transformers_validation_results.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display the detailed results CSV\n",
    "if VALIDATION_METHOD == \"vLLM\":\n",
    "    df_detailed = pd.read_csv('/kaggle/working/tt12_detailed_results.csv')\n",
    "else:\n",
    "    df_detailed = pd.read_csv('/kaggle/working/tt12_transformers_detailed_results.csv')\n",
    "\n",
    "df_detailed.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
