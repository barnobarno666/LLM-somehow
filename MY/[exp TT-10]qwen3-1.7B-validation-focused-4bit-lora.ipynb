{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b3e9ec",
   "metadata": {},
   "source": [
    "# TT-10: Validation-Focused Training with Example-Based Learning\n",
    "\n",
    "This notebook implements a validation-focused approach where the model is trained on examples (like test-time training) and validated on real comments with known labels.\n",
    "\n",
    "**Key Concept:**\n",
    "- **Training**: Model learns from positive/negative examples (not actual comments)\n",
    "- **Validation**: Model predicts on real `body` comments with `rule_violation` labels\n",
    "- **Analysis**: Comprehensive metrics to understand generalization from examples to real data\n",
    "\n",
    "**Features:**\n",
    "- **Stratified Sampling**: Controllable % of training data while maintaining rule distribution\n",
    "- **Example-Based Training**: Similar to test-time training approach\n",
    "- **Real Comment Validation**: Test on actual comments with ground truth labels\n",
    "- **Comprehensive Metrics**: AUC, F1, Recall, Precision, Confusion Matrix\n",
    "- **Visualizations**: Performance plots and analysis\n",
    "- **4-bit + LoRA**: Memory-efficient training without DoRA for vLLM compatibility\n",
    "\n",
    "**Benefits:**\n",
    "- **Understand Generalization**: How well example-based training transfers to real comments\n",
    "- **Validate Test-Time Training**: Effectiveness of the example-learning approach\n",
    "- **Performance Analysis**: Detailed metrics with controllable data amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies - 4-bit BitsAndBytes + LoRA + Validation setup\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n",
    "# Install latest PEFT for LoRA support (no DoRA for vLLM compatibility)\n",
    "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'\n",
    "# Install analysis libraries\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'scikit-learn' 'matplotlib' 'seaborn'\n",
    "\n",
    "print(\"âœ… Dependencies installed for TT-10 validation-focused training\")\n",
    "print(\"ðŸ“Š Analysis libraries: scikit-learn, matplotlib, seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680572c",
   "metadata": {},
   "source": [
    "# 1. Configuration and Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants.py\n",
    "# Using base Qwen3 1.7B model from Kaggle input (no internet needed)\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen-3/transformers/1.7b/1\"  # Update this path as needed\n",
    "LORA_PATH = \"qwen3_1.7b_4bit_lora_validation/\"  # 4-bit LoRA output path for validation\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "# TT-10 Validation Parameters\n",
    "TRAINING_DATA_PERCENTAGE = 1.0  # Controllable % of training data (0.1 = 10%, 1.0 = 100%)\n",
    "USE_STRATIFIED_SAMPLING = True  # Maintain rule distribution when sampling\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''\n",
    "\n",
    "print(\"âœ… Using Qwen3 1.7B model from local Kaggle input\")\n",
    "print(f\"ðŸŽ¯ TT-10: Example-based training with {TRAINING_DATA_PERCENTAGE*100:.0f}% of data\")\n",
    "print(f\"ðŸ“Š Stratified sampling: {USE_STRATIFIED_SAMPLING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT, TRAINING_DATA_PERCENTAGE, USE_STRATIFIED_SAMPLING\n",
    "import random, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_example_based_training_data(data_path):\n",
    "    \"\"\"\n",
    "    TT-10: Create training data from examples (like test-time training)\n",
    "    This trains the model on examples, not actual comments\n",
    "    \"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    \n",
    "    # Sample data if needed while maintaining rule distribution\n",
    "    if TRAINING_DATA_PERCENTAGE < 1.0:\n",
    "        if USE_STRATIFIED_SAMPLING:\n",
    "            # Stratified sampling to maintain rule distribution\n",
    "            train_dataset = train_dataset.groupby('rule', group_keys=False).apply(\n",
    "                lambda x: x.sample(frac=TRAINING_DATA_PERCENTAGE, random_state=42)\n",
    "            ).reset_index(drop=True)\n",
    "            print(f\"ðŸ“Š Stratified sampling: {len(train_dataset)} samples ({TRAINING_DATA_PERCENTAGE*100:.0f}%)\")\n",
    "        else:\n",
    "            # Simple random sampling\n",
    "            train_dataset = train_dataset.sample(frac=TRAINING_DATA_PERCENTAGE, random_state=42).reset_index(drop=True)\n",
    "            print(f\"ðŸ“Š Random sampling: {len(train_dataset)} samples ({TRAINING_DATA_PERCENTAGE*100:.0f}%)\")\n",
    "    \n",
    "    print(f\"ðŸ“Š Training data size: {len(train_dataset)} samples\")\n",
    "    print(f\"ðŸ“Š Rule distribution: {train_dataset['rule'].value_counts().to_dict()}\")\n",
    "    \n",
    "    flatten = []\n",
    "    \n",
    "    # Create training data from examples (similar to test-time training)\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = train_dataset[[\"rule\",\"subreddit\",\n",
    "                                        \"positive_example_1\",\"positive_example_2\",\n",
    "                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                # Use positive example as the \"body\" to classify\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"  # other positive\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                # negative_example randomly selected\n",
    "                sub_dataset[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"negative_example_1\"],\n",
    "                    sub_dataset[\"negative_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 1  # Positive examples violate rules\n",
    "\n",
    "            else:  # violation_type == \"negative\"\n",
    "                # Use negative example as the \"body\" to classify\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                sub_dataset[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"positive_example_1\"],\n",
    "                    sub_dataset[\"positive_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 0  # Negative examples don't violate rules\n",
    "\n",
    "            # Drop original candidate columns\n",
    "            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    # Merge all DataFrames\n",
    "    example_training_df = pd.concat(flatten, axis=0)\n",
    "    example_training_df = example_training_df.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    print(f\"ðŸ“Š Example-based training dataset: {len(example_training_df)} samples\")\n",
    "    print(f\"ðŸ“Š Positive examples: {sum(example_training_df['rule_violation'] == 1)}\")\n",
    "    print(f\"ðŸ“Š Negative examples: {sum(example_training_df['rule_violation'] == 0)}\")\n",
    "    \n",
    "    return example_training_df\n",
    "\n",
    "\n",
    "def get_real_comment_validation_data(data_path):\n",
    "    \"\"\"\n",
    "    TT-10: Get real comments with labels for validation\n",
    "    This is what we actually want to predict\n",
    "    \"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    \n",
    "    # Use actual comments and their labels for validation\n",
    "    validation_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                                  \"positive_example_1\",\"positive_example_2\",\n",
    "                                  \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # Randomly select positive_example and negative_example for prompts\n",
    "    validation_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(validation_df)) < 0.5,\n",
    "        validation_df[\"positive_example_1\"],\n",
    "        validation_df[\"positive_example_2\"]\n",
    "    )\n",
    "    validation_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(validation_df)) < 0.5,\n",
    "        validation_df[\"negative_example_1\"],\n",
    "        validation_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # Drop original candidate columns\n",
    "    validation_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                               \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "    \n",
    "    print(f\"ðŸ“Š Real comment validation dataset: {len(validation_df)} samples\")\n",
    "    print(f\"ðŸ“Š Rule violations: {sum(validation_df['rule_violation'] == 1)} positive, {sum(validation_df['rule_violation'] == 0)} negative\")\n",
    "    \n",
    "    return validation_df\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_validation_dataset(dataframe):\n",
    "    \"\"\"Build dataset for validation (keep labels for evaluation)\"\"\"\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "    dataframe = dataframe[[\"prompt\", \"rule_violation\"]]  # Keep true labels for evaluation\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_example_based_training_data\n",
    "from constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n",
    "\n",
    "\n",
    "def main():\n",
    "    # TT-10: Get example-based training data (train on examples, not real comments)\n",
    "    train_df = get_example_based_training_data(DATA_PATH)\n",
    "    train_dataset = build_dataset(train_df)\n",
    "    \n",
    "    print(f\"Training dataset size: {len(train_dataset)} samples\")\n",
    "    \n",
    "    # BitsAndBytes 4-bit quantization config\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,  # Enable 4-bit quantization\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # Compute in FP16\n",
    "        bnb_4bit_use_double_quant=True,  # Use double quantization for better quality\n",
    "        bnb_4bit_quant_type=\"nf4\"  # Use NF4 quantization\n",
    "    )\n",
    "    print(\"âœ… BitsAndBytes 4-bit quantization config created\")\n",
    "    \n",
    "    # LoRA configuration (no DoRA for vLLM compatibility)\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,  # LoRA rank\n",
    "        lora_alpha=32,  # LoRA alpha  \n",
    "        lora_dropout=0.05,  # LoRA dropout\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # No use_dora=True for vLLM compatibility\n",
    "    )\n",
    "    print(\"âœ… LoRA config created (no DoRA for vLLM compatibility)\")\n",
    "    \n",
    "    # Training config optimized for validation\n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=1,  # Single epoch for validation\n",
    "        \n",
    "        # Batch sizes for 4-bit training\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 4*4*2 = 32\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",  # 8-bit optimizer for memory efficiency\n",
    "        learning_rate=1e-4,  # Learning rate\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"no\",  # Don't save during validation training\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    print(\"âœ… Training config created for example-based learning\")\n",
    "    \n",
    "    # Load model with BitsAndBytes quantization\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        # Remove device_map=\"auto\" to avoid distributed training conflicts\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,  # Use only local files (no internet)\n",
    "    )\n",
    "    print(\"âœ… Base model loaded with 4-bit quantization\")\n",
    "    \n",
    "    # Create SFTTrainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,  # Pass loaded model directly\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸš€ Starting example-based training (like test-time training)...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save LoRA adapters\n",
    "    trainer.save_model(LORA_PATH)\n",
    "    print(f\"âœ… LoRA adapters saved to: {LORA_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a52773",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile validation.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                           roc_auc_score, confusion_matrix, classification_report, roc_curve)\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "from utils import build_validation_dataset, get_real_comment_validation_data\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "\n",
    "\n",
    "def run_validation():\n",
    "    \"\"\"Run validation on real comments using example-trained model\"\"\"\n",
    "    \n",
    "    # Get real comment validation data\n",
    "    val_df = get_real_comment_validation_data(DATA_PATH)\n",
    "    val_dataset = build_validation_dataset(val_df)\n",
    "    \n",
    "    print(f\"ðŸ” Running validation on {len(val_dataset)} real comments\")\n",
    "    \n",
    "    # Initialize vLLM with LoRA\n",
    "    llm = vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2836,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n",
    "\n",
    "    texts = val_dataset[\"prompt\"]\n",
    "    true_labels = val_dataset[\"rule_violation\"]\n",
    "\n",
    "    # Generate predictions\n",
    "    outputs = llm.generate(\n",
    "        texts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,  # Get log probabilities for AUC calculation\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n",
    "    )\n",
    "\n",
    "    # Extract predictions and probabilities\n",
    "    predictions = []\n",
    "    probabilities = []  # For AUC calculation\n",
    "    \n",
    "    for i, out in enumerate(outputs):\n",
    "        log_probs = {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        \n",
    "        # Get prediction (highest probability)\n",
    "        if POSITIVE_ANSWER in log_probs and NEGATIVE_ANSWER in log_probs:\n",
    "            if log_probs[POSITIVE_ANSWER] > log_probs[NEGATIVE_ANSWER]:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "            \n",
    "            # Calculate probability for positive class (for AUC)\n",
    "            exp_pos = np.exp(log_probs[POSITIVE_ANSWER])\n",
    "            exp_neg = np.exp(log_probs[NEGATIVE_ANSWER])\n",
    "            prob_positive = exp_pos / (exp_pos + exp_neg)\n",
    "            probabilities.append(prob_positive)\n",
    "        else:\n",
    "            # Fallback if logprobs not available\n",
    "            predictions.append(0)\n",
    "            probabilities.append(0.5)\n",
    "\n",
    "    return true_labels, predictions, probabilities, val_df\n",
    "\n",
    "\n",
    "def calculate_and_display_metrics(true_labels, predictions, probabilities):\n",
    "    \"\"\"Calculate comprehensive metrics and display results\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    auc = roc_auc_score(true_labels, probabilities)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸ“Š TT-10 VALIDATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ðŸŽ¯ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"ðŸŽ¯ F1 Score:  {f1:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Precision: {precision:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Recall:    {recall:.4f}\")\n",
    "    print(f\"ðŸŽ¯ AUC Score: {auc:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    print(\"\\nðŸ“ˆ Confusion Matrix:\")\n",
    "    print(f\"True Negative: {cm[0,0]:4d} | False Positive: {cm[0,1]:4d}\")\n",
    "    print(f\"False Negative: {cm[1,0]:4d} | True Positive:  {cm[1,1]:4d}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nðŸ“‹ Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=['No Violation', 'Violation']))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "\n",
    "def create_visualizations(true_labels, predictions, probabilities, metrics):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('TT-10: Example-Based Training Validation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Confusion Matrix Heatmap\n",
    "    cm = metrics['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
    "                xticklabels=['No Violation', 'Violation'],\n",
    "                yticklabels=['No Violation', 'Violation'])\n",
    "    axes[0,0].set_title('Confusion Matrix')\n",
    "    axes[0,0].set_xlabel('Predicted')\n",
    "    axes[0,0].set_ylabel('Actual')\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(true_labels, probabilities)\n",
    "    axes[0,1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {metrics[\"auc\"]:.3f})')\n",
    "    axes[0,1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    axes[0,1].set_xlabel('False Positive Rate')\n",
    "    axes[0,1].set_ylabel('True Positive Rate')\n",
    "    axes[0,1].set_title('ROC Curve')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Probability Distribution\n",
    "    pos_probs = [probabilities[i] for i in range(len(probabilities)) if true_labels[i] == 1]\n",
    "    neg_probs = [probabilities[i] for i in range(len(probabilities)) if true_labels[i] == 0]\n",
    "    \n",
    "    axes[1,0].hist(neg_probs, bins=30, alpha=0.7, label='No Violation', color='blue', density=True)\n",
    "    axes[1,0].hist(pos_probs, bins=30, alpha=0.7, label='Violation', color='red', density=True)\n",
    "    axes[1,0].set_xlabel('Predicted Probability')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('Probability Distribution by True Label')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Metrics Bar Chart\n",
    "    metric_names = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'AUC']\n",
    "    metric_values = [metrics['accuracy'], metrics['f1'], metrics['precision'], metrics['recall'], metrics['auc']]\n",
    "    \n",
    "    bars = axes[1,1].bar(metric_names, metric_values, color=['skyblue', 'lightgreen', 'orange', 'pink', 'gold'])\n",
    "    axes[1,1].set_ylabel('Score')\n",
    "    axes[1,1].set_title('Performance Metrics')\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                      f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/tt10_validation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_by_rule(true_labels, predictions, probabilities, val_df):\n",
    "    \"\"\"Analyze performance by rule type\"\"\"\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    analysis_df = val_df.copy()\n",
    "    analysis_df['predictions'] = predictions\n",
    "    analysis_df['probabilities'] = probabilities\n",
    "    \n",
    "    print(\"\\nðŸ“Š PERFORMANCE BY RULE:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    rule_metrics = []\n",
    "    for rule in analysis_df['rule'].unique():\n",
    "        rule_data = analysis_df[analysis_df['rule'] == rule]\n",
    "        \n",
    "        rule_true = rule_data['rule_violation'].values\n",
    "        rule_pred = rule_data['predictions'].values\n",
    "        rule_prob = rule_data['probabilities'].values\n",
    "        \n",
    "        if len(np.unique(rule_true)) > 1:  # Check if both classes exist\n",
    "            rule_auc = roc_auc_score(rule_true, rule_prob)\n",
    "        else:\n",
    "            rule_auc = np.nan\n",
    "            \n",
    "        rule_acc = accuracy_score(rule_true, rule_pred)\n",
    "        rule_f1 = f1_score(rule_true, rule_pred) if len(np.unique(rule_true)) > 1 else np.nan\n",
    "        \n",
    "        print(f\"Rule: {rule}\")\n",
    "        print(f\"  Samples: {len(rule_data)}\")\n",
    "        print(f\"  Accuracy: {rule_acc:.3f}\")\n",
    "        print(f\"  F1 Score: {rule_f1:.3f}\" if not np.isnan(rule_f1) else \"  F1 Score: N/A\")\n",
    "        print(f\"  AUC Score: {rule_auc:.3f}\" if not np.isnan(rule_auc) else \"  AUC Score: N/A\")\n",
    "        print()\n",
    "        \n",
    "        rule_metrics.append({\n",
    "            'rule': rule,\n",
    "            'samples': len(rule_data),\n",
    "            'accuracy': rule_acc,\n",
    "            'f1': rule_f1,\n",
    "            'auc': rule_auc\n",
    "        })\n",
    "    \n",
    "    # Save detailed results\n",
    "    analysis_df.to_csv('/kaggle/working/tt10_detailed_results.csv', index=False)\n",
    "    pd.DataFrame(rule_metrics).to_csv('/kaggle/working/tt10_rule_metrics.csv', index=False)\n",
    "    \n",
    "    return rule_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸ”¬ TT-10: Example-Based Training Validation\")\n",
    "    print(\"ðŸ“š Training: Model learned from examples (like test-time training)\")\n",
    "    print(\"ðŸ§ª Validation: Testing on real comments with ground truth labels\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Run validation\n",
    "    true_labels, predictions, probabilities, val_df = run_validation()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_and_display_metrics(true_labels, predictions, probabilities)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(true_labels, predictions, probabilities, metrics)\n",
    "    \n",
    "    # Analyze by rule\n",
    "    rule_metrics = analyze_by_rule(true_labels, predictions, probabilities, val_df)\n",
    "    \n",
    "    print(\"âœ… Validation completed!\")\n",
    "    print(\"ðŸ“ˆ Visualizations saved: /kaggle/working/tt10_validation_results.png\")\n",
    "    print(\"ðŸ“Š Detailed results: /kaggle/working/tt10_detailed_results.csv\")\n",
    "    print(\"ðŸ“‹ Rule metrics: /kaggle/working/tt10_rule_metrics.csv\")\n",
    "    \n",
    "    return metrics, rule_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baefaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 32\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36210a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch --config_file accelerate_config.yaml train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcfacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python validation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display saved results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load detailed results\n",
    "try:\n",
    "    detailed_results = pd.read_csv('/kaggle/working/tt10_detailed_results.csv')\n",
    "    print(\"ðŸ“Š Detailed Results Shape:\", detailed_results.shape)\n",
    "    print(\"\\nðŸ“‹ Sample Results:\")\n",
    "    print(detailed_results[['rule', 'rule_violation', 'predictions', 'probabilities']].head(10))\n",
    "    \n",
    "    # Load rule metrics\n",
    "    rule_metrics = pd.read_csv('/kaggle/working/tt10_rule_metrics.csv')\n",
    "    print(\"\\nðŸ“ˆ Rule-wise Performance:\")\n",
    "    print(rule_metrics)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Results files not found: {e}\")\n",
    "    print(\"Run the validation cell first to generate results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis - data distribution and performance insights\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    detailed_results = pd.read_csv('/kaggle/working/tt10_detailed_results.csv')\n",
    "    \n",
    "    # Analyze performance by confidence level\n",
    "    print(\"ðŸŽ¯ Performance Analysis by Confidence Level:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create confidence bins\n",
    "    detailed_results['confidence'] = np.abs(detailed_results['probabilities'] - 0.5) * 2  # 0 = least confident, 1 = most confident\n",
    "    detailed_results['confidence_bin'] = pd.cut(detailed_results['confidence'], \n",
    "                                               bins=[0, 0.3, 0.6, 1.0], \n",
    "                                               labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Calculate accuracy by confidence bin\n",
    "    confidence_analysis = detailed_results.groupby('confidence_bin').agg({\n",
    "        'rule_violation': 'count',\n",
    "        'predictions': lambda x: accuracy_score(detailed_results.loc[x.index, 'rule_violation'], x)\n",
    "    }).rename(columns={'rule_violation': 'sample_count', 'predictions': 'accuracy'})\n",
    "    \n",
    "    print(confidence_analysis)\n",
    "    \n",
    "    # Data distribution analysis\n",
    "    print(\"\\nðŸ“Š Data Distribution Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Overall rule violation distribution:\")\n",
    "    print(detailed_results['rule_violation'].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nRule violation distribution by rule:\")\n",
    "    rule_dist = detailed_results.groupby('rule')['rule_violation'].agg(['count', 'mean'])\n",
    "    rule_dist.columns = ['total_samples', 'violation_rate']\n",
    "    print(rule_dist)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Run validation first to generate analysis data.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Analysis error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daddaa6c",
   "metadata": {},
   "source": [
    "# ðŸ“Š TT-10 Analysis Guide\n",
    "\n",
    "## ðŸŽ¯ **What TT-10 Tests:**\n",
    "- **Generalization**: How well example-based training transfers to real comments\n",
    "- **Effectiveness**: Whether test-time training approach works for this task\n",
    "- **Performance**: Comprehensive metrics on real comment classification\n",
    "\n",
    "## ðŸ”§ **How to Adjust Training Data:**\n",
    "\n",
    "### **Change Data Percentage** (Cell 4 - `constants.py`):\n",
    "```python\n",
    "TRAINING_DATA_PERCENTAGE = 0.5  # Use 50% of training data\n",
    "TRAINING_DATA_PERCENTAGE = 0.1  # Use 10% of training data\n",
    "TRAINING_DATA_PERCENTAGE = 1.0  # Use 100% of training data (default)\n",
    "```\n",
    "\n",
    "### **Toggle Stratified Sampling** (Cell 4 - `constants.py`):\n",
    "```python\n",
    "USE_STRATIFIED_SAMPLING = True   # Maintain rule distribution (recommended)\n",
    "USE_STRATIFIED_SAMPLING = False  # Random sampling\n",
    "```\n",
    "\n",
    "## ðŸ“ˆ **Understanding Results:**\n",
    "\n",
    "### **Key Metrics:**\n",
    "- **AUC Score**: Most important - measures discrimination ability (0.5 = random, 1.0 = perfect)\n",
    "- **F1 Score**: Balance of precision and recall\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Confusion Matrix**: Detailed breakdown of correct/incorrect predictions\n",
    "\n",
    "### **Visualizations Generated:**\n",
    "1. **Confusion Matrix**: Shows prediction accuracy breakdown\n",
    "2. **ROC Curve**: Illustrates true vs false positive rates\n",
    "3. **Probability Distribution**: How confident the model is by true label\n",
    "4. **Metrics Bar Chart**: Visual comparison of all performance metrics\n",
    "\n",
    "### **Rule-wise Analysis**:\n",
    "- Performance broken down by individual rules\n",
    "- Identifies which rules are easier/harder to learn\n",
    "- Shows sample distribution across rules\n",
    "\n",
    "## ðŸš€ **Optimization Tips:**\n",
    "\n",
    "### **If Performance is Low:**\n",
    "1. **Increase Training Data**: Set `TRAINING_DATA_PERCENTAGE = 1.0`\n",
    "2. **Adjust LoRA Parameters**: Increase rank (`r=32`) in `train.py`\n",
    "3. **More Training**: Increase `num_train_epochs` in `train.py`\n",
    "\n",
    "### **If Training is Too Slow:**\n",
    "1. **Reduce Data**: Set `TRAINING_DATA_PERCENTAGE = 0.3`\n",
    "2. **Smaller Batches**: Reduce `per_device_train_batch_size` in `train.py`\n",
    "3. **Lower Rank**: Reduce LoRA rank (`r=8`) in `train.py`\n",
    "\n",
    "## ðŸ’¡ **Key Insights:**\n",
    "- **High AUC (>0.8)**: Example-based training works well\n",
    "- **Low AUC (<0.6)**: May need more data or different approach\n",
    "- **Rule Variation**: Some rules may be inherently harder to learn\n",
    "- **Confidence Analysis**: Higher confidence predictions should be more accurate\n",
    "\n",
    "This validation approach helps understand whether the test-time training methodology is effective for your specific classification task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13efa93",
   "metadata": {},
   "source": [
    "# ðŸš€ TT-10 Speed Optimization Guide\n",
    "\n",
    "## âš¡ **Training Speed Optimization Strategies**\n",
    "\n",
    "### **1. Alternative Libraries for Faster Training**\n",
    "\n",
    "#### **A. Unsloth - 2x-5x Faster Training**\n",
    "```python\n",
    "# Replace TRL + PEFT with Unsloth for massive speed gains\n",
    "!pip install unsloth\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load model with Unsloth (much faster than standard transformers)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_PATH,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Add LoRA adapters (Unsloth handles this automatically)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "# Training arguments (much faster convergence)\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,  # Smaller batches work better with Unsloth\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=60,  # Converges much faster\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    ")\n",
    "\n",
    "# Use Unsloth's trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "```\n",
    "\n",
    "#### **B. Flash Attention v2 - 2x Faster Attention**\n",
    "```python\n",
    "# Install Flash Attention\n",
    "!pip install flash-attn --no-build-isolation\n",
    "\n",
    "# Use with transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\",  # Enable Flash Attention\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "```\n",
    "\n",
    "#### **C. xFormers - Memory Efficient Attention**\n",
    "```python\n",
    "# Alternative to Flash Attention\n",
    "!pip install xformers\n",
    "\n",
    "# Use with transformers\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"sdpa\",  # Use xFormers via SDPA\n",
    ")\n",
    "```\n",
    "\n",
    "### **2. Quantization Alternatives**\n",
    "\n",
    "#### **A. GPTQ Quantization (Faster Inference)**\n",
    "```python\n",
    "# Use GPTQ instead of BitsAndBytes for faster inference\n",
    "!pip install auto-gptq optimum\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=GPTQConfig(\n",
    "        bits=4,\n",
    "        dataset=\"c4\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "#### **B. AWQ Quantization (Better Quality)**\n",
    "```python\n",
    "# AWQ often gives better quality than GPTQ\n",
    "!pip install autoawq\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    BASE_MODEL_PATH,\n",
    "    quant_file=\"awq_model_w4_g128.pt\",\n",
    "    fuse_layers=True,\n",
    ")\n",
    "```\n",
    "\n",
    "### **3. Training Framework Alternatives**\n",
    "\n",
    "#### **A. PyTorch Lightning - Better Structure**\n",
    "```python\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "class LoRALightningModule(L.LightningModule):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch, labels=batch[\"input_ids\"])\n",
    "        return outputs.loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=2e-4)\n",
    "\n",
    "# Usage\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(save_top_k=1, monitor=\"train_loss\"),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "#### **B. HuggingFace Accelerate - Simpler Multi-GPU**\n",
    "```python\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Automatic device placement\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader\n",
    ")\n",
    "\n",
    "# Automatic mixed precision\n",
    "with accelerator.autocast():\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "\n",
    "accelerator.backward(loss)\n",
    "accelerator.step(optimizer)\n",
    "```\n",
    "\n",
    "### **4. Data Processing Optimizations**\n",
    "\n",
    "#### **A. Faster Data Loading with HuggingFace Datasets**\n",
    "```python\n",
    "from datasets import load_dataset, Dataset\n",
    "import multiprocessing\n",
    "\n",
    "# Use streaming for large datasets\n",
    "dataset = load_dataset(\n",
    "    \"csv\", \n",
    "    data_files=\"train.csv\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Parallel processing\n",
    "dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")\n",
    "```\n",
    "\n",
    "#### **B. Memory-Efficient Data Processing**\n",
    "```python\n",
    "# Use IterableDataset for memory efficiency\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class StreamingDataset(IterableDataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for chunk in pd.read_csv(self.data_path, chunksize=1000):\n",
    "            for item in chunk:\n",
    "                yield self.preprocess(item)\n",
    "```\n",
    "\n",
    "### **5. Advanced Speed Techniques**\n",
    "\n",
    "#### **A. Gradient Checkpointing + Offloading**\n",
    "```python\n",
    "# Combine gradient checkpointing with CPU offloading\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Use disk offloading for very large models\n",
    "from accelerate import disk_offload\n",
    "model = disk_offload(model, offload_dir=\"./offload\")\n",
    "```\n",
    "\n",
    "#### **B. Dynamic Batch Sizing**\n",
    "```python\n",
    "# Adjust batch size based on available memory\n",
    "def get_optimal_batch_size(model, tokenizer, max_length=2048):\n",
    "    \"\"\"Find largest batch size that fits in memory\"\"\"\n",
    "    batch_size = 1\n",
    "    while True:\n",
    "        try:\n",
    "            # Test if batch fits\n",
    "            inputs = tokenizer(\"test\" * max_length, return_tensors=\"pt\")\n",
    "            inputs = {k: v.repeat(batch_size, 1) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                model(**inputs.to(model.device))\n",
    "            batch_size *= 2\n",
    "        except RuntimeError:\n",
    "            return batch_size // 2\n",
    "```\n",
    "\n",
    "#### **C. Model Parallelism**\n",
    "```python\n",
    "# Use model parallelism for very large models\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH)\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint=BASE_MODEL_PATH,\n",
    "    device_map=\"auto\",  # Automatic device mapping\n",
    "    max_memory={0: \"10GB\", 1: \"10GB\"},  # Memory limits per GPU\n",
    ")\n",
    "```\n",
    "\n",
    "### **6. Performance Comparison Table**\n",
    "\n",
    "| Method | Speed Improvement | Memory Usage | Quality Impact | Complexity |\n",
    "|--------|------------------|--------------|----------------|------------|\n",
    "| **Unsloth** | 2x-5x faster | Same | Same/Better | Low |\n",
    "| **Flash Attention** | 2x faster | 20% less | Same | Low |\n",
    "| **GPTQ** | 3x faster inference | 75% less | Slight decrease | Medium |\n",
    "| **AWQ** | 3x faster inference | 75% less | Minimal decrease | Medium |\n",
    "| **PyTorch Lightning** | Same | Same | Same | Low |\n",
    "| **Accelerate** | Same | Better distribution | Same | Low |\n",
    "\n",
    "### **7. Recommended Optimization Stack**\n",
    "\n",
    "For **maximum speed** with TT-10:\n",
    "```python\n",
    "# 1. Use Unsloth for training\n",
    "!pip install unsloth\n",
    "\n",
    "# 2. Add Flash Attention\n",
    "!pip install flash-attn\n",
    "\n",
    "# 3. Use GPTQ for inference\n",
    "!pip install auto-gptq\n",
    "\n",
    "# 4. Optimize data loading\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_csv(\"train.csv\").to_iterable_dataset()\n",
    "```\n",
    "\n",
    "### **8. Monitoring and Profiling**\n",
    "\n",
    "#### **A. Training Speed Monitoring**\n",
    "```python\n",
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Profile training step\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    trainer.train()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "```\n",
    "\n",
    "#### **B. Memory Usage Tracking**\n",
    "```python\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "def log_system_stats():\n",
    "    # CPU and RAM\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    ram_percent = psutil.virtual_memory().percent\n",
    "    \n",
    "    # GPU\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_percent = gpus[0].memoryUtil * 100 if gpus else 0\n",
    "    \n",
    "    print(f\"CPU: {cpu_percent}%, RAM: {ram_percent}%, GPU: {gpu_percent}%\")\n",
    "```\n",
    "\n",
    "### **9. Quick Wins (5-30 minutes)**\n",
    "\n",
    "1. **Reduce data size**: `TRAINING_DATA_PERCENTAGE = 0.3`\n",
    "2. **Use smaller batches**: `per_device_train_batch_size = 2`\n",
    "3. **Enable gradient checkpointing**: `gradient_checkpointing=True`\n",
    "4. **Use 8-bit optimizer**: `optim=\"paged_adamw_8bit\"`\n",
    "5. **Reduce LoRA rank**: `r=8` instead of `r=16`\n",
    "\n",
    "### **10. Advanced Optimizations (1-4 hours)**\n",
    "\n",
    "1. **Switch to Unsloth**: Replace TRL training loop\n",
    "2. **Add Flash Attention**: Modify model loading\n",
    "3. **Use GPTQ quantization**: For inference speed\n",
    "4. **Implement dynamic batching**: Automatic batch size optimization\n",
    "5. **Add model parallelism**: For multi-GPU setups\n",
    "\n",
    "**Expected Results:**\n",
    "- **Unsloth + Flash Attention**: 3x-8x faster training\n",
    "- **GPTQ Inference**: 5x-10x faster inference\n",
    "- **Combined optimizations**: 10x-20x total speedup\n",
    "\n",
    "These optimizations can dramatically reduce training time while maintaining or even improving model quality!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
