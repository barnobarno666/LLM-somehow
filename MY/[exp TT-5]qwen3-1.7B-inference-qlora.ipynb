{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115f8136",
   "metadata": {},
   "source": [
    "# Qwen3 1.7B Test-Time Inference Notebook (4-bit BitsAndBytes + QLoRA)\n",
    "\n",
    "This notebook loads the pre-trained Qwen3-1.7B model (4-bit) and performs test-time training on test data only, then generates submission.\n",
    "\n",
    "**Key Changes from DoRA Version:**\n",
    "- Uses 4-bit BitsAndBytes quantized model (from 4-bit training notebook)\n",
    "- Uses QLoRA for test-time adaptation (standard QLoRA)\n",
    "- Optimized for Kaggle runtime constraints with lower memory usage\n",
    "- Compatible with the 4-bit training notebook output\n",
    "\n",
    "**Prerequisites:**\n",
    "- Upload the trained 4-bit model from the 4-bit training notebook as a Kaggle dataset\n",
    "- Update MODEL_DATASET_PATH to point to your uploaded 4-bit model dataset\n",
    "- Use `qwen3_1.7b_4bit_qlora_model.tar.gz` from the training notebook\n",
    "\n",
    "**Benefits of 4-bit Inference:**\n",
    "- **Lower VRAM**: ~6-8GB per GPU\n",
    "- **Faster loading**: 4-bit models load faster\n",
    "- **Better compatibility**: Full QLoRA support for test-time adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies - BitsAndBytes + QLoRA setup (no auto-gptq needed)\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n",
    "# Install latest PEFT for QLoRA support\n",
    "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'\n",
    "\n",
    "print(\"✅ Dependencies installed for 4-bit BitsAndBytes + QLoRA inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc8be7",
   "metadata": {},
   "source": [
    "# 1. Test Drive Training (Verification on Training Data)\n",
    "\n",
    "This section performs a quick test drive on the first 100 training examples to verify the setup works correctly. The fine-tuned model from this test is **not used** - we reload the original model for the actual test-time training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants.py\n",
    "# Base model path for test drive training\n",
    "BASE_MODEL_PATH = \"/kaggle/working/qwen3-1.7b\"  # Local Kaggle path for original model\n",
    "\n",
    "# Update this path to your uploaded 4-bit model dataset on Kaggle\n",
    "MODEL_DATASET_PATH = \"/kaggle/input/qwen3-1-7b-4bit-qlora-model\"  # TODO: Update this path\n",
    "PRETRAINED_MODEL_PATH = MODEL_DATASET_PATH + \"/qwen3_1.7b_4bit_finetuned/\"  # Extracted 4-bit model path\n",
    "\n",
    "# Test-time training paths\n",
    "TESTTIME_LORA_PATH = \"testtime_4bit_qlora_output/\"\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150234d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\n",
    "import random, numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_training_dataframe(data_path, sample_size=None):\n",
    "    \"\"\"Process training data for test drive\"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    \n",
    "    if sample_size:\n",
    "        train_dataset = train_dataset.head(sample_size)\n",
    "    \n",
    "    # Process training data\n",
    "    train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                              \"positive_example_1\",\"positive_example_2\",\n",
    "                              \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # Randomly select positive_example and negative_example\n",
    "    train_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"positive_example_1\"],\n",
    "        train_df[\"positive_example_2\"]\n",
    "    )\n",
    "    train_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"negative_example_1\"],\n",
    "        train_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # Drop original candidate columns\n",
    "    train_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                           \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def get_testtime_dataframe(data_path):\n",
    "    \"\"\"Only process test data for test-time training\"\"\"\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    flatten = []\n",
    "    \n",
    "    # ---------- Process test data only ----------\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                        \"positive_example_1\",\"positive_example_2\",\n",
    "                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                sub_dataset[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"negative_example_1\"],\n",
    "                    sub_dataset[\"negative_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "            else:  # violation_type == \"negative\"\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                sub_dataset[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"positive_example_1\"],\n",
    "                    sub_dataset[\"positive_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    # Merge all DataFrames\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    dataset.to_pandas().to_csv(\"/kaggle/working/testtime_dataset.csv\", index=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_drive.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_training_dataframe\n",
    "from constants import DATA_PATH, BASE_MODEL_PATH\n",
    "\n",
    "\n",
    "def test_drive_training():\n",
    "    print(\"🚀 Starting test drive training on first 100 training examples...\")\n",
    "    \n",
    "    # Load first 100 training examples\n",
    "    dataframe = get_training_dataframe(DATA_PATH, sample_size=100)\n",
    "    train_dataset = build_dataset(dataframe)\n",
    "    \n",
    "    print(f\"Test drive dataset size: {len(train_dataset)} samples\")\n",
    "    \n",
    "    # BitsAndBytes 4-bit quantization config\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    print(\"✅ BitsAndBytes 4-bit quantization config created\")\n",
    "    \n",
    "    # QLoRA configuration for test drive\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.045,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    print(\"✅ QLoRA config created for test drive\")\n",
    "    \n",
    "    # Load the base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    print(\"✅ Base model loaded for test drive\")\n",
    "    \n",
    "    # Short training config for test drive\n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=1,  # Very short training\n",
    "        \n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"no\",  # Don't save test drive model\n",
    "        output_dir=\"test_drive_output/\",\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    print(\"✅ Test drive training config created\")\n",
    "    \n",
    "    # Create trainer for test drive\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    # Run test drive training\n",
    "    trainer.train()\n",
    "    print(\"✅ Test drive training completed successfully!\")\n",
    "    print(\"📝 Setup verified - proceeding to main test-time training...\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del trainer, base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_drive_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4683e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test drive training\n",
    "!python test_drive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a08cc",
   "metadata": {},
   "source": [
    "# 2. Main Test-Time Training and Inference\n",
    "\n",
    "Now that the setup is verified, we proceed with the actual test-time training using the pre-trained model and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_testtime_dataframe\n",
    "from constants import DATA_PATH, PRETRAINED_MODEL_PATH, TESTTIME_LORA_PATH\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"📝 Note: Using original pre-trained model (not the test drive fine-tuned model)\")\n",
    "    \n",
    "    # Load test data for test-time training\n",
    "    dataframe = get_testtime_dataframe(DATA_PATH)\n",
    "    test_dataset = build_dataset(dataframe)\n",
    "    \n",
    "    print(f\"Test-time training dataset size: {len(test_dataset)} samples\")\n",
    "    \n",
    "    # BitsAndBytes 4-bit quantization config (same as training)\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    print(\"✅ BitsAndBytes 4-bit quantization config created\")\n",
    "    \n",
    "    # QLoRA config for test-time adaptation (same settings as TT-1)\n",
    "    testtime_lora_config = LoraConfig(\n",
    "        r=8,  # Reduced from 16 for speed\n",
    "        lora_alpha=16,  # From TT-1 config\n",
    "        lora_dropout=0.045,  # From TT-1 config\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # Removed use_dora=True for standard QLoRA\n",
    "    )\n",
    "    print(\"✅ QLoRA config created for test-time adaptation\")\n",
    "    \n",
    "    # Test-time training config (shorter training)\n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=1,  # Only 1 epoch for test-time adaptation\n",
    "        \n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=1e-4,  # Slightly lower for test-time\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"no\",  # Don't save intermediate checkpoints\n",
    "        output_dir=TESTTIME_LORA_PATH,\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    print(\"✅ Test-time training config created\")\n",
    "    \n",
    "    # Load the pre-trained 4-bit model (original model, not test drive result)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        PRETRAINED_MODEL_PATH,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    print(\"✅ Pre-trained 4-bit model loaded (original model)\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_PATH, trust_remote_code=True, local_files_only=True)\n",
    "    \n",
    "    # Test-time training with QLoRA\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        args=training_args,\n",
    "        train_dataset=test_dataset,\n",
    "        peft_config=testtime_lora_config,\n",
    "    )\n",
    "    \n",
    "    print(\"🚀 Starting test-time training with 4-bit QLoRA...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Keep the model in memory for inference (don't save test-time adapters)\n",
    "    print(\"✅ Test-time training completed - model ready for inference\")\n",
    "    \n",
    "    return trainer.model, tokenizer\n",
    "\n",
    "\n",
    "def generate_predictions(model, tokenizer, test_df):\n",
    "    \"\"\"Generate predictions for the test set\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Generating predictions\"):\n",
    "        prompt = f\"\"\"\n",
    "You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "Answer: Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "Answer: No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Extract Yes/No from response\n",
    "        if \"Yes\" in response or \"yes\" in response:\n",
    "            predictions.append(1)\n",
    "        elif \"No\" in response or \"no\" in response:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            # Default to 0 if unclear\n",
    "            predictions.append(0)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run test-time training\n",
    "    model, tokenizer = main()\n",
    "    \n",
    "    # Load test data for inference\n",
    "    test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = generate_predictions(model, tokenizer, test_df)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"],\n",
    "        \"prediction\": predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
    "    print(\"✅ Submission file created: /kaggle/working/submission.csv\")\n",
    "    print(\"🎉 4-bit BitsAndBytes + QLoRA inference completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference script\n",
    "!python inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69611da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check submission file\n",
    "import pandas as pd\n",
    "submission = pd.read_csv(\"/kaggle/working/submission.csv\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(\"First 5 predictions:\")\n",
    "print(submission.head())\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(submission[\"prediction\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3841e",
   "metadata": {},
   "source": [
    "# ⚡ Performance Notes for 4-bit QLoRA Inference\n",
    "\n",
    "## Memory Usage:\n",
    "- **4-bit Model**: ~6-8GB VRAM per GPU\n",
    "- **Test-time Training**: Additional ~2GB for QLoRA adapters\n",
    "- **Total**: ~8-10GB per GPU (fits on T4 GPUs)\n",
    "\n",
    "## Speed Optimizations:\n",
    "- **4-bit Quantization**: Faster inference than 8-bit\n",
    "- **QLoRA**: Efficient parameter updates\n",
    "- **Batch Processing**: Can be added for faster prediction generation\n",
    "\n",
    "## Compatibility:\n",
    "- **BitsAndBytes**: Full support for 4-bit operations\n",
    "- **QLoRA**: Standard and reliable\n",
    "- **Kaggle**: Optimized for offline execution\n",
    "\n",
    "## Expected Performance:\n",
    "- **Accuracy**: Similar to DoRA with potentially better stability\n",
    "- **Speed**: 20-30% faster than GPTQ\n",
    "- **Memory**: 40-50% less VRAM usage"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
