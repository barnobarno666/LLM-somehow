{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9134eebc",
   "metadata": {},
   "source": [
    "# Qwen3 1.7B Inference with Test-Time Training (GPTQ + LoRA)\n",
    "\n",
    "This notebook performs inference with test-time training using GPTQ quantized Qwen3-1.7B and LoRA fine-tuning.\n",
    "\n",
    "**Key Changes from BitsAndBytes Version:**\n",
    "- Uses GPTQ quantized Qwen3-1.7B model (Int4/Int8)\n",
    "- Uses auto-gptq for quantization handling\n",
    "- Uses standard LoRA (no DoRA support with GPTQ)\n",
    "- Performs test-time training on test data before predictions\n",
    "- Compatible with pre-quantized models from Kaggle\n",
    "\n",
    "**Benefits of GPTQ + LoRA:**\n",
    "- **Pre-quantized**: Model already quantized, no dynamic quantization\n",
    "- **Stable**: GPTQ provides consistent quantization\n",
    "- **Memory Efficient**: Int4/Int8 quantization reduces VRAM usage\n",
    "- **Test Drive**: Verifies setup with small sample before full inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies - GPTQ + LoRA setup\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n",
    "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n",
    "# Install latest PEFT for LoRA support (no BitsAndBytes needed for GPTQ)\n",
    "!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'\n",
    "\n",
    "print(\"‚úÖ Dependencies installed for GPTQ + LoRA setup\")\n",
    "print(\"üìÅ Models will be loaded from GPTQ datasets on Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be756547",
   "metadata": {},
   "source": [
    "# 1. Test Drive Training (Verification on Test Data Sample)\n",
    "\n",
    "This section performs a quick test drive on the first 100 test examples to verify the setup works correctly. The fine-tuned model from this test is **not used** for final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constants.py\n",
    "# GPTQ model paths\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen3-gptq/transformers/1.7b-gptq-int4/1\"  # TODO: Update this path\n",
    "PRETRAINED_MODEL_PATH = \"/kaggle/input/qwen3-gptq-finetuned/qwen3_1.7b_gptq_finetuned/\"  # TODO: Update this path\n",
    "\n",
    "LORA_PATH = \"qwen3_1.7b_gptq_lora_output/\"  # GPTQ LoRA output path\n",
    "TESTTIME_MODEL_PATH = \"qwen3_1.7b_gptq_testtime/\"  # Path for test-time fine-tuned model\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are a moderator of subreddit.  given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''\n",
    "\n",
    "print(\"‚úÖ Using GPTQ model paths from Kaggle inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\n",
    "import random, numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_testtime_dataframe(data_path, sample_size=None):\n",
    "    \"\"\"Process test data for test-time training\"\"\"\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\")\n",
    "    \n",
    "    if sample_size:\n",
    "        test_dataset = test_dataset.head(sample_size)\n",
    "    \n",
    "    flatten = []\n",
    "    \n",
    "    # Process test data for test-time training\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                        \"positive_example_1\",\"positive_example_2\",\n",
    "                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                sub_dataset[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"negative_example_1\"],\n",
    "                    sub_dataset[\"negative_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "            else:  # violation_type == \"negative\"\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                sub_dataset[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"positive_example_1\"],\n",
    "                    sub_dataset[\"positive_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    # Merge all DataFrames\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def get_dataframe_to_train(data_path, training_only=True):\n",
    "    \"\"\"Modified: Only use training data when training_only=True\"\"\"\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    \n",
    "    flatten = []\n",
    "\n",
    "    # ---------- Process training data ----------\n",
    "    train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n",
    "                              \"positive_example_1\",\"positive_example_2\",\n",
    "                              \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "    # Randomly select positive_example and negative_example\n",
    "    train_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"positive_example_1\"],\n",
    "        train_df[\"positive_example_2\"]\n",
    "    )\n",
    "    train_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"negative_example_1\"],\n",
    "        train_df[\"negative_example_2\"]\n",
    "    )\n",
    "\n",
    "    # Drop original candidate columns\n",
    "    train_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                           \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "    flatten.append(train_df)\n",
    "    \n",
    "    # Changed: Skip test data processing when training_only=True\n",
    "    if not training_only:\n",
    "        test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # ---------- Process test data ----------\n",
    "        for violation_type in [\"positive\", \"negative\"]:\n",
    "            for i in range(1, 3):\n",
    "                sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                            \"positive_example_1\",\"positive_example_2\",\n",
    "                                            \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "                if violation_type == \"positive\":\n",
    "                    body_col = f\"positive_example_{i}\"\n",
    "                    other_positive_col = f\"positive_example_{3-i}\"\n",
    "                    sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                    sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                    sub_dataset[\"negative_example\"] = np.where(\n",
    "                        np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                        sub_dataset[\"negative_example_1\"],\n",
    "                        sub_dataset[\"negative_example_2\"]\n",
    "                    )\n",
    "                    sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "                else:  # violation_type == \"negative\"\n",
    "                    body_col = f\"negative_example_{i}\"\n",
    "                    other_negative_col = f\"negative_example_{3-i}\"\n",
    "                    sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                    sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                    sub_dataset[\"positive_example\"] = np.where(\n",
    "                        np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                        sub_dataset[\"positive_example_1\"],\n",
    "                        sub_dataset[\"positive_example_2\"]\n",
    "                    )\n",
    "                    sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "                sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                          \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "                flatten.append(sub_dataset)\n",
    "\n",
    "    # Merge all DataFrames\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_drive.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_testtime_dataframe\n",
    "from constants import DATA_PATH, BASE_MODEL_PATH\n",
    "\n",
    "\n",
    "def test_drive_training():\n",
    "    print(\"üöÄ Starting test drive training on first 100 test examples...\")\n",
    "    \n",
    "    # Load first 100 test examples for test-time training\n",
    "    dataframe = get_testtime_dataframe(DATA_PATH, sample_size=100)\n",
    "    train_dataset = build_dataset(dataframe)\n",
    "    \n",
    "    print(f\"Test drive dataset size: {len(train_dataset)} samples\")\n",
    "    \n",
    "    # LoRA configuration for test drive (same as TT-1)\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # No use_dora=True - GPTQ doesn't support DoRA\n",
    "    )\n",
    "    print(\"‚úÖ LoRA config created for test drive\")\n",
    "    \n",
    "    # Load the GPTQ model (no quantization config needed - already quantized)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    print(\"‚úÖ GPTQ model loaded for test drive\")\n",
    "    \n",
    "    # Short training config for test drive\n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=1,  # Very short training\n",
    "        \n",
    "        per_device_train_batch_size=2,  # Smaller for GPTQ\n",
    "        gradient_accumulation_steps=8,\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"no\",  # Don't save test drive model\n",
    "        output_dir=\"test_drive_output/\",\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    print(\"‚úÖ Test drive training config created\")\n",
    "    \n",
    "    # Create trainer for test drive\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    # Run test drive training\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Test drive training completed successfully!\")\n",
    "    print(\"üìù Setup verified - proceeding to test-time training...\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del trainer, base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_drive_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e31a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test drive training\n",
    "!python test_drive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760df2be",
   "metadata": {},
   "source": [
    "# 2. Test-Time Training on Full Test Data\n",
    "\n",
    "Now that the setup is verified, we perform test-time training on the full test dataset using our fine-tuned GPTQ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e05069",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_testtime_dataframe, build_prompt\n",
    "from constants import (DATA_PATH, BASE_MODEL_PATH, PRETRAINED_MODEL_PATH, \n",
    "                      TESTTIME_MODEL_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE)\n",
    "\n",
    "\n",
    "def test_time_training():\n",
    "    print(\"üöÄ Starting test-time training on full test dataset...\")\n",
    "    \n",
    "    # Load full test data for test-time training\n",
    "    dataframe = get_testtime_dataframe(DATA_PATH)\n",
    "    train_dataset = build_dataset(dataframe)\n",
    "    \n",
    "    print(f\"Test-time training dataset size: {len(train_dataset)} samples\")\n",
    "    \n",
    "    # LoRA configuration for test-time training (same as training)\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # No use_dora=True - GPTQ doesn't support DoRA\n",
    "    )\n",
    "    print(\"‚úÖ LoRA config created for test-time training\")\n",
    "    \n",
    "    # Check if pretrained model exists, otherwise use base GPTQ model\n",
    "    if os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "        print(f\"üì¶ Loading fine-tuned GPTQ model from: {PRETRAINED_MODEL_PATH}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            PRETRAINED_MODEL_PATH,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"üì¶ Fine-tuned model not found, using base GPTQ model from: {BASE_MODEL_PATH}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL_PATH,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True,\n",
    "        )\n",
    "    \n",
    "    print(\"‚úÖ GPTQ model loaded for test-time training\")\n",
    "    \n",
    "    # Test-time training configuration\n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=1,  # Single epoch for test-time training\n",
    "        \n",
    "        # GPTQ batch sizes for test-time training\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=2e-5,  # Lower learning rate for test-time training\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir=TESTTIME_MODEL_PATH,\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    print(\"‚úÖ Test-time training config created\")\n",
    "    \n",
    "    # Create trainer for test-time training\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    # Run test-time training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the test-time trained model\n",
    "    trainer.save_model(TESTTIME_MODEL_PATH)\n",
    "    print(f\"‚úÖ Test-time trained model saved to: {TESTTIME_MODEL_PATH}\")\n",
    "    \n",
    "    # Merge and save the final test-time model\n",
    "    print(\"üîÑ Merging test-time LoRA adapters...\")\n",
    "    \n",
    "    # Reload base model for merging\n",
    "    if os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            PRETRAINED_MODEL_PATH,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_PATH, trust_remote_code=True, local_files_only=True)\n",
    "    else:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL_PATH,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, local_files_only=True)\n",
    "    \n",
    "    # Load and merge test-time LoRA adapters\n",
    "    peft_model = PeftModel.from_pretrained(base_model, TESTTIME_MODEL_PATH)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    \n",
    "    # Save merged test-time model\n",
    "    final_testtime_path = TESTTIME_MODEL_PATH + \"_merged\"\n",
    "    merged_model.save_pretrained(final_testtime_path)\n",
    "    tokenizer.save_pretrained(final_testtime_path)\n",
    "    \n",
    "    print(f\"‚úÖ Final test-time trained model saved to: {final_testtime_path}\")\n",
    "    \n",
    "    return merged_model, tokenizer\n",
    "\n",
    "\n",
    "def generate_predictions(model, tokenizer):\n",
    "    print(\"üîÆ Generating predictions for test dataset...\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Generating predictions\"):\n",
    "        prompt = build_prompt(row)\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5,  # We only need \"Yes\" or \"No\"\n",
    "                do_sample=False,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,\n",
    "            )\n",
    "        \n",
    "        # Extract the generated text\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the answer after \"Answer:\"\n",
    "        if COMPLETE_PHRASE in generated_text:\n",
    "            answer_part = generated_text.split(COMPLETE_PHRASE)[-1].strip()\n",
    "            if POSITIVE_ANSWER.lower() in answer_part.lower():\n",
    "                prediction = 1\n",
    "            elif NEGATIVE_ANSWER.lower() in answer_part.lower():\n",
    "                prediction = 0\n",
    "            else:\n",
    "                # Default to negative if unclear\n",
    "                prediction = 0\n",
    "        else:\n",
    "            # Default to negative if no answer found\n",
    "            prediction = 0\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'rule_violation': predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "    print(\"‚úÖ Submission saved to: /kaggle/working/submission.csv\")\n",
    "    \n",
    "    # Show prediction distribution\n",
    "    print(f\"\\nPrediction Distribution:\")\n",
    "    print(f\"No Violation (0): {sum(p == 0 for p in predictions)} ({sum(p == 0 for p in predictions)/len(predictions)*100:.1f}%)\")\n",
    "    print(f\"Violation (1): {sum(p == 1 for p in predictions)} ({sum(p == 1 for p in predictions)/len(predictions)*100:.1f}%)\")\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"üìù Note: Using GPTQ model for test-time training and inference\")\n",
    "    \n",
    "    # Step 1: Perform test-time training\n",
    "    model, tokenizer = test_time_training()\n",
    "    \n",
    "    # Step 2: Generate predictions\n",
    "    submission_df = generate_predictions(model, tokenizer)\n",
    "    \n",
    "    print(\"üéâ GPTQ test-time training and inference completed successfully!\")\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2708cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 8  # Higher accumulation for GPTQ\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 32  # Same effective batch size: 2*8*2 = 32\n",
    "  train_micro_batch_size_per_gpu: 2  # Lower for GPTQ memory usage\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch --config_file accelerate_config.yaml inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ac5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify submission file\n",
    "import pandas as pd\n",
    "submission = pd.read_csv('/kaggle/working/submission.csv')\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Submission head:\\n{submission.head()}\")\n",
    "print(f\"Value counts:\\n{submission['rule_violation'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4ec6d",
   "metadata": {},
   "source": [
    "# ‚ö° Speed Optimization Guide for 2x T4 GPUs (28GB Total VRAM) - GPTQ Edition\n",
    "\n",
    "## Current Settings Status: ‚úÖ **GOOD** for 2x T4 GPUs with GPTQ\n",
    "- **Memory**: GPTQ Int4 model + LoRA fits in ~10-14GB per GPU\n",
    "- **Batch Size**: 2 per device √ó 8 accumulation = 32 effective batch size  \n",
    "- **DeepSpeed**: ZeRO Stage 2 with FP16 - optimal for this setup\n",
    "- **Speed**: Stable with GPTQ pre-quantization\n",
    "\n",
    "## üöÄ Additional Speed Optimizations for GPTQ:\n",
    "\n",
    "### **Quick Wins for Test-Time Training:**\n",
    "1. **Increase Batch Size Carefully** (Cell 9 - `inference.py`):\n",
    "   ```python\n",
    "   per_device_train_batch_size=3,  # Can try 3-4 if VRAM allows\n",
    "   gradient_accumulation_steps=6,   # Adjust accordingly\n",
    "   ```\n",
    "   \n",
    "2. **Faster Optimizer** (Cell 9 - `inference.py`):\n",
    "   ```python\n",
    "   optim=\"adamw_torch_fused\",  # If PyTorch 2.0+\n",
    "   ```\n",
    "\n",
    "3. **Reduce LoRA Rank** (Cell 9 - `inference.py`):\n",
    "   ```python\n",
    "   r=8,              # Can reduce from 16 to 8 for faster training\n",
    "   lora_alpha=16,     # Adjust proportionally\n",
    "   ```\n",
    "\n",
    "### **Inference Speed Optimizations:**\n",
    "1. **Batch Inference** (Cell 9 - `inference.py`):\n",
    "   ```python\n",
    "   # Process multiple samples at once\n",
    "   batch_size = 4  # Can increase if VRAM allows\n",
    "   ```\n",
    "\n",
    "2. **Optimized Generation** (Cell 9 - `inference.py`):\n",
    "   ```python\n",
    "   max_new_tokens=3,     # Reduce from 5 to 3\n",
    "   do_sample=False,      # Keep deterministic\n",
    "   use_cache=True,       # Enable KV cache\n",
    "   ```\n",
    "\n",
    "## üí° **GPTQ Performance Notes:**\n",
    "1. **Memory**: GPTQ uses more VRAM than 4-bit BitsAndBytes but is pre-quantized\n",
    "2. **Speed**: Generally stable, may be slower than dynamic quantization\n",
    "3. **Quality**: Consistent quantization quality, good for production\n",
    "4. **Compatibility**: Works with all standard LoRA configurations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
